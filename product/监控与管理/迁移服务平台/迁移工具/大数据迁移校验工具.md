## 概述

客户完成 Hive 文件数据迁移后需要确认迁移是否完整，校验工具以 jar 包的形式提供针对迁移源数据和目的数据的多维度的校验，判断迁移前后数据是否一致。


## 使用环境
#### 系统环境

Windows、Linux 和 macOS 系统。

#### 软件依赖

- Java Platform（JDK）版本1.8以上。
- 安装 Hadoop 环境。


## 步骤1：获取工具

前往下载 [大数据迁移校验工具](https://tools-release-1256125716.cos.ap-shanghai.myqcloud.com/package/hive-verify/hive-verify_v3.0.1.zip)。

## 步骤2：解压缩工具包
解压并保存到某个目录，例如：
```plaintext
unzip hive-verify-3.0.1.zip && cd hive-verify
```

## 步骤3：使用工具
1. Java 虚拟机的运行参数
请根据实际迁移文件数量和机器的资源进行指定，否则默认配置，具体例子设置如下：
```plaintext
java -Xms???m -Xmx ????m -XX:PermSize=????m -XX:MaxPermSize=???m -XX:MaxNewSize=???m
-XX:NewRatio=??? -XX:SurvivorRatio=???
-jar hive-check.jar
```
2. 校验模式
目前支持指定格式的校验方式有三种，通过 -m 参数指定：
<table>
<thead>
<tr>
<th>校验方式</th>
<th>命令参数（-m）</th>
<th>校验内容</th>
</tr>
</thead>
<tbody><tr>
<td>HDFS 基本信息校验</td>
<td>hive-base</td>
<td>基于 HDFS 文件总长度、文件空间大小以及包含的副本块数对比校验</td>
</tr>
<tr>
<td>ORC 文件中元数据文件内容信息校验</td>
<td>hive-orc-content</td>
<td>基于 ORC 文件结构中列块、行块、每个 ORC 文件的起始偏移量、Footer 中使用压缩方式、统计信息、Stripe 中的所有信息以及每条 Column 中的属性字段等对比校验</td>
</tr>
<tr>
<td>PARQUET 元数据文件内容信息校验</td>
<td>hive-parquet-content</td>
<td>基于 PARQUET 文件结构中列块行、组、页大小、每页起始偏移量、每行数据起始偏移量、表 scheme 结构定义以及文件创建使用的 API 库等详细信息进行对比</td>
</tr>
</tbody></table>
3. 单目录校验
```plaintext
java -jar hive-check.jar 
	 -src 源数据目录/源数据文件具体路径；注：source&target 库(/表)(/分区)要求一致
	 -tgt 迁移目的数据数据库目录/目的具体文件绝对路径；注：source&target 库(/表)(/分区)要求一致
	 -log 对比差异输出文件（若没有该文件会自行创建，无需手动创建，若不传默认输出当前目录difference.log）
	 -m  校验方式（如果是自动识别，可以不传；指定校验方式的话，请参考下表格中的支持校验方式,如果不传默认元数据文件内容校验）
	 -checkacl 对文件的权限进行检测(不传则不进行权限校验)
```
单目录校验示例1：指定文件格式校验
```plaintext
java -jar hive-check.jar
	-src hdfs://host:9000/user/hive/warehouse/raw.db/
	-tgt hdfs://remote-host:9000/user/hive/warehouse/targe.db/
	-log hive-orc-content-diff.log
	-m hive-orc-content  
	-checkacl
```
单目录校验示例2：自动识别文件格式校验
```plaintext
java -jar hive-check.jar
	-checkacl
	-src hdfs://host:9000/user/hive/warehouse/raw.db/
	-tgt hdfs://remote-host:9000/user/hive/warehouse/targe.db/
	-log hive-orc-content-diff.log
```
4. 批量校验
```plaintext
java -jar hive-check.jar 
	-plist：批量文件目录，具体文件内容格式请参考下述的表格，若格式不符合，将无法读取；注意：source&target 库(/表)(/分区)要求一致
	-log：对比差异输出文件（若没有该文件会自行创建，无需手动创建，若不传默认输出 difference.log）
	-m：校验方式（若自动识别，可以不传；指定校验方式的话，请参考下表格中的支持校验方式；若不传默认元数据文件内容校验）
	-t：并发线程数（如果不指定，该参数可以不传，默认异步线程256个）
	-checkacl：对文件的权限进行检测（不传则不进行权限校验）
```
批量文件中，每一行是一个校验的源和目标记录。源目录和目的目录之间支持英文符号`,`和`;`以及以一个 tab 三种方式作分割，即“`\t`”；示例如下：
```plaintext
hdfs://host:9000/user/hive/warehouse/source.db0;hdfs://host:9000/user/hive/warehouse/target.db0
hdfs://host:9000/user/hive/warehouse/source.db1;hdfs://host:9000/user/hive/warehouse/target.db1
hdfs://host:9000/user/hive/warehouse/source.db2,hdfs://host:9000/user/hive/warehouse/target.db2
```
批量校验示例1：指定文件格式校验
```plaintext
ava -jar hive-check.jar
	-plist filePath.txt (文件格式最好是 txt 或 log，避免因为格式而出现解析出错)
	-log hive-orc-content-diff.log
	-m hive-orc-content 
	-t 300（指定线程数300）
	-checkacl
```
批量校验示例2：自动识别文件格式校验
```plaintext
java -jar hive-check.jar
	-plist filePath.txt (文件格式最好是 txt 或 log，避免因为格式而出现解析出错)
	-log hive-orc-content-diff.log
	-checkacl
```

## 工具性能说明
对比校验过程中需要网络请求远程文件，因此性能主要在于寄主机的带宽，而对于性能要求高的场景，需要将校验文件拆成多批，多台服务节点并行分配校验。

 在带宽理想的情况下，可适当增加启动内存，保证文件读内存时，不会因 Jvm 发生 GC 而影响性能，至于回收器方式，请启动时自行选择。

 文件校验中将读入内存一次性操作，不存在持久对象，则可考虑适当增加年轻代的堆大小，建议比例8:2（G1回收不适用）。

 在批量情况下，该版本开放了批量目录任务调度的异步线程数，仅对于批量目录情况下，可指定异步执行各目录的线程数。由于工具在校验完成即关闭进程，并考虑到占用资源的临时性，传入的线程数就是异步线程池的**核心线程数**也是**最大线程数**。

 针对单个目录下的文件异步执行的线程数目前仍是工具默认，考虑到服务的默认线程数在1500个左右以及存在混合部署的进程，因此目前处理目录下的文件对比最大备用异步线程数是300。





