为了节约用户的开发成本和提升使用效率，TSF 提供了 Spring Cloud 全链路跟踪的组件。用户只需要在代码中配置组件依赖，即可直接使用 TSF 的全链路跟踪功能，无需关心日志采集、分析、存储等过程。 用户仅需要安装了对应的依赖包及添加依赖项即可，无须其他配置。

## 准备工作
开始实践调用链功能前，请确保已完成 [SDK 下载](https://cloud.tencent.com/document/product/649/20231)。

## 一、依赖项

### 1.15.0-`Series`-RELEASE 以及之后版本
`Series` 代指 Edgware、Finchley、Greenwich。

向工程中添加 `spring-cloud-tsf-starter` 依赖并开启 `@EnableTsf` 注解，详情请参考 [快速入门](https://cloud.tencent.com/document/product/649/20261) 文档。

### 1.15.0-Edgware-RELEASE/1.15.0-Finchley-RELEASE 之前版本
添加 pom.xml 依赖：
```xml
<dependency>
	<groupId>com.tencent.tsf</groupId>
	<artifactId>spring-cloud-tsf-sleuth</artifactId>
	<version><!-- 调整为 SDK 最新版本号 --></version>
</dependency>
```

## 二、微服务对下游组件访问的链路支持
微服务对 Redis、MySQL、MongoDB、消息队列 CMQ 的访问操作会产生跟踪日志， TSF 会对该日志进行采集、分析、统计，这些组件的调用会展现在 TSF 平台的链路追踪中。
>?非微服务组件的链路支持功能依赖于1.14.0版本及以上版本的 SDK，详情请参考 [SDK 更新日志](https://cloud.tencent.com/document/product/649/38983)。

### 1. Redis 链路使用说明
考虑到 Redis 库的多样性，以及 spring-data-redis 库的易用性，目前只对`spring-boot-starter-data-redis`进行支持，在引用 spring-boot-starter-data-redis 时不要指定版本，只需要整个工程依赖 parent pom 即可：
```pom
<parent>
    <groupId>com.tencent.tsf</groupId>
    <artifactId>spring-cloud-tsf-dependencies</artifactId>
    <version>tsf 的版本号（1.14以后开始支持 Redis）</version>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-data-redis</artifactId>
     </dependency>
</parent>
```

`spring-boot-starter-data-redis`的版本为 parent pom 文件管理的 Redis Starter 的版本。在代码中具体使用时，引入 RedisTemplate，然后使用其方法即可。不建议直接引用 Jedis 和 Lettuce 相关的依赖，spring-boot-starter-data-redis 会自动引用相关的依赖，并做适配。
如果通过其他方式引入 Redis 客户端（例如直接 new Jedis），则将无法在 TSF 的链路中查看到相应的信息。


#### 16.1-Finchley-RELEASE 版本 Redis 调用链使用方式

- 使用 lettuce 方式，pom 依赖如下：
```pom.xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-data-redis</artifactId>
</dependency>
```
不需要额外依赖，他自动依赖了lettuce方面的jar包，可以直接使用。也具有链路追踪的能力。

- 使用 jedis 方式，pom 依赖如下：
```pom.xml
<dependency>
    <groupId>org.springframework.data</groupId>
    <artifactId>spring-data-redis</artifactId>
</dependency>
<!-- redis -->
<dependency>
    <groupId>redis.clients</groupId>
    <artifactId>jedis</artifactId>
</dependency>
```

这样也能使用 SDK 的链路追踪能力。

### 2. MySQL 链路使用说明
支持实现 JDBC 规范的 MySQL 驱动器和各类连接池（如 Tomcat-JDBC、DBCP、Hikari、Druid），在使用时需引入 SpringBoot 相关依赖和 MySQL 驱动依赖：
```
<!-- spring-boot-starter-jdbc -->
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-jdbc</artifactId>
    <version>RELEASE</version>
</dependency>
<!-- mysql -->
<dependency>
    <groupId>mysql</groupId>
    <artifactId>mysql-connector-java</artifactId>
    <version>RELEASE</version>
</dependency>
```
引入依赖后，根据需要添加相关数据库连接池依赖或直接使用 SpringBoot 默认选项。

### 3. MongoDB 链路使用说明
考虑到 spring-data-mongodb 库的易用性，目前只对`spring-boot-starter-data-mongodb`进行支持，在引用 spring-boot-starter-data-mongodb 时不要指定版本，只需要整个工程依赖 parent pom 即可，示例如下：
```pom
  <parent>
      <groupId>com.tencent.tsf</groupId>
      <artifactId>spring-cloud-tsf-dependencies</artifactId>
      <version>tsf的版本号（1.14以后开始支持 MongoDB）</version>
      <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-data-mongodb</artifactId>
       </dependency>
  </parent>
```

`spring-boot-starter-data-mongodb`的版本即是 parent pom 文件管理的 mongoldb starter 的版本。在代码中具体使用时，引入MongoTemplate，然后使用其方法即可。
- 如果需要制定 MongoDB 连接的 URI，需要满足以下格式：
```shell
mongodb://host[:port1][/[database][?options]] #暂时只支持单节点 MongonDB
```
也可以不用填写，即默认 host 是本机，默认端口27017。
- 如果通过其他方式引入 MongoDB 客户端，例如直接`new MongoClient(host,port)`，则在 TSF 的链路中将无法查看到相应的信息。

### 4. 消息队列 CMQ 链路使用说明
CMQ 组件目前通过 Spring Cloud Stream Binder 方式接入Spring Cloud 体系，对于 CMQ 组件的全链路追踪目前基于`spring-cloud-stream-binder-cmq`扩展实现，使用时需在上下游服务中添加`spring-cloud-stream-binder-cmq`依赖并按照规范进行 CMQ 配置。
```
<!-- 使用CMQ -->
<dependency>
    <groupId>com.qcloud</groupId>
    <artifactId>spring-cloud-stream-binder-cmq</artifactId>
    <version>VERSION</version>
</dependency>
```
配置参考：
```
spring:
  application:
    name: cmq-demo
  cloud:
    stream:
      bindings:
        input:
          destination: test-topic
        output:
          destination: test-topic
      cmq:
        bindings:
          input:
            consumer:
              pollingWaitSeconds: 3
        binder:
          secretId: ***
          secretKey: ***
          endpoint: https://cmq-queue-***.api.qcloud.com
```

### 5. Kafka 链路追踪使用说明

Kafka 的链路追踪能力通过 Spring Boot 的自动配置方式实现。

#### 1.16.0-Edgware 版本说明

实现方式是向 bean 容器中各加入了一个 producerFactory 和 consumerFactory 类的 bean。他们各自分别重写了 createKafkaProducer 方法和 createKafkaConsumer 方法。使用该 producerFactory/consumerFactory 便具有了链路追踪的能力。建议使用 Spring 默认的 KafkaTemplate，无须自定义。

 **1.16.0-Edgware** 版使用的 spring-kafka 版本必须在**1.3.0**以上，因为从这个版本开始 kafka-client 中增加了请求头，这才能方便的实现链路追踪。

```xml
<!--支持kafka使用调用链-->
        <dependency>
            <groupId>org.springframework.kafka</groupId>
            <artifactId>spring-kafka</artifactId>
            <version>1.3.0.RELEASE</version>
        </dependency>
```

参考配置：

```properties
#============== kafka ===================
# 指定 kafka 代理地址，可以多个
spring.kafka.bootstrap-servers=
  
#=============== provider  =======================
spring.kafka.producer.retries=0
# 每次批量发送消息的数量
spring.kafka.producer.batch-size=16384
spring.kafka.producer.buffer-memory=33554432
  
# 指定消息 key 和消息体的编解码方式
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer
  
#=============== consumer  =======================
# 指定默认消费者 Group ID
spring.kafka.consumer.group-id=test-consumer-group
  
spring.kafka.consumer.auto-offset-reset=earliest
spring.kafka.consumer.enable-auto-commit=true
spring.kafka.consumer.auto-commit-interval=100
  
# 指定消息 key 和消息体的编解码方式
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
```

其他更详细配置、使用方法详见 Demo 示例。

#### 1.16.0-Finchley 版本说明
实现方式是向 Spring 容器中各增加了一个对切面，分别作用于 ProducerFactory.createProducer 方法和 ConsumerFactory.createConsumer 方法的，使得通过他们返回的 KafkaProducer 和 KafkaConsumer 具有链路追踪的能力。

使用 1.16.0-Finchley 版本**无须**考虑 spring-kafka 版本问题，无须指定它的版本，因为底层依赖的 Finchley 版本的 spring cloud 会自动为它配置高于 1.3.0.RELEASE 版本的spring-kafka。

```xml
        <!--支持kafka使用调用链-->
        <dependency>
            <groupId>org.springframework.kafka</groupId>
            <artifactId>spring-kafka</artifactId>
        </dependency>
```

参考配置：

```yml
  server:
    servlet:
      context-path: /
    port: xxxxx
  spring:
    application:
      name: xxxxxx
    kafka:
      bootstrap-servers: xxxxxx
      #生产者的配置，大部分我们可以使用默认的，这里列出几个比较重要的属性
      producer:
        #每批次发送消息的数量
        batch-size: 16
        #设置大于0的值将使客户端重新发送任何数据，一旦这些数据发送失败。注意，这些重试与客户端接收到发送错误时的重试没有什么不同。允许重试将潜在的改变数据的顺序，如果这两个消息记录都是发送到同一个partition，则第一个消息失败第二个发送成功，则第二条消息会比第一条消息出现要早。
        retries: 0
        #producer可以用来缓存数据的内存大小。如果数据产生速度大于向broker发送的速度，producer会阻塞或者抛出异常，以“block.on.buffer.full”来表明。这项设置将和producer能够使用的总内存相关，但并不是一个硬性的限制，因为不是producer使用的所有内存都是用于缓存。一些额外的内存会用于压缩（如果引入压缩机制），同样还有一些用于维护请求。
        buffer-memory: 33554432
        #key序列化方式
        key-serializer: org.apache.kafka.common.serialization.StringSerializer
        value-serializer: org.apache.kafka.common.serialization.StringSerializer
      #消费者的配置
      consumer:
        #Kafka中没有初始偏移或如果当前偏移在服务器上不再存在时,默认区最新 ，有三个选项 【latest, earliest, none】
        auto-offset-reset: latest
        #是否开启自动提交
        enable-auto-commit: true
        #自动提交的时间间隔
        auto-commit-interval: 100
        #key的解码方式
        key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
        #value的解码方式
        value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
        #在/usr/local/etc/kafka/consumer.properties中有配置
        group-id: test-consumer-group
  tsf:
    swagger:
      enabled: false
  logging:
    file: /tsf-demo-logs/${spring.application.name}/root.log
```

其他更详细配置、使用方法详见 Demo 示例。

## 标签与自定义元数据
调用链支持用户在代码中设置标签（Tag） 和自定义元数据（CustomMetada），分别用于调用链的筛选和附带业务信息。

**接口定义：**
```
public class TsfContext {
    /**
     * 设置多个 Tag。如果有某个 Tag 之前已经被设置过，那么它的值会被覆盖。
     */
    public static void putTags(Map<String, String> tagMap, TagControlFlag... flags) {}

    /**
     * 设置 Tag。如果该 key 之前已经被设置过，那么它的值会被覆盖。
     */
    public static void putTag(String key, String value, TagControlFlag... flags) {}
    
    /**
     * 设置 CustomMetadata。
     */
    public static void putCustomMetadata(Object customMetadata)
}
```
