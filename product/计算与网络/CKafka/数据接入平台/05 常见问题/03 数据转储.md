## 如何知道数据转储是否有堆积？

CKafka 数据转储，即 Kafka 的数据流出转储到其他源中，常见的例如 kafka to es，kafka to clickhouse 等等。

同步服务会消费 CKafka 实例的消息，因此会生成对应的消费分组，可在控制台的 ConsumerGroup 管理页面查看，一般消费分组命名为 datahub-task-xxx。同步服务消费到消息后，会写入转储目标的服务中，然后提交写入条数对应的 offset 位置。

因此判断转储是否堆积，可通过查看该消费分组的未消费的消息条数是否在持续增加来了解堆积情况。
![](https://qcloudimg.tencent-cloud.cn/raw/47ae382d9c121f0a8c3ae3cea40646c2.png)

## 数据有堆积如何处理？

数据有堆积的情况一般分为两种：

- 一种是同步服务的消费能力受限，可提高任务并发度，后台同步服务会增加消费者的数量；或者适当增加 Topic 的分区数，提高消费者吞吐能力；如果实例的消费流量达到配额上限被限流，还需要升配实例的带宽规格。
- 另一种情况是在上面提升 Kafka 端消费能力后，堆积仍未有效改善。可能是写入流出源的速率受限，导致同步服务未能快速完成写入并提交 offset 的流程。例如 es 在大批写入达到瓶颈时可能会产生保护服务的锁拒绝外部写入甚至导致同步任务异常；或是 tdw 每秒写入条数达到上限被限制写入等场景。这个时需要判断出流出源的写入瓶颈在哪里并调整提高流出源的写入速率。
