
本文以使用 VASP 软件进行高性能计算介绍如何配置自动伸缩策略。
## 背景信息
当您需要每天不定时提交作业，使用 E-HPC 集群几个小时进行大规模计算， 然后释放节点，您可以针对不同的作业类型，配置不同的伸缩策略。配置伸缩策略后，系统可以根据实时负载自动增加或减少计算节点。可以帮您合理利用资源，减少使用成本。
## 前置条件
请依照 [准备工作](https://cloud.tencent.com/document/product/1527/64694) 中的指引获取 SecretId 和 SecretKey，并完成  [服务角色授权](https://cloud.tencent.com/document/product/1527/64533)。
## 操作步骤
1. 安装最新版本的 [TCCLI 命令工具](https://cloud.tencent.com/document/product/440/34011) ，并进行 [初始化配置](https://cloud.tencent.com/document/product/440/34012) 请参见 THPC [地域列表](https://cloud.tencent.com/document/product/1527/72111#.E5.9C.B0.E5.9F.9F.E5.88.97.E8.A1.A8)。
2. 使用 [CreateCluster](https://cloud.tencent.com/document/product/1527/72102 ) 接口创建一个有 1 个 [管控节点]( https://cloud.tencent.com/document/api/1527/72108#ManagerNode，) 0 个 [计算节点](https://cloud.tencent.com/document/api/1527/72108#ComputeNode) 的 THPC 集群。
>!
>- 需要把 AutoScalingType 设置为 THPC_AS，调用后续步骤的 SetAutoScalingConfiguration 接口才会生效。
>- Placement 为 TCCLI Region 地域对应可用区。
>- VirtualPrivateCloud 设置需要是集群同可用区的 VPC ID，子网 ID。
>- 管控节点登录密码设置请参见 [ LoginSettings]( https://cloud.tencent.com/document/api/1527/72108#LoginSettings)。                                                                                         
>- 在您选择 Slurm 为 SchedulerType 调度器类型时，ImageId 请填写 img-l8og963d(centos 7.9)，SGE 请填写 img-3la7wgnt  (centos 7.8)，或者使用基于这两个镜像版本 [创建自定义镜像](https://cloud.tencent.com/document/product/213/4942)，传入自定义镜像 ID。
>- 集群创建时如果您不 [指定挂载目录](https://cloud.tencent.com/document/api/1527/72108#StorageOption) ，将自动为您在 /opt 目录下创建并挂载 CFS，所有节点共享这一目录，您可以在这个目录下安装您业务需要的软件，本文默认您已在 /opt 目录下安装编译完成依赖的 Intel oneAPI 和 VASP 软件。

 执行命令：
```PLAINTEXT
tccli thpc CreateCluster --cli-input-json file://./test.json
```
test.json 文件参数配置可参考：
```json
{
	"Placement": {
		"Zone": "ap-chongqing-1"
	},
	"ManagerNodeCount": 1,
	"ManagerNode": {
		"InternetAccessible": {
			"InternetMaxBandwidthOut": 10
		},
		"InstanceName": "ThpcTestSlurmManagerNode",
		"InstanceType": "S5.MEDIUM4"
	},
	"SchedulerType": "SLURM",
	"LoginSettings": {
		"Password": "xxxxxxxx"
	},
	"ImageId": "img-l8og963d",
	"VirtualPrivateCloud": {
		"VpcId": "vpc-r9jw2jzv",
		"SubnetId": "subnet-0v4eybey"
	},
    "AutoScalingType": "THPC_AS"
}
```
提交请求并执行完毕后，您需要记录 ClusterId：
<img src="https://qcloudimg.tencent-cloud.cn/raw/5c8d0c2c7d1782ff0703c0ea56a2249a.png" width="800"/>
<br>使用 ClusterId 查询集群状态：
```
tccli thpc DescribeClusters --ClusterIds '["hpc-xxxxxxxx"]'
```
返回结果，其中：

 - ClusterStatus 为 RUNNING 状态表示集群创建完成，可以开始使用。
 -  ClusterStatus 为 INITING 状态表示集群正在初始化中，需要等待直到为 RUNNING或者 INIT_FAILED 状态之一。
 -  ClusterStatus 为 INIT_FAILED 状态表示集群创建失败，您可以使用 [API Explorer 查看集群活动](https://console.cloud.tencent.com/api/explorer?Product=thpc&Version=2022-04-01&Action=DescribeClusterActivities) 查看详细集群状态信息。
<img src="https://qcloudimg.tencent-cloud.cn/raw/931f9a8a2a69a15a03bd467932417c29.png" width="800"/><br>
3. 使用 [SetAutoScalingConfiguration](https://cloud.tencent.com/document/product/1527/81796)  接口开启 THPC AS 功能。
```PLAINTEXT
tccli thpc SetAutoScalingConfiguration --cli-input-json file://./SetAutoScalingConfiguration.json
```
```json
{
	"ClusterId": "hpc-xxxxxxxx",      
	"ExpansionBusyTime": 120,        
	"ShrinkIdleTime": 120,           
	"QueueConfigs": [{
		"QueueName": "compute",       
		"MinSize": 1,                 
		"MaxSize": 5,                 
		"EnableAutoExpansion": true,  
		"EnableAutoShrink": true,     
		"ExpansionNodeConfigs": [{
			"Placement": {
				"Zone": "ap-chongqing-1"
			},
			"VirtualPrivateCloud": {
				"VpcId": "vpc-r9jw2jzv",
				"SubnetId": "subnet-0v4eybey"
			},
			"InstanceType": "S6.4XLARGE32"
		}, {
			"Placement": {
				"Zone": "ap-chongqing-1"
			},
			"VirtualPrivateCloud": {
				"VpcId": "vpc-r9jw2jzv",
				"SubnetId": "subnet-0v4eybey"
			},
			"InstanceType": "S6.2XLARGE32"
		}]
	}]
}
```
以上 SetAutoScalingConfiguration.json 文件接口配置好集群自动扩缩容参数如下：
 - 开启 Slurm 调度器 compute 队列的自动扩容能力，THPC 会根据扩容策略对连续等待超过120秒的作业进行自动扩容，并且弹性节点的最大值不超过5个。ExpansionBusyTime 参数取值最小值为120s。
 - 开启 Slurm 调度器 compute 队列的自动缩容能力，THPC 会对连续空闲超过120s的节点进行自动缩容，并且弹性节点的最小值不小于1个。ShrinkIdleTime 取值最小值为0s。
compute 队列配置了两个扩容机型。
<table>
<thead>
<tr>
<th>序号</th>
<th>机型</th>
<th>CPU</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>S6.4XLARGE32</td>
<td>16</td>
</tr>
<tr>
<td>2</td>
<td>S6.2XLARGE32</td>
<td>8i'm'a</td>
</tr>
</tbody></table>
4. 登录管控节点提交 VASP 作业：
submit.sh 脚本内容如下，脚本提交一个 vasp 作业，在一个节点上使用8个核运行：
```PLAINTEXT
#!/bin/sh
#SBATCH -p compute
#SBATCH -o job.%j.out
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=8
#SBATCH -D /opt/graphene

source /opt/intel/oneapi/setvars.sh
/opt/intel/oneapi/mpi/2021.7.0/bin/mpirun -n 16 /opt/vasp.6.3.0/bin/vasp_std
```
提交作业前，集群没有任何节点存在：<br>
<img src="https://qcloudimg.tencent-cloud.cn/raw/0b660e673ad8c678095ec4ca4c3b29a5.png"  width="600"/><br>
提交作业：
```PLAINTEXT
sbatch submit.sh
```
5. 集群扩容并执行作业，作业结束后缩容到0节点。
提交作业之后，THPC 会根据扩容策略，扩容出最匹配的两个8核的 S6.2XLARGE32 实例。等待约4～8分钟后，节点加入集群并运行提交的作业。
<dx-alert infotype="explain" title="">
等待的时间主要包括作业连续等待时间 120 秒、创建实例的时间和节点创建完初始化并加入集群的时间。
</dx-alert>
<img src="https://qcloudimg.tencent-cloud.cn/raw/89c5e1252e3138b774e3f290431aba94.png"  width="800"/><br>
任务执行完成之后,节点进入空闲（idle）状态：<br>
<img src="https://qcloudimg.tencent-cloud.cn/raw/d3529f92767fd8749abc0218f1639a9d.png"  width="800"/><br>
等待2～3分钟之后，会进行自动缩容到最小节点数：<br>
<img src="https://qcloudimg.tencent-cloud.cn/raw/b47b5e7168ac0c851e91aca692ec42d2.png" width="800"/><br>
6. 使用完毕后，使用如下命令删除集群，关联的计算实例将会被销毁。 CFS 文件存储不会删除，如果您需要销毁，可前往 [CFS 控制台](https://console.cloud.tencent.com/cfs/)。
```PLAINTEXT
tccli thpc DeleteCluster --ClusterId hpc-xxxxxxxx

```
