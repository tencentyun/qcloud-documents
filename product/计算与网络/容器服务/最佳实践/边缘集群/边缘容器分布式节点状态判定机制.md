边缘弱网络会触发 Kubernetes 驱逐机制，引起不符合预期的 Pod 驱逐动作。边缘计算情景下，边缘节点与云端的网络环境十分复杂，网络质量无法保证，容易出现 API Server 和节点连接中断等问题。如果不加改造直接使用原生 Kubernetes，节点状态会经常出现异常，引起 Kubernetes 驱逐机制生效，导致 Pod 的驱逐和 EndPoint 的缺失，最终造成服务的中断和波动。

为解决这个问题，[边缘容器服务](https://cloud.tencent.com/document/product/457/42876)（简称 TKE Edge）首创分布式节点状态判定机制。该机制可以更好地识别驱逐时机，保障系统在弱网络下正常运转，避免服务中断和波动。

## 需求场景

在边缘场景下，需面对云边弱网络的环境。边缘设备位于边缘云机房和移动边缘站点，与云端连接的网络环境较为复杂，既包含云端（控制端）和边缘端的网络环境不可靠，也包含边缘节点之间的网络环境不可靠。

### 智慧工厂[](id:SmartFactory)

以智慧工厂为例，边缘节点位于厂房仓库和车间，控制端 Master 节点在腾讯云的中心机房内。示意图如下：
![img](https://main.qcloudimg.com/raw/ee1d15998778f214887ddc36ad605789.png)

- 仓库和车间内的边缘设备同云端集群之间的网络较复杂，因特网、5G、WIFI 等形态均有可能，网络质量差次不齐没有保障。
- 相比于云端的网络环境，仓库和车间内的边缘设备之间是本地网络，网络质量优于同云端集群之间的连接，相对而言更加可靠。

### 音视频拉流场景
音视频拉流场景如下图所示：
![img](https://main.qcloudimg.com/raw/c2a78c02550fa12e5f139e5ba6202245.png)

考虑到用户体验及公司成本，音视频拉流经常需要进行提高边缘缓存命中率减少回源、将用户请求的同一文件调度到同一个服务实例以及服务实例缓存文件。

在原生 Kubernetes 的情况下，如果 Pod 因为网络波动而频繁重建，一方面会影响服务实例缓存效果，另一方面会引起调度系统将用户请求调度到其他服务实例。这两点都会对 CDN 效果造成很大甚至不能接受的影响。

事实上，边缘节点完全运行正常，Pod 驱逐或重建其实是完全不必要的。为了克服这个问题，保持服务的持续可用，TKE 边缘容器团队提出了分布式节点状态判定机制。

## 需求痛点

### 原生 Kubernetes 处理方式

云边弱网络是影响了运行在边缘节点上的 kubelet 与云端 APIServer 之间通信，云端 APIServer 无法收到 kubelet 的心跳或者进行续租，无法准确获取该节点和节点上 Pod 的运行情况，如果持续时间超过设置的阈值， APIServer 会认为该节点不可用，并做出如下作：

- 失联的节点状态被置为 NotReady 或者 Unknown 状态，并被添加 NoSchedule 和 NoExecute 的 taints。
- 失联的节点上的 Pod 被驱逐，并在其他节点上进行重建。
- 失联的节点上的 Pod 从 Service 的 Endpoint 列表中移除。

## 解决方案

### 设计原则

在边缘计算场景中，仅依赖边缘端和 APIServer 的连接情况来判断节点是否正常并不合理，为了让系统更健壮，需要引入额外的判断机制。

相较于云端和边缘端，边缘端节点之间的网络更稳定，可利用更稳定的基础设施提高准确性。边缘容器服务首创了边缘健康分布式节点状态判定机制，除了考虑节点与 APIServer 的连接情况，还引入了边缘节点作为评估因子，以便对节点进行更全面的状态判断。经过测试及大量的实践证明，该机制在云边弱网络情况下提高了系统在节点状态判断上的准确性，为服务稳定运行保驾护航。该机制的主要原理如下：
- 每个节点定期探测其他节点健康状态
- 集群内所有节点定期投票决定各节点的状态
- 云端和边缘端节点共同决定节点状态

首先，节点内部之间进行探测和投票，共同决定具体某个节点是否存在状态异常，保证大多数节点的一致判断才能决定节点的具体状态。其次，即使节点之间的网络状态通常情况下优于云边网络，但也应该考虑边缘节点复杂的网络情况，其网络并非100%可靠。因此，也不能完全信赖节点之间的网络，节点的状态不能只由节点自行决定，云边共同决定才更为可靠。基于这个考虑，做出如下设计：

| 节点最终状态     | 云端判定正常 | 云端判定异常                                                 |
| ---------------- | ------------ | ------------------------------------------------------------ |
| 节点内部判定正常 | 正常         | 不再调度新的 Pod 到该节点                                    |
| 节点内部判定异常 | 正常         | 驱逐存量 Pod；从 EndPoint 列表摘除； 不再调度新的 Pod 到该节点 |

### 方案特性    

当云端判定节点异常，但是其他节点认为节点正常的时候，虽然不会驱逐已有 Pod，但为了确保增量服务的稳定性，将不会再将新的 Pod 调度到该节点上，存量节点的正常运行也得益于边缘集群的边缘自治能力。

由于边缘网络和拓扑的特殊性，经常会存在节点组之间网络单点故障的问题，例如 [智慧厂房](#SmartFactory)，仓库和车间虽然都属于厂房这个地域内，但他们间的网络连接仅依靠一条关键链路，一旦这条链路发生中断，就会造成节点组之间的分裂，本文提供的方案能够确保两个分裂的节点组失联后互相判定时始终保持多数的一方节点不会被判定为异常，避免被判定为异常造成 Pod 只能被调度到少部分的节点上，造成节点负载过高的情况。

边缘设备可能位于不同的地区且相互不通，本文提供的方案支持多地域内的节点状态判定，可以方便地将节点依据地域或者其他方式进行分组，实现组内的检查。即使重新分组也无需重新部署检测组件或重新初始化，适应边缘计算的网络情况。分组后，节点只会判定同一个组内的节点状态。

## 操作步骤    

边缘容器集群分布式节点状态判定机制功能默认开启，多地域检测功能默认关闭。如果需要使用多地域功能，请执行如下操作：

### 通过给节点编辑标签的方式为节点分组

1. 登录 [容器服务控制台](https://console.cloud.tencent.com/tke2) ，选择左侧导航栏中的**边缘集群**。
2. 选择需要编辑标签的节点所在的集群 ID，进入该集群管理页面。
3. 选择**节点管理** > **节点**，进入节点列表页。如下图所示：
   ![](https://main.qcloudimg.com/raw/712450952968557eefd23e4833deaa5f.png)
4. 选择需要编辑标签的节点所在行右侧的**更多** > **编辑标签**。
5. 在弹出的“编辑标签”窗口，按需新增 Label。如下图所示：
   ![](https://main.qcloudimg.com/raw/3ae0a48eba8a180ccdea78727fe26917.png)
6. 单击**确定**即可。

### 开启多地域检查开关

> ! 当开启多地域开关后，如果没有为节点分组，则默认各节点自行成组，不会检查其他节点。
> 
1. 登录 [容器服务控制台](https://console.cloud.tencent.com/tke2)，选择左侧导航栏中的**边缘集群**。
2. 单击需要开启多地域检查功能的集群 ID，进入该集群管理页面。
3. 选择**基本信息**，进入“基本信息”页面，打开**开启多地域检查**即可。
   ![](https://main.qcloudimg.com/raw/84ee3dd3d061f34675e907dd51309189.png)
