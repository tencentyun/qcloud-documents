为确保您的业务或应用可在容器服务集群中稳定可靠的运行，本文介绍创建集群时的推荐配置。

## 磁盘类型及大小
- **磁盘类型**
推荐选择 SSD 云硬盘。
对于 Worker 节点，在创建集群时推荐选择【挂载数据盘】。该盘用于给 `/var/lib/docker` 存放本地镜像，可避免后续因镜像太多而造成磁盘根目录容量不够的问题。在节点运行一段时间后，本地会存在大量无用镜像，此时您可先下线该机器，重新构建磁盘后再上线。

- **磁盘大小**
因 Docker 镜像、系统日志、应用日志均保存在磁盘上，集群节点需要一定量的磁盘空间。在创建集群时，需考虑每个节点上要部署的 Pod 数量、每个 Pod 的日志大小、镜像大小、临时数据及一些系统预留值。
容器服务集群中，CVM 操作系统占用3G左右的磁盘空间，建议预留 CVM 操作系统8G的空间。剩余的磁盘空间由集群中的资源对象使用。

## 是否立即构建 Worker 节点
您可在创建集群时，同时构建按量计费或包年包月模式的 Worker 节点。也可根据后续需求，向集群内添加 Worker 节点。

## 网络选择
- **复用已有私有网络**
如需连接一些外部服务（例如云数据库 MySQL），由于不同私有网络间默认内网隔离，则需考虑是否复用已有私有网络，而不是重新创建。您可创建一个交换机，将运行中集群的机器放置在该交换机下，便于管理。

- **选择网络插件**
目前创建集群时提供 GlobalRouter 及 VPC-CNI 两种网络插件，请参考 [如何选择容器服务网络模式](https://cloud.tencent.com/document/product/457/41636) 进行选择。

- **设置 Pod 网络 CIDR**
Pod 网络 CIDR 不能设置过小，设置过小会导致可支持的节点数量受限。设置该值时需考虑节点的 Pod 数量，例如，Pod 网络 CIDR 的网段是/16，那么就会有256 × 256个地址，若每个节点的 Pod 数量为128，则最多可支持512个节点。


## 声明每个 Pod 的 resource
在使用集群时，经常会遇到在一个节点上调度了太多 Pod，导致节点的负载过高，无法正常对外提供服务的问题。

为了避免该问题，在集群中部署 Pod 时，您可以通过指定该 Pod 所需的 Request 及 Limit 资源解决。Kubernetes 在部署该 Pod 时，会根据 Pod 需求寻找一个具有充足空闲资源的节点进行部署。以下 YAML 示例声明名为 Nginx 的 Pod 需要1核 CPU、1024M内存、运行中实际使用不能超过2核 CPU 和4096M内存。
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    resources: # 资源声明
      requests:
        memory: "1024Mi"
        cpu: "1000m"
      limits:
        memory: "4096Mi"
        cpu: "2000m"
```
Kubernetes 采用静态资源调度方式，对于每个节点上的剩余资源进行计算：节点剩余资源 = 节点总资源 - 已经分配出去的资源。并不是实际使用的资源，如果您手动运行一个很耗资源的程序，Kunbernetes 是不能感知的。

另外，所有 Pod 上都要声明 resources。对于没有声明 resources 的 Pod，它被调度到某个节点后，Kubernetes 也不会在对应节点上扣除该 Pod 使用的资源。该现象可能会导致节点上调度走过多的 Pod。



## 启动时等待下游服务，不要直接退出
有时应用可能会有一些外部依赖，例如，需要从数据库读取数据或依赖另一个服务的接口。应用在启动时，外部依赖未必都能满足。手动运维时，通常会采取依赖不满足则立即退出的方式，即 failfast。但在 Kubernetes 中，该策略不再适用，Kubernetes 中多数运维操作都是自动的，无需人工介入。例如部署应用，您不用自行选择节点，再到节点上启动应用，若应用 fail 后也无需手动重启，Kubernetes 会自动重启应用，还支持通过 HPA 自动扩容。

您可通过以下示例理解“依赖不满足”：
假设有两个应用 A 和 B，A 依赖 B，且均运行在同一节点上。此节点因某些原因重启，重启后 A 应用首先启动，此时 B 应用尚未启动。对于 A 应用，该场景即为“依赖不满足”。若 A 按照传统方式直接退出，则当 B 启动后，A 也不会再启动了，需人工介入来解决。
Kubernetes 对于此问题提供了启动时依赖检查，如果依赖不满足则轮询等待，而不是直接退出。详情请参见 [Init Container](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/?spm=a2c4g.11186623.2.22.74b21c9b5UM79r#what-can-init-containers-be-used-for)。


## 配置 restart policy
Pod 运行过程中进程退出是个很常见的问题，代码里的 bug 或占用内存太多，都会导致应用进程及 Pod 退出。针对此问题，您可在 Pod上配置 restartPolicy，Pod 退出后的自动启动。示例如下：
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: tomcat
spec:
  containers:
  - name: tomcat
    image: tomcat
    restartPolicy: OnFailure # 
```
restartPolicy 可选值如下：
- **Always**： 总是自动重启。
- **OnFailure**：异常退出才自动重启 （进程退出状态非0）。
- **Never**：从不重启。


## 配置 Liveness Probe 及 Readiness Probe
Pod 处于 Running 状态和 Pod 能正常提供服务是完全不同的概念，一个 Running 状态的 Pod，可能会因其中的进程发生了死锁而无法提供服务。但由于 Pod 还处于 Running 状态，Kubernetes 不会自动重启该 Pod。针对此问题，我们需在所有的 Pod 上配置 Liveness Probe，以探测 Pod 是否真的存活且能提供服务。如果 Liveness Probe 发现了问题，Kubernetes 会重启该 Pod。


Readiness Probe 用于探测 Pod 是否可对外提供服务。应用启动过程中需要一些时间完成初始化，在此过程中是无法对外提供服务的，通过 Readiness Probe，可以通知 Ingress 或者 Service 是否能把流量转发到该 Pod 上。当 Pod 出现问题的时候，Readiness Probe 可避免新流量继续转发到该 Pod。示例如下：
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: tomcat
spec:
  containers:
  - name: tomcat
    image: tomcat
    livenessProbe:
      httpGet:
        path: /index.jsp
        port: 8080
      initialDelaySeconds: 3
      periodSeconds: 3
    readinessProbe:
      httpGet:
        path: /index.jsp
        port: 8080
```

## 每个进程一个容器
很多容器服务的初学者通常将容器当做虚拟机（VM）使用，在一个容器中存放多个进程。例如，监控进程、日志进程、sshd 进程、甚至整个 Systemd。该操作会存在以下问题：
- 判断 Pod 整体的资源占用会变复杂，无法便捷进行 [声明每个 Pod 的 resource](#resource) 步骤。
- 若容器内仅有一个进程，且该进程异常退出，外部容器引擎可感知此问题，会对容器进行重启。若容器内有多个进程，某个进程异常退出，此时外部容器引擎无法感知容器内问题，也不会对容器进行操作，但实际上该容器已无法正常工作了。

如果有几个进程需要协同工作，在 Kubernetes 中也可以实现。例如，nginx 和 php-fpm，通过 Unix domain socket 通信，我们可以用一个包含两个容器的 Pod，unix socket 放在两个容器的共享 volume 中。


## 确保不存在 SPOF（Single Point of Failure）
如果应用只有一个实例，当实例失败的时候，即使 Kubernetes 能够重启实例，但是其中仍不可避免地存在一段时间的不可用。甚至在更新应用或发布一个新版本的时候，也会出现这种情况。在 Kubernetes 里，尽量避免直接使用 Pod，尽可能使用 Deployment/StatefulSet，并且让应用的 Pod 在两个以上。
