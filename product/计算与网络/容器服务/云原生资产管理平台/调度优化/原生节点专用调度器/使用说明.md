

本文向您介绍如何使用原生节点专用调度器。

## 安装原生节点专用调度器
1. 登录 [容器服务控制台](https://console.cloud.tencent.com/tke2/cluster?rid=8)。
2. 在左侧选择**云原生资产管理 > Node Map**。
3. 在“Node Map”页面中，鼠标悬浮到页面下方某一个 Node 上，单击“详情”。
4. 在该 Node 的详情页的右上角，打开“原生节点专用调度器”开关。
![](https://qcloudimg.tencent-cloud.cn/raw/4ba513112b247e1714ad149d2bc92b5c.png)

>! 注意：该能力为全局能力，即一个集群只需要开启一次即可，其他节点详情页里的开关会同步开启/关闭。该组件仅针对 [原生节点](https://cloud.tencent.com/document/product/457/78197) 生效，若原生节点资源不足，容易引发 Pod Pending。

## 使用原生节点专用调度器

### 设置节点放大系数和水位线

1. 登录 [容器服务控制台](https://console.cloud.tencent.com/tke2/cluster?rid=8)。
2. 在左侧选择**云原生资产管理 > Node Map**。
3. 在“Node Map”页面中，鼠标悬浮到页面下方某一个需要进行节点放大的原生节点上上，单击“详情”。
4. 在该原生节点的详情页的右上角，单击原生节点专用调度器右侧的“编辑”。
5. 在原生节点专用调度器编辑页面，设置节点放大系数和水位线。如下图所示：
![](https://qcloudimg.tencent-cloud.cn/raw/e524a2924da3cb0bd4e95090e64071a0.png)
	- **规格放大**：虚拟放大原生节点规格，调度更多的 Pod，让当前节点装箱率突破 100%。
		- **节点 CPU 放大系数**：支持输入1～3之间的数字，保留小数点后一位。
		- **节点内存放大系数**：支持输入1～2之间的数字，保留小数点后一位。
	>! 当前节点规格放大系数仅会虚拟放大当前[原生节点](https://cloud.tencent.com/document/product/457/78197)的规格，该功能有一定的风险：若节点上调度过多的 Pod，且 Pod 真正使用的总资源量大于节点规格，会引发 Pod 的驱逐和重新调度。
	- **调度时水位控制**：设定当前原生节点目标资源利用率，保障稳定性。Pod 调度时，高于该水位的节点将不被选中。
		- **调度时 CPU 目标利用率**：支持输入0～100之间的整数。
		- **调度时内存目标利用率**：支持输入0～100之间的整数。
	- **运行时水位控制**：设定当前原生节点目标资源利用率，保障稳定性。节点运行时，高于该水位的节点可能引发驱逐，前提需要在指定工作负载上标识可驱逐的标签。
		- **运行时 CPU 目标利用率**：支持输入0～100之间的整数。注意：运行时 CPU 目标利用率要大于调度时 CPU 目标利用率
		- **运行时内存目标利用率**：支持输入0～100之间的整数。注意：运行时内存目标利用率要大于调度时内存目标利用率

### 设置指定命名空间的 Pod 调度至原生节点

除了设置当前原生节点上放大系数和水位控制参数之外，原生节点专用调度器还支持设置指定命名空间下的 Pod 调度到原生节点上，帮助用户自动化迁移 Pod，充分利用原生节点的优势。

1. 登录 [容器服务控制台](https://console.cloud.tencent.com/tke2/cluster?rid=8)。
2. 在“集群管理”页面单击目标集群 ID，进入集群详情页。
3. 选择左侧菜单栏中的组件管理，进入 “组件列表” 页面。
4. 在“组件列表”页面中选择"cranescheduler"（原生节点专用调度器），单击**更新配置**。
5. 选择命名空间。指定命名空间下的Pod 只能调度到原生节点上，若 [原生节点](https://cloud.tencent.com/document/product/457/78197) 资源不足，会引发 Pod Pending。


## 节点规格放大能力说明

若指定了节点的 CPU 和内存的放大系数，您可以查看原生节点上和放大系数相关的 Annotation：`expansion.scheduling.crane.io/cpu`, `expansion.scheduling.crane.io/memory`，示例如下：

```yaml
kubectl describe node 10.8.22.108

...
Annotations:        expansion.scheduling.crane.io/cpu: 1.5	# CPU 放大系数
                    expansion.scheduling.crane.io/memory: 1.2	# 内存放大系数
...
Allocatable:
  cpu:                1930m	# 该节点原始可调度资源量
  ephemeral-storage:  47498714648
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             1333120Ki
  pods:               253
...
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests         Limits
  --------           --------         ------
  cpu                960m (49%)       8100m (419%)	# 该节点 Request 和 Limit 占用量
  memory             644465536 (47%)  7791050368 (570%)
  ephemeral-storage  0 (0%)           0 (0%)
  hugepages-1Gi      0 (0%)           0 (0%)
  hugepages-2Mi      0 (0%)           0 (0%)
...
```

**说明：**
- 当前节点原始 CPU 可调度量：1930m
- 节点上所有 Pod 得总 CPU Request 申请量 960m
- 正常情况下，该节点只能最多调度 1930m-960m=970m CPU
- 但实际上该节点 CPU 的可调度量已经被虚拟放大成：1930m*1.5=2895m；实际剩余 CPU 可调度资源为：2895-960 =1935m

此时，创建一个工作负载，只有一个 Pod，CPU 申请量为 1500m，如果没有节点放大的能力，则无法调度到该节点。

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: default
  name: test-scheduler
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      nodeSelector:	# 指定节点调度
        kubernetes.io/hostname: 10.8.20.108	# 指定使用被样例中的原生节点
      containers:
      - name: nginx
        image: nginx:1.14.2
        resources:
          requests:
            cpu: 1500m	# 申请量大于放大之前的节点可调度量，但有小于放大之后的节点可调度量
        ports:
        - containerPort: 80
```

该工作负载创建成功：

```yaml
% kubectl get deployment 
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
test-scheduler     1/1     1            1           2m32s

```

再次检查节点的资源占用情况：

```yaml
kubectl describe node 10.8.22.108

...
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests         Limits
  --------           --------         ------
  cpu                2460m (127%)     8100m (419%)	# 该节点 Request 和 Limit 占用量。可以看到，Request 总和超过了节点原始可调度量，节点规格放大成功。
  memory             644465536 (47%)  7791050368 (570%)
  ephemeral-storage  0 (0%)           0 (0%)
  hugepages-1Gi      0 (0%)           0 (0%)
  hugepages-2Mi      0 (0%)           0 (0%)
```
