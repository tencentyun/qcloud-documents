## 数据组织
我们将Chunk server上的存储空间划分为若干固定大小的block，它是空间分配回收的基本单位；用户看到的统一逻辑地址空间，就是由分布在不同chunk server上的block构成；chunk server负责分配block，保存用户数据。

如果整个系统的路由都以block为粒度进行管理，势必对Master和Client内存造成巨大压力(PB级别的系统，其路由条目可到GB级别)，为了优化路由，大多数分布式存储系统都会在block基础上增加一个逻辑管理层，我们称为partition。

partion是路由和故障恢复的基本单位，每个partion包含若干block；通过引入partition，路由条目可缩小到MB级别。

![](//mccdn.qcloud.com/static/img/07dad25d196a1511bd8b46174ca8eb3d/image.png)

## 一致性哈希
**一致性哈希，解决节点故障。**

在分布式系统中，节点故障是常态，如何降低节点故障对用户访问的影响，以及减少故障恢复所需要搬迁的数据量，是分布式系统设计首要需要考虑的问题。一致性hash算法能有效解决这两个问题，所以一致性hash在分布式系统的设计中，有着广泛的应用。

为了让用户访问均衡，我们将hash ring划分为多个hash node，hash node对应一组partition group(包含3个partition)，不同的hash node可能对应相同的partition group。

- 首先，每块云盘都有独一无二的diskid，diskid与用户请求的blockid(逻辑地址空间上的id)号，可以构成一个独一无二的key，用来标识某个block；
- 然后，使用该key可以从一致性hash环上定位到具体的hash node，从而找到该block所属的partition group，该partition group包含了三份数据所属chunk server的ip，磁盘信息；
- 最后，client根据该信息将请求forward到指定的chunk server上。

![](//mccdn.qcloud.com/static/img/6b29e258c2bb0c2008f1d2e19a9b7517/image.png)

## 多副本冗余
**多副本冗余，保障数据可靠性。**

由于我们采用了三份副本存储，任何一份数据异常，另外两份数据必须是可以用的，这就要求三份数据严格一致。

分布式系统写多份通常有两种方法：
从client直接写多份到存储副本；
client先写到主副本，再由主副本流水同步到从副本。

第一种方式，写延时最小，但是由于client写三份数据，带来的网络流量压力较大；第二种方式，client流量压力最小，但是写延时相对较高。

我们采用了折衷的方案，client先写主副本，同时写两份从副本，当从副本更新成功后，向用户返回写成功。原则是写三份成功再返回用户成功。对于读请求，直接向主副本请求数据。

![](//mccdn.qcloud.com/static/img/1724b5db3c838602c6e4bb71093c09f8/image.png)

## 快速恢复
**增量恢复策略，快速解决副本故障。**

分布式系统中，副本故障是常态，在故障期间，分布式系统对外提供的是降级服务，如何快速完成数据恢复，是衡量存储系统好坏的重要标准。

导致副本故障的原因很多，有存储设备自身硬件问题，也有网络问题，或者驱动，自身程序的bug等。

若对于所有故障采用相同恢复方式的话，恢复代价较高，比如：对于4T的sata盘，受限磁盘带宽，按照50MB/s的恢复时间，大概需要花费23小时，如果是整机故障，则恢复代价更高。

实际上除了硬件故障外，软件或者网络故障其实都可以在短时间内恢复，而在这个时间内发生的数量修改并不多。

我们采用增量恢复技术来加快用户数据恢复，所谓增量恢复，即故障节点恢复后，只向主节点同步故障期间的数据修改即可，这个数据量是远远小于全量恢复的。

![](//mccdn.qcloud.com/static/img/fb13ace73a2e0eb5dac28f78bd95b8f8/image.png)

**增量恢复策略**：
迁移恢复的最小逻辑单位是partition，最小物理单位是block，每个block维护自身的seq号，每次更新，seq号会自增，用户新的写请求依然会同时写到三个副本(包括故障恢复副本)，但是故障副本的新写入数据只能写在内存cache，不能刷新到磁盘，后台进程负责比较故障副本上block与主副本block的seq，相同的则跳过恢复，不同的则恢复到故障副本磁盘。

![](//mccdn.qcloud.com/static/img/1f7135c4c04fba632ebd4ae9c9ec8167/image.png)

## 数据迁移
**数据迁移策略，解决不可逆硬件故障。**

对于不可逆的硬件故障，无法进行快速恢复，而更换物理设备的时间又会比较长，这个时候可能我们需要进行全量的数据迁移，即将故障partition的所有数据迁移到另外三份正常的partition。

Master负责分配三份目的partition，用户新请求写到主节点后，同时写到另一份从副本和目的主副本，然后目的主副本负责同步到两份目的从副本，后台进程负责将存量数据从源主副本同步到目的副本。

这里需要考虑后台进程与用户写请求的互斥，具体实现原理与快速恢复相同。

![](//mccdn.qcloud.com/static/img/086b806102327a1e874df22f413339a9/image.png)

## 快照
**快照用于数据容灾；应用快照回滚功能，可实时回滚数据。**

快照指的是数据集合在某个时间点（拷贝开始的时间点）的完整拷贝或者镜像，当生产系统数据丢失时，可通过快照完整的恢复到快照时间点，是一种重要的数据容灾手段。

磁盘快照是把磁盘某个时刻的数据冻结起来，形成一个副本，需要的时候可以随时回溯到这个副本上的数据。对于用户数据的加固，或者定期备份是个非常有用的系统。

**快照存储**：
快照存储有两个方案， 一个是把快照数据和磁盘存储数据放一起，一个是把快照数据存到第三方系统。

第一个办法的优点是用户的快照创建和回滚比较快，但是达不到真正的容灾效果。第二种办法能够达到真正的容灾效果，但是数据搬迁过程会耗一定的时间和带宽。考虑快照的核心诉求点是达到真正的容灾，我们选择第二种办法。

我们将快照数据储存在与生产系统隔离的、不同存储引擎的TFS系统中，除了在硬件上容灾外，还可以在软件和运营层面容灾。

**快照创建**：
用户创建快照后，我们立即启动后台进程，以block为单位，将用户数据从生产系统拷贝到快照系统，为了保证用户数据的时间点一致，快照设计采用了多版本和Cow（Copy on Write）技术。

用户创建一次快照，用户的写入数据版本就会自增，并分配新的block进行保存，避免对原来数据进行修改，从而保证数据的时间点一致。

由于快照分配也是以block为粒度，而用户写不一定覆盖整个block块，所以需要在用户第一次写时，将旧block数据拷贝的新block后，再实施修改。后台进程会负责将所有的旧block拷贝到快照系统。

所以，创建快照是一个瞬间过程，但整个快照数据的备份则需要耗费一定的时间，时间长短由备份的带宽决定。

![](//mccdn.qcloud.com/static/img/de1ff31e363ad1e7fac17298403e1575/image.png)

**快照回滚**：
通过读写的trigger机制实时回滚数据。

用户一旦使用快照实施回滚，则立即启动回滚进程实施数据的搬迁，且用户在此时即可使用该快照数据。

后台通过bitmap记录完成搬迁的block块，当用户请求时，先检查对应的block是否已经完成回滚动作。若未完成，则先阻塞用户请求，优先触发对该block的回滚，完成后再执行用户请求，从而保证用户能实时使用回滚数据。

![](//mccdn.qcloud.com/static/img/f359c850d50a92c328f114ea7e525c8d/image.png)

**使用快照创建磁盘**：
原理与快照回滚类似，提供快速、批量克隆磁盘的能力，满足批量服务器部署的需求。

## 持续数据保护
**持续数据保护（CDP系统，Continuous Data Protection）用于消除快照的RPO窗口，保障数据容灾结果的完整性。**

快照虽然能有效快速的将数据恢复到某个时间点，但毕竟存在一定的RPO窗口，其窗口内丢失的部分数据，可能就是一家企业的生命线。

为了彻底消除RPO窗口，我们加入了CDP机制。其实现原理非常简单，即所有的用户修改请求，除了应用到生产系统，也会旁路到CDP系统。当用户线上系统出现数据损坏时，可以通过重放请求，帮助用户恢复到任意时刻的数据。

我们结合快照在CDP系统中保留7天的修改数据，从而保证用户能恢复到7天内任意时刻数据，彻底解决用户数据丢失的后顾之忧。

![](//mccdn.qcloud.com/static/img/b31fd3844df86ad4973899f0f5ad0a88/image.png)

