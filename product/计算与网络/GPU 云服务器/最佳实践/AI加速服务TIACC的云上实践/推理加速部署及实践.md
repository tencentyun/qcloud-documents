目前 TI-ACC 处于公测阶段，您可参考下文在 GPU 云服务器实例上部署及使用 TI-ACC 推理加速。


## 使用要求[](id:explanation)
- [申请](https://cloud.tencent.com/apply/p/vl6fzemdq1) 加速产品私有镜像仓库临时登录指令，获取 password。
- TI-ACC 推理加速仅支持以下操作系统、Python版本、设备类型、框架版本及镜像版本：
  - 操作系统：Linux
  - Python 版本：Python 3.6
  - 设备类型：GPU 实例，并支持 CUDA 10.0、10.2、11.1
  - 框架版本：Tensorflow1.15，PyTorch 1.7.1、1.8.1、1.9.0
  - 镜像版本：
<table>
<thead>
<tr>
<th>框架类型</th>
<th>仓库地址</th>
<th>镜像版本</th>
</tr>
</thead>
<tbody>
<tr>
<td rowspan=5>PyTorch</td>
<td rowspan=6>tiacc-test.tencentcloudcr.com/ti-acc/ti-accv1.0</td>
<td>tiacc-inference-v1.0.0-torch1.7.1-cu102-py36-ubuntu18.04</td>
</tr>
<tr>
<td>tiacc-inference-v1.0.0-torch1.8.1-cu102-py36-ubuntu18.04</td>
</tr>
<tr>
<td>tiacc-inference-v1.0.0-torch1.8.1-cu111-py36-ubuntu18.04</td>
</tr>
<tr>
<td>tiacc-inference-v1.0.0-torch1.9.0-cu102-py36-ubuntu18.04</td>
</tr>
<tr>
<td>tiacc-inference-v1.0.0-torch1.9.0-cu111-py36-ubuntu18.04</td>
</tr>
<tr>
<td>TensorFlow</td>
<td>tiacc-inference-v1.0.0-tensorflow1.15-cu100-py36-ubuntu18.04</td>
</tr>
</tbody></table>


## 操作步骤

### 实例环境准备

1. 参考 [购买 NVIDIA GPU 实例](https://cloud.tencent.com/document/product/560/30211) 创建实例。其中：
   - **镜像**：请选择 Ubuntu 18.04及以上版本。并勾选“后台自动安装GPU驱动”，使用自动安装功能安装 GPU 驱动。
CUDA 及 cuDNN 的自动安装非本次部署的必选项，您可根据实际情况选择。如下图所示：
![](https://main.qcloudimg.com/raw/9450ebf911ca9b88a847c2a299fcc2cc.png)
   - **系统盘**：结合 Docker 镜像的大小以及训练中间状态文件的存储，推荐配置100G以上的系统盘。
2. 请对应实例的操作系统类型，参考以下文档安装 Docker。
<table>
<thead>
<tr>
<th>操作系统</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>CentOS</td>
<td>参考 <a href="https://docs.docker.com/engine/install/centos/">Docker 官方文档 - 在 CentOS 中安装 Docker</a> 进行安装。</td>
</tr>
<tr>
<td>Ubuntu</td>
<td>参考 <a href="https://docs.docker.com/engine/install/ubuntu/">Docker 官方文档 - 在 Ubuntu 中安装 Docker</a> 进行安装。</td>
</tr>
</tbody></table>
3. 安装 nvidia-docker，详情请参见 [NVIDIA 官方文档 - 安装nvidia-docker](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker)。
4. 登录实例，并依次执行命令获取加速镜像。本文提供如下参考命令：
   - [使用要求](#explanation) 中已申请的私有镜像临时登录指令，登录腾讯云容器服务 Docker Registry 加速镜像仓库。
```plaintext
docker login tiacc-test.tencentcloudcr.com --username xxx --password xxx
```
<dx-alert infotype="explain" title="">
username 为账号 ID，password 为临时登录指令。请结合实际情况进行替换。
</dx-alert>
   - 从 TI-ACC 加速镜像仓库拉取镜像，参考命令如下。
```plaintext
docker pull tiacc-test.tencentcloudcr.com/ti-acc/ti-accv1.0:[tag]
```
<dx-alert infotype="explain" title="">
tag 为镜像版本，请参考 [使用要求](#explanation) 中支持的镜像版本。
</dx-alert>
5. 执行命令，启动加速镜像并进入容器实例。本文提供参考命令如下：
```plaintext
docker run -it --net=host --privileged --rm --gpus all --shm-size=32g --ulimit memlock=-1 --ulimit stack=67108864 --name TI-ACC-inference-gpu -d tiacc-test.tencentcloudcr.com/ti-acc/ti-accv1.0:[tag]
```


### 使用推理加速
1. 执行以下命令，引入部分推理加速包。
```plaintext
import tiacc_inference
```
2. 通过模型定义和训练，获取 `input_model`。
3. 执行以下命令，对完成训练的 `input_model` 进行推理加速。
```plaintext
output_model,optimized_report = tiacc_inference.optimize(input_model,optimization_level,device_type,input_shapes,input_nodes_names,output_nodes_names,test_data,save_path,optimization_config)
```

### 推理实测效果
本文硬件环境以 GPU 实例 GN7.2XLARGE32 为例，获取的推理加速实测数据如下表所示。TI-ACC 优化过的模型包括但不限于以下模型。

<table>
<tr>
<th >模型</th>
<th>batch</th>
<th>torchscript(ms)</th>
<th>TI-ACC(ms)</th>
<th>加速比</th>
</tr>
<tr>
<td rowspan=2>resnet50<br>(torchvision)<br>224x224</td>
<td>1</td>
<td>5.4622</td>
<td>1.1482</td>
<td>4.8x</td>
</tr>
<tr>
<td>8</td>
<td>27.062 </td>
<td>4.5707</td>
<td>5.9x</td>
</tr>
<tr>
<td rowspan=2>resnest50<br>(mmcls)<br>224x224</td>
<td>1</td>
<td>7.7667</td>
<td>4.3958</td>
<td>1.8x</td>
</tr>
<tr>
<td>8</td>
<td>36.806</td>
<td>14.1152</td>
<td>2.6x</td>
</tr>
<tr>
<td rowspan=2>centernet<br>640x640</td>
<td>1</td>
<td>20.9992</td>
<td>4.7775</td>
<td>4.4x</td>
</tr>
<tr>
<td>8</td>
<td>170.5488</td>
<td>34.3523</td>
<td>5.0x</td>
</tr>
<tr>
<td rowspan=2> yolov3<br>(ultralytics)<br>640x640</td>
<td>1</td>
<td>47.19</td>
<td>10.3671</td>
<td>4.5x</td>
</tr>
<tr>
<td>8</td>
<td>302.983</td>
<td>82.6971</td>
<td>3.7x</td>
</tr>
<tr>
<td> Cascade Mask R-CNN<br>(mmdet)<br>2016x3008</td>
<td>1</td>
<td>600.0671</td>
<td>165.8467</td>
<td>3.6x</td>
</tr>
<tr>
<td> Faster R-CNN<br>(mmdet)<br>1088x800</td>
<td>1</td>
<td>107.3483</td>
<td>35.5021</td>
<td>3.0x</td>
</tr>
<tr>
<td>Vision Transformer<br>224x224</td>
<td>8</td>
<td>28.887</td>
<td>10.53</td>
<td>2.7x</td>
</tr>
<tr>
<td>Wide & Deep<br>(NVIDIA DeepLearningExamples)</td>
<td>512</td>
<td>15.7</td>
<td>4.436</td>
<td>3.5x</td>
</tr>
<tr>
<td>DeepFM<br>(NVIDIA DeepLearningExamples)</td>
<td>512</td>
<td>12.91</td>
<td>4.51</td>
<td>2.9x</td>
</tr>
</table>

## 推理加速接口说明

### 输入参数及说明

<table>
<tr>
<th>参数</th>
<th>类型</th>
<th>是否必填</th>
<th>参数说明</th>
<th>默认值</th>
<th>示例</th>
</tr>
<tr>
<td>input_model</td>
<td>多种<br>(STRING+torch.jit.ScriptModule 类型+GraphDef 类型)</td>
<td>是</td>
<td>输入待优化的原始模型。<br>
Pytorch 模型格式支持以下格式：
<ul style="margin-bottom:0px">
<li>torch.jit.ScriptModule 导出的模型文件，以 <code>.pt</code> 及 <code>.pth</code> 为后缀（计算图结构+参数的模型）。
</li>
<li>torch.jit.ScriptModule 对象。</li>
</ul><br>
Tensorflow 模型，支持以下格式：
<ul style="margin-bottom:0px">
<li>SavedModel 方式保存的模型文件夹路径。
</li>
<li>Frozen Graph 方式保存的模型文件，以 <code>.pb</code> 为后缀。
</li>
<li>GraphDef 对象。</li>
</ul>
</td>
<td>无</td>
<td>字符串格式的路径，<code>./lzz</code>、<code>./lzz.pb</code>。</td>
</tr>
<tr>
<td>optimization_level</td>
<td>INT</td>
<td>是</td>
<td>推理加速的优化级别。<br>0：无损。1：FP16。</td>
<td>无</td>
<td>0</td>
</tr>
<tr>
<td>device_type</td>
<td>INT</td>
<td>是</td>
<td>运行设备。<br>0：GPU。</td>
<td>无</td>
<td>0</td>
</tr>
<tr>
<td>input_shapes</td>
<td>多种<br>(LIST[STRING]+LIST[LIST)]+LIST[DICT])</td>
<td>是</td>
<td>
模型输入的相关信息，主要包括形状 shape 和类型 data_type。最外层为 list，其中每个元素代表一个输入节点 input，每个 input 允许 str,list,dict 的格式，允许嵌套：
<ul style="margin-bottom:0px">
<li>str 用于单一节点固定输入尺寸。</li>
<li>list 用于多节点固定尺寸。</li>
<li>dict 用于输入非固定尺寸或者非 float 类型，对应的 key 为 'seperate|range,int|float|half'。seperate 静态则  value 填写 range。动态则 value 填写 min 和 max。</li>
</ul>
</td>
<td>无</td>
<td>
<ul style="margin-bottom:0px">
<li>单一节点固定尺寸：[ '1*3*224*224' ]</li>
<li>多节点固定尺寸: [[ '1*3*224*224' ],[ '1*3*512*512' ]]</li>
<li>单节点非固定静态尺寸：[{'seperate': ['1*3*224*224','2*3*224*224']}]</li>
<li>单节点非固定动态尺寸：[{'range': ['1*3*224*224','10*3*224*224']}]</li>
</ul>
</td>
</tr>
<tr>
<td>inputs_nodes_names</td>
<td>LIST[STRING]</td>
<td>否</td>
<td>原始模型的输入节点。如果不指定该参数，则系统尝试自动推断。</td>
<td>None</td>
<td>[“lzz_input1”,“lzz_input2”]</td>
</tr>
<tr>
<td>outputs_nodes_names</td>
<td>LIST[STRING]</td>
<td>否</td>
<td>原始模型的输出节点。如果不指定该参数，则系统尝试自动推断。</td>
<td>None</td>
<td>[“lzz_output”]</td>
</tr>
<tr>
<td>test_data</td>
<td>多种<br>(LIST[DICT[STRING, np.ndarray]]+LIST[Tuple[torch.tensor, ]])</td>
<td>否</td>
<td>
用于模型推理速度对比的测试数据。<br>
用户给出则使用给出的测试数据。如果用户未给出，TI-ACC 将自动提供测试数据。<br>对于不同类型的模型，其测试数据格式存在差异，PyTorch 模型的测试数据为若干组输入 Tensor Tuple，类型为 LIST[Tuple[torch.tensor, ]]。TensorFlow 模型的测试数据为包含若干组feed_dict的列表，类型为LIST[DICT[STRING, np.ndarray]]。
</td>
<td>None</td>
<td>/</td>
</tr>
<tr>
<td>save_path</td>
<td>STRING</td>
<td>否</td>
<td>优化模型的保存路径。如果需要将优化后的模型进行保存，则必填。
<br> PyTorch 模型格式，则支持 torch.jit.ScriptModule 保存方式，即填写导出的模型文件，以 <code>.pt</code> 或 <code>.pth</code>为后缀。<br>
如果是 Tensorflow 模型格式，则支持以下方式：
<ul style="margin-bottom:0px">
<li>savemodel 保存方式，即填写模型文件夹路径。</li>
<li>frozen graph 保存方式，即填写保存的模型文件名。</li>
</ul>
</td>
<td>None</td>
<td><code>../lzz</code>、<code>../lzz.pb</code>、<code>../lzz.pbt</code>
</td>
</tr>
<tr>
<td>optimization_config</td>
<td>tiacc 中自定义的 OptimizeConfig 类</td>
<td>否</td>
<td>用于指导模型优化 tf.nn.embedding_lookup_sparse 层。设置参数时，需包含前缀 "tiemb/"。参数包括：
<ul style="margin-bottom:0px">
<li>最大 batch 数 (max_batch_size, int类型)</li>
<li>slot 数 (slot_num, int类型)</li>
<li>最大 nnz 数 (max_nnz, int类型)</li>
<li>gpu cache 使用率 (cache_percentage, float类型)</li>
<li>cpu cache 使用率 (cpu_cache_percentage, float类型)</li>
<li>gpu cache 命中率阈值 (hit_rate_threshold, float类型)</li>
<li>是否开启分布式模式 (enable_distributed, bool类型)</li>
</ul>
此外还具备 output_names 参数，该参数类型和 output_nodes_names 相同。如果用户手动设置了 output_nodes_names，tiacc 内部自动将 names 同步到 output_names 参数中。用户应尽量使用 output_nodes_names 进行设置，而非在 config 中设置。
</td>
<td>
<ul style="margin-bottom:0px">
<li>output_names,max_batch_size, slot_num, max_nnz 无默认值</li>
<li>cache_percentage 默认值为0.2</li>
<li>cpu_cache_percentage 默认值为1.0</li>
<li>hit_rage_threshold 默认值为0.95</li>
<li>enable_distributed 默认值为 false</li>
</ul>
</td>
<td>
optimization_config = tiacc_tf.OptimizeConfig()
optimization_config.parameter_map["tiemb/max_batch_size"].i = batch_size
optimization_config.parameter_map["tiemb/slot_num"].i = slot_num
optimization_config.parameter_map["tiemb/max_nnz"].i = max_nnz
optimization_config.parameter_map["tiemb/cache_percentage"].f = 0.2
optimization_config.parameter_map["tiemb/cpu_cache_percentage"].f = 1.0
optimization_config.parameter_map["tiemb/hit_rate_threshold"].f = 0.95
optimization_config.parameter_map["tiemb/enable_distributed"].b = False

</td>
</tr>
</table>


### 输出参数及说明
<table>
<tr>
<th>参数</th>
<th>类型</th>
<th>参数说明</th>
</tr>
<tr>
<td>output_model</td>
<td>多种<br>(GraphDef 类型+torch.nn.Module 类型)</td>
<td>优化后的模型，输出与 input_model 输入模型相同类型的模型对象。</td>
</tr>
<tr>
<td>optimized_report</td>
<td>OptimizedReport</td>
<td>输出的优化报告。</td>
</tr>
</table>

optimized_report 示例如下：
```
optimized_report: {
  // 软件环境，包括框架、CUDA。
  "software_environment": [
    {
      "software": "tensorflow",
      "version": "1.15.0"
    },
    {
      "software": "cuda",
      "version": "9.0.176"
    },
    {
      "software": "cuDNN",
      "version": "7.4"
    },
    {
      "software": "TensorRT",
      "version": "5"
    }
  ],
  // 硬件环境。
  "hardware_environment": {
    "device_type": "gpu"
    "microarchitecture": "V100"
  },
  // 测试数据信息。
    "test_data_info": {
    "test_data_source": "user provided/tiacc provided",
    "test_data_shape": " (1, 9240)" ,
	"test_data_type": " int32"
  },
  // 优化结果。
"optimization_result": {
    "baseline": "10.00 ms",   
    "optimized": "4.38 ms",   
    "speedup": "2.28"         
  }
}
```

### 输出报告字段含义


<table>
<tr>
<th>字段</th>
<th>子字段</th>
<th>说明及示例</th>
</tr>
<tr>
<td>software_environment</td>
<td>software、version</td>
<td>软件环境。包括模型训练框架+版本、CUDA+版本、cudnn+版本、tensorRT+版本。
<br>示例如下：
<pre style="color:white">
  "software_environment": [
    {
      "software": "tensorflow",
      "version": "1.15.0"
    },
    {
      "software": "cuda",
      "version": "9.0.176"
    },
    {
      "software": "cuDNN",
      "version": "7.4"
    },
    {
      "software": "TensorRT",
      "version": "5"
    }
  ]
</pre>
</td>
</tr>
<tr>
<td>hardware_environment</td>
<td>device_type、microarchitecture</td>
<td>硬件环境。包括 device_type 类型以及显卡类型。
<br>示例如下：
<pre style="color:white">
 "hardware_environment": {
    "device_type": "gpu"
    "microarchitecture": "V100"
  }
</pre>
</td>
</tr>
<tr>
<td>test_data_info</td>
<td>test_data_source、test_data_shape、test_data_type</td>
<td>测试数据信息，包括测试数据来源和测试数据的形状和类型。数据来源有两类：如果用户本身提供了测试数据，则为 user provided。如果用户没有提供，则为 tiacc provided，即 tiacc 自动提供的测试数据。
<br>示例如下：
<pre style="color:white">
 "test_data_info": {
    "test_data_source": "user provided/tiacc provided",
    "test_data_shape": " (1, 9240)" ,
    "test_data_type": " int32"
  }
</pre>
</td>
</tr>
<tr>
<td>optimization_result</td>
<td>baseline_time、optimized_time、baseline_qps、optimized_qps、speedup</td>
<td>优化结果，包括原始模型的平均时延 baseline、优化后的模型的平均时延 optimized、原始模型的 qps、优化后的模型的 qps，以及加速比 speedup。
<br>示例如下：
<pre style="color:white">
 "optimization_result": {
    "baseline_time": "10.00 ms",   
    "optimized_time": "4.38 ms",   
    "baseline_qps": "100",   
    "optimized_qps": "228",   
    "speedup": "2.28"          
  }
</pre>
</td>
</tr>
</table>
