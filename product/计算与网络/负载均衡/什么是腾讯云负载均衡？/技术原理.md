## 底层实现
腾讯云负载均衡 CLB 当前提供四层和七层上的负载均衡服务，由腾讯发起的 Tencent gateway（TGW） 统一网关产品提供负载均衡能力，具有可靠性高、扩展性强、性能高、抗攻击能力强等特点，可支持大规模的并发访问，并防止恶意攻击的流量。CLB 的产品形态包括：提供负载均衡能力，收敛外网 IP，抗 DDoS 攻击，QoS，支持 FTP、SIP 等能力。
负载均衡采用集群部署，可实现会话同步，以消除服务器单点，提升冗余，保证服务稳定。已在同一个地域部署多个机房，实现同城容灾。
本文档主要介绍 CLB 的几个关键技术点，从平台纬度、资源纬度到业务纬度分析 CLB 通过哪些技术来实现云平台的租户隔离、架构容灾、资源容灾、抗攻击等能力。
>?
>- RS 指绑定 CLB 负载均衡器的 CVM 云服务器。
>- VIP 指 CLB 负载均衡器对外服务的 IP 地址。

### 租户隔离
云平台一个关键的能力就是租户隔离，接入云的用户都希望自己的业务不受别人的影响，至少在网络上能够隔离，别人不能随意访问我的机器。当然这个功能业界有多种实现方式，比较常用的是在硬件上做隔离，使用特定的接入交换机，采用 vxlan 协议来达到隔离的目的。但是存在以下缺点：
- 需要使用特殊的交换机。
- 需要额外的设备打通 vxlan 网络和普通网络，这样就存在单点问题。
- 不易于与现有的网络环境兼容。

基于以上原因，我们采用了软件的方案，通过 IP 隧道 + VPC 的方式来实现租户隔离。
![](//mccdn.qcloud.com/static/img/cf8f46731a218bf7fef43843eef0d4e4/image.png)
从图中左侧我们可以看到，CLB 和 RS 之间的交互采用的 IP 隧道方式，云服务器 CVM（RS）分配实际的内网 IP，和物理网络是打通的。这样做的好处是实现简单，能够兼容以前的物理机方案，但是存在以下缺点：
- 需要额外的模块实现租户间隔离。
- 租户之间内网 IP 无法重用，无法实现自由组网。
- 由于 IP 在内网唯一，迁移时必须变换 RS 的 IP，因此也无法实现热迁移。

基于以上原因，腾讯云开发出 VPC。在上图右侧可以看到同一个租户分配一个 VPCID，在 VPC 内，客户可以自由组网，租户网络本身就是隔离的，具体的处理由 vpc.ko 内核模块来实现。

### IP 收敛
提起负载均衡，业界最著名的非 LVS（Linux Virtual Server）技术莫属。LVS 共有以下模式：
- DR 模式：主要的限制是要求 LVS 和 RS 必须在同一个 vlan，部署起来限制很大，并且可扩展性较差。
- NAT 模式：NAT 模式主要的缺陷在于 RS 的回包需要使用默认路由，同样存在扩展性的问题。因此我们最初搭建 LVS 集群采用的是 TUNNEL 模式。
- TUNNEL 模式：要求每台 RS 都需要分配外网 IP。对于腾讯云以及 RS 较多的业务，外网 IP 是一个很大的挑战。

基于以上原因， CLB 应运而生。

下图是之前使用的LVS方案和CLB方案的一个简图，主要区别在于：
- CLB 不需要给 RS 分配外网 IP，起到收敛 IP 的作用。
- 出流量仍然通过 CLB，更易于定位问题，另外对于业务流量形成闭环，起到桥头堡的作用。
![](//mccdn.qcloud.com/static/img/e4575f5f414666505d8c1a7cdea2c6f0/image.png)

### 高可靠实现
高可靠是衡量云服务的一个重要指标。CLB 可实现以下方案：为您提供高可靠的服务，
- [集群容灾](#jqrz)
- [session 同步](#jqrz)
- [资源隔离](#zygl)
- [抗 DDos 攻击](#kgj)

<span id="jqrz"></span>
#### 集群容灾和 session 同步
集群容灾就是一个集群中一台服务器宕机不会影响整个集群的服务能力。传统的集群容灾采用的 HA 模式（vrrp 协议），常见的开源方案如 LVS，传统方式的缺点在于：一个集群同时只有一半的服务器在工作，另外一半的机器处于冷备状态；宕机之后的切换速度相对较慢。
CLB 在设计之初就考虑到这个问题，采用 ospf 动态路由协议来实现集群的容灾，若一台机器发生故障，ospf 协议可以保证在10s以内把机器从集群中剔除。CLB 一个集群放在两个接入交换机下，并且保证跨机架的容灾，这样保证在即便有单边的交换机出故障或者单边机架掉电时，本集群的服务不受影响。
集群容灾保证 CLB 集群的可用性，但是对于一个客户来讲，如果服务器宕机，即便该机器被剔除了，也只能保证新的连接不会落在这台机器，那么长连接就会断掉。为了解决这个问题，我们实现了集群内 session 连接定期同步。这样在别的服务器接管故障机器的包时，能够正确找到 session，保证提供正常服务。
![](//mccdn.qcloud.com/static/img/4cdd6084a39561e04539a8866374bb24/image.png)

LVS 的做法：连接状态变化时同步；在长短连接并存的情况下，短连接业务的同步流量非常大，会对正常转发造成冲击。
CLB 解决方案：每个连接创建5秒后才同步，连接如果在5秒内，则不同步；只同步长连接。
![](//mccdn.qcloud.com/static/img/397479668381a345c8bae877e4aa4ff3/image.png)

<span id="zygl"></span>
#### 资源隔离

资源隔离功能，是为了在个别业务受到攻击时，CLB 出现高负载的情况下，保护别的业务不会受到影响。

具体实现是：定期（5s）检测 CLB 是否达到配置的高负载的警戒线，如果达到了则开启资源隔离，CLB 将检测每个业务的流量、包量、连接数，超过上限的就丢弃，保证 CLB 服务器不会达到真正的高负载而影响别的业务。在正常情况下，资源隔离功能是关闭的；只有当业务突增放量，或者某个业务被攻击导致 CLB 达到警戒线时，保证在5s以内开启该功能，保证正常的业务转发不受影响。

资源隔离采用经典的令牌桶算法，工作过程是：定期向桶中放置一定量的令牌，每到一个包，消耗一个令牌，在一段时间内令牌耗尽，则开始丢包。
![](//mccdn.qcloud.com/static/img/86cd36ef04f3200b8d0c591b0c4e7675/image.png)

<span id="kgj"></span>
#### 抗 DDoS 攻击

集群容灾和资源隔离都是为了保护 CLB 平台自身，而对于单个业务，如果被攻击，则受伤害的概率就是100%，CLB 是不允许这种情况发生的。当然对于 DDos 攻击，腾讯云有非常强大的大禹系统来保护业务，但是大禹系统的宙斯盾的检测时长是10s，那么在大禹系统生效之前，可能客户的 RS 已经被压垮。为了解决这10s内的问题，我们开发了 synproxy 的功能。

具体实现是：CLB 在接收到客户端的三步握手请求时，代理三步握手，在数据包到来之前，不会打扰到 RS，一旦第一个包到来，CLB 将其缓存，此时再和 RS 进行三步握手，握手成功之后，将缓存的数据包发送给 RS，之后的流程就透传数据包了。这样保证 DDos 攻击不会到达 RS，而是由 CLB 来承担压力。CLB 本身承载能力比较强，又是集群模式，同时又具备资源隔离的能力，所以一般情况下，很难在10s以内把 CLB 机器压垮。
![](//mccdn.qcloud.com/static/img/5c96f1c2548dd15bd00d0ff01b63eddf/image.png)

## 腾讯云负载均衡不同类型的特点

腾讯云 CLB 负载均衡器目前提供了四层和七层协议的转发能力：
- 四层负载均衡，对应监听器的 TCP 和 UDP；
- 七层负载均衡，对应监听器的 http/https 能力。

### 四层负载均衡

四层负载均衡是 CLB 最早实现的方案，也是作为一款负载均衡产品必备的功能。基本原理就是在 CLB 上通过端口来区分不同的业务，转发规则（rule）的key：vip:vport:protocol，目前 QCloud 中使用最多的就是这种负载均衡方式，但是在 QCloud 中，VIP 是属于同一个开发商的，不同开发商之间的流量严格隔离。
![](//mccdn.qcloud.com/static/img/bb969f908e3931c61267c316e6e4f909/image.png)

### 七层负载均衡

无日租型的方案能够应对普通的七层负载均衡服务，但是对于有 session 和 cookie 需求的七层用户，就得自己搭建自己的 nginx 来在做一层反向代理，不但浪费，可靠性也会受到影响。
![](//mccdn.qcloud.com/static/img/6d385cd8c23ca392c36540eff8689e5c/image.png)

七层负载均衡在设计之初讨论了两种方案：
1. 外网 IP 直接起在 nginx 机器上，搭建 nginx 集群
2. nginx 集群接在四层 CLB 之下

方案1存在的弱点是对 DDos 攻击束手无策，对于腾讯云来说，需要一个 VIP 可同时接入四层和七层，而方案1无法做到，因此最终我们采取了方案2。此外方案2还有一个优势是可以很方便的动态扩缩容，这样利于应对业务要求快速扩容的场景。

但是，nginx 本身是通过反向代理来实现负载均衡功能的，在腾讯云上存在一个致命的问题：由于 VPC 网络是虚拟网络，和物理网络之间是通过宿主机来打通的，七层 LD 和 RS 之间没办法直接使用 nginx 的反向代理功能。因此，我们想到了模拟四层，在七层 LD 上插入了 l7.ko 内核模块，用于负责封装 gre 隧道和 IP 隧道和 RS 之间交互。
![](//mccdn.qcloud.com/static/img/9874ed32509218619ef4cea119bc3790/image.png)
