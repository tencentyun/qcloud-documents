## 底层实现

腾讯云负载均衡CLB当前提供四层和七层上的负载均衡服务，由腾讯发起的Tencent gateway（TGW）统一网关产品提供负载均衡能力，具有可靠性高、扩展性强、性能高、抗攻击能力强等特点，可支持大规模的并发访问，并防止恶意攻击的流量。CLB的产品形态包括：提供负载均衡能力，收敛外网IP，抗DDoS攻击，QoS，支持FTP、SIP等能力。

负载均衡采用集群部署，可实现会话同步，以消除服务器单点，提升冗余，保证服务稳定。已在同一个地域部署多个机房，实现同城容灾。

本章主要介绍CLB的几个关键技术点，从平台纬度、资源纬度到业务纬度分析CLB通过哪些技术来实现云平台的租户隔离、架构容灾、资源容灾、抗攻击等能力。
> 注：本文RS指绑定CLB负载均衡器的CVM虚拟机。VIP指CLB负载均衡器对外服务的IP地址。

### 租户隔离

云平台一个关键的能力就是租户隔离，接入云的用户都希望自己的业务不受别人的影响，至少在网络上能够隔离，别人不能随意访问我的机器。当然这个功能业界有多种实现方式，比较常用的是在硬件上做隔离，使用特定的接入交换机，采用vxlan协议来达到隔离的目的。这么做的缺点是：
1. 需要使用特殊的交换机；
2. 需要额外的设备打通vxlan网络和普通网络，这样就存在单点问题；
3. 不易于与现有的网络环境兼容。
基于以上原因，我们采用了软件的方案，通过IP隧道+VPC的方式来实现租户隔离。

![](//mccdn.qcloud.com/static/img/cf8f46731a218bf7fef43843eef0d4e4/image.png)

从图中左侧我们可以看到，CLB和Rs之间的交互采用的IP隧道方式，虚拟机CVM（RS）分配实际的内网IP，和物理网络是打通的。这样做的好处是实现简单，能够兼容以前的物理机方案，但是缺点在于：
1. 需要额外的模块实现租户间隔离；
2. 租户之间内网IP无法重用，无法实现自由组网；
3. 由于IP在内网唯一，迁移时必须变换Rs IP，因此也无法实现热迁移。

基于以上原因，腾讯云开发出VPC私有网络。在上图右侧可以看到同一个租户分配一个VPCID，在VPC内，客户可以自由组网，租户网络本身就是隔离的，具体的处理由vpc.ko内核模块来实现。

### IP收敛

提起负载均衡，业界最著名的非LVS（Linux Virtual Server）技术莫属。

LVS共有三种模式：DR模式、NAT模式及TUNNEL模式。其中DR模式主要的限制是要求LVS和RS必须在同一个vlan，部署起来限制很大，并且可扩展性较差；NAT模式主要的缺陷在于RS的回包需要使用默认路由，同样存在扩展性的问题；因此我们最初搭建LVS集群采用的是TUNNEL模式。然而TUNNEL模式要求每台RS都需要分配外网IP。对于腾讯云以及RS较多的业务，外网IP是一个很大的挑战。

基于以上原因， CLB应运而生。

下图是之前使用的LVS方案和CLB方案的一个简图，主要区别在于：
1. CLB不需要给RS分配外网IP，起到收敛IP的作用；
2. 出流量仍然通过CLB，更易于定位问题，另外对于业务流量形成闭环，起到桥头堡的作用。
![](//mccdn.qcloud.com/static/img/e4575f5f414666505d8c1a7cdea2c6f0/image.png)

### 高可靠实现

高可靠是衡量云服务的一个重要指标。CLB为了实现高可靠的服务，我们进行了以下的方案实现：
1. 集群容灾；
2. session同步；
3. 资源隔离；
4. 抗DDos攻击。

#### 集群容灾和session同步

集群容灾，简单来说就是一个集群中一台服务器倒掉不会影响整个集群的服务能力。传统的集群容灾采用的HA模式（vrrp协议），常见的开源方案如LVS，传统方式的缺点在于：一个集群同时只有一半的服务器在工作，另外一半的机器处于冷备状态；死机之后的切换速度相对较慢。

CLB在设计之初就考虑到这个问题，采用ospf动态路由协议来实现集群的容灾，若一台机器倒掉，ospf协议可以保证在10s以内把机器从集群中剔除。CLB一个集群放在两个接入交换机下，并且保证跨机架的容灾，这样保证在即便有单边的交换机出故障或者单边机架掉电时，本集群的服务不受影响。

集群容灾保证CLB集群的可用性，但是对于一个客户来讲，如果服务器倒掉，即便该机器被剔除了，也只能保证新的连接不会落在这台机器，那么长连接就会断掉。为了解决这个问题，我们实现了集群内session连接定期同步。这样在别的服务器接管故障机器的包时，能够正确找到session，保证提供正常服务。

![](//mccdn.qcloud.com/static/img/4cdd6084a39561e04539a8866374bb24/image.png)

LVS的做法：连接状态变化时同步；在长短连接并存的情况下，短连接业务的同步流量非常大，会对正常转发造成冲击。
CLB解决方案：每个连接创建5秒后才同步，连接如果在5秒内，则不同步；只同步‘长连接’。

![](//mccdn.qcloud.com/static/img/397479668381a345c8bae877e4aa4ff3/image.png)

#### 资源隔离

资源隔离功能，是为了在个别业务受到攻击时，CLB出现高负载的情况下，保护别的业务不会受到影响。

具体实现是：定期（5s）检测CLB是否达到配置的高负载的警戒线，如果达到了则开启资源隔离，CLB将检测每个业务的流量、包量、连接数，超过上限的就丢弃，保证CLB服务器不会达到真正的高负载而影响别的业务。在正常情况下，资源隔离功能是关闭的；只有当业务突增放量，或者某个业务被攻击导致CLB达到警戒线时，保证在5s以内开启该功能，保证正常的业务转发不受影响。

资源隔离采用经典的令牌桶算法，工作过程是：定期向桶中放置一定量的令牌，每到一个包，消耗一个令牌，在一段时间内令牌耗尽，则开始丢包。

![](//mccdn.qcloud.com/static/img/86cd36ef04f3200b8d0c591b0c4e7675/image.png)

#### 抗DDoS攻击

集群容灾和资源隔离都是为了保护CLB平台自身，而对于单个业务，如果被攻击，则受伤害的概率就是100%，CLB是不允许这种情况发生的。当然对于DDos攻击，腾讯云有非常厉害的大禹系统来保护业务，但是大禹系统的宙斯盾的检测时长是10s，那么在大禹系统生效之前，可能客户的RS已经被压垮。为了解决这10s内的问题，我们开发了synproxy的功能。

具体实现是：CLB在接收到客户端的三步握手请求时，代理三步握手，在数据包到来之前，不会打扰到RS，一旦第一个包到来，CLB将其缓存，此时再和RS进行三步握手，握手成功之后，将缓存的数据包发送给RS，之后的流程就透传数据包了。这样保证DDos攻击不会到达RS，而是由CLB来承担压力。CLB本身承载能力比较强，又是集群模式，同时又具备资源隔离的能力，所以一般情况下，很难在10s以内把CLB机器压垮。

![](//mccdn.qcloud.com/static/img/5c96f1c2548dd15bd00d0ff01b63eddf/image.png)

## 腾讯云负载均衡不同类型的特点

腾讯云CLB负载均衡器目前提供了3种服务类型：
- 4层负载均衡，对应监听器的tcp和udp；
- 7层负载均衡，公网无日租型CLB服务；
- 7层负载均衡，对应监听器的http/https能力。

### 4层负载均衡

4层负载均衡是CLB最早实现的方案，也是作为一款负载均衡产品必备的功能。基本原理就是在CLB上通过端口来区分不同的业务，转发规则（rule）的key：vip:vport:protocol，目前QCloud中使用最多的就是这种负载均衡方式，但是在QCloud中，VIP是属于同一个开发商的，不同开发商之间的流量严格隔离。

![](//mccdn.qcloud.com/static/img/bb969f908e3931c61267c316e6e4f909/image.png)

### 公网无日租型负载均衡

无日租型最初是为了应对大量的网页、手机端游戏需求，这类游戏有2个主要特点：
1、分区分服，每个区至少需要一个vip，对于个别有几百个服的游戏，就需要几百个外网vip，造成外网vip的极大浪费；
2、网页游戏都需要使用80端口，4层负载均衡没有办法复用IP。

为了解决这个问题，我们提出了无日租型的方案，具体的接入方式是在rule key中增加一个domain纬度。

![](//mccdn.qcloud.com/static/img/e9d4ed8a62b76264f811d6b0dbf24c2e/image.png)

### 7层负载均衡

无日租型的方案能够应对普通的7层负载均衡服务，但是对于有session和cookie需求的7层用户，就得自己搭建自己的nginx来在做一层反向代理，不但浪费，可靠性也会受到影响。

![](//mccdn.qcloud.com/static/img/6d385cd8c23ca392c36540eff8689e5c/image.png)

7层负载均衡在设计之初讨论了2种方案：
1. 外网IP直接起在nginx机器上，搭建nginx集群
2. nginx集群接在4层CLB之下

方案1存在的弱点是对DDos攻击束手无策，对于腾讯云来说，需要一个VIP可同时接入4层和7层，而方案1无法做到，因此最终我们采取了方案2。此外方案2还有一个优势是可以很方便的动态扩缩容，这样利于应对业务要求快速扩容的场景。

但是，nginx本身是通过反向代理来实现负载均衡功能的，在腾讯云上存在一个致命的问题：由于vpc网络是虚拟网络，和物理网络之间是通过母机来打通的，7层LD和RS之间没办法直接使用nginx的反向代理功能。因此，我们想到了模拟4层，在7层LD上插入了l7.ko内核模块，用于负责封装gre隧道和IPIP隧道和RS之间交互。

![](//mccdn.qcloud.com/static/img/9874ed32509218619ef4cea119bc3790/image.png)



