## 版本支持
本页文档所描述功能，在腾讯云视立方中支持情况如下：

| 版本名称 | 基础直播 Smart | 互动直播 Live | 短视频 UGSV | 音视频通话 TRTC | 播放器 Player | 全功能 |
| -------- | -------- | -------- | -------- | -------- | -------- | -------- |
| 支持情况 | -  | -  | -  | &#10003;  | -  | &#10003;  |
| SDK 下载 <div style="width: 90px"/> | [下载](https://vcube.cloud.tencent.com/home.html?sdk=basicLive) | [下载](https://vcube.cloud.tencent.com/home.html?sdk=interactivelive) | [下载](https://vcube.cloud.tencent.com/home.html?sdk=shortVideo) | [下载](https://vcube.cloud.tencent.com/home.html?sdk=video) | [下载](https://vcube.cloud.tencent.com/home.html?sdk=player) | [下载](https://vcube.cloud.tencent.com/home.html?sdk=allPart) |

不同版本 SDK 包含的更多能力，具体请参见 [SDK 下载](https://cloud.tencent.com/document/product/1449/56978)。

## 内容介绍

- **自定义视频采集**
  如果您自研（或者购买第三方）美颜和特效处理模块，则需要自己采集和处理摄像头拍摄画面，您可以通过 TRTCCloud 的 `enableCustomVideoCapture` 接口关闭音视频通话 TRTC SDK 自己的摄像头采集和图像处理逻辑。然后您可以使用 `sendCustomVideoData` 接口向音视频通话 TRTC SDK 填充您自己的视频数据。
- **自定义视频渲染**
  音视频通话 TRTC SDK 使用 OpenGL 进行视频画面的渲染，如果您是用在游戏开发中，或者需要在自己的界面引擎中嵌入音视频通话 TRTC SDK，那么就要自己渲染视频画面。
- **自定义音频采集**
  如果您是在特殊硬件设备上使用音视频通话 TRTC SDK，当需要外接声音采集设备并自己采集声音数据时，您可以通过 TRTCCloud 的 `enableCustomAudioCapture` 接口关闭音视频通话 TRTC SDK 默认的声音采集流程。然后您可以使用 `sendCustomAudioData` 接口向 TRTC SDK 填充您自己的声音数据。
>!开启自定义音频采集后，有可能会导致回声抵消（AEC）的功能失效。
- **获取音频原数据**
  声音模块是一个高复杂度的模块，SDK 需要严格控制声音设备的采集和播放逻辑。在某些场景下，当您需要获取远程用户的音频数据或者需要获取本地麦克风采集到的音频数据时，可以通过音视频通话 TRTC SDK 提供的相应的回调接口来实现。

## 支持的平台

|   iOS    | Android  |  Mac OS  | Windows  | 微信小程序 | Web 端 |
| :------: | :------: | :------: | :------: | :--------: | :-----------: |
| &#10003; | &#10003; | &#10003; | &#10003; |     ×      |       ×       |

## 自定义视频采集

您可以通过 TRTCCloud 的 `enableCustomVideoCapture` 接口关闭 TRTC SDK 自己的摄像头采集和图像处理逻辑。然后您可以使用 `sendCustomVideoData` 接口向 TRTC SDK 填充您自己的视频数据。

在 `sendCustomVideoData` 接口中有一个叫 `TRTCVideoFrame` 的参数，它表示一帧视频画面。为了避免不必要的性能损失，对于输入音视频通话 TRTC SDK 的视频数据，在不同平台上有不同的格式要求，具体要求如下：

<dx-tabs>
::: iOS\s平台
TRTC SDK 支持 NV12 和 i420 两种 iOS 版本的 YUV 数据格式。在 iOS 平台上，比较高性能的图像传递方式是 CVPixelBufferRef，因此我们建议参数格式如下：

|  参数名称   |       参数类型       |                      推荐取值                       |                           备注说明                           |
| :---------: | :------------------: | :-------------------------------------------------: | :----------------------------------------------------------: |
| pixelFormat | TRTCVideoPixelFormat |              TRTCVideoPixelFormat_NV12              |       iOS 平台上摄像头原生采集出的视频格式即是 NV12。        |
| bufferType  | TRTCVideoBufferType  |                     PixelBuffer                     |            iOS 中原生支持的视频帧格式，性能最佳。            |
| pixelBuffer |   CVPixelBufferRef   | 如果 TRTCVideoBufferType 是 PixelBuffer，则需填写。 |     iPhone 摄像头采集的数据是 NV12 格式的 PixelBuffer。      |
|    data     |       NSData\*       |   如果 TRTCVideoBufferType 是 NSData，则需填写。    |                    性能不如 PixelBuffer。                    |
|  timestamp  |       uint64_t       |                          0                          | 可以填0，这样 SDK 会自定填充 timestamp 字段，但请**均匀**地控制 sendCustomVideoData 的调用间隔。 |
|    width    |       uint64_t       |                   视频画面的宽度                    |                请严格填写传入画面的像素宽度。                |
|   height    |       uint32_t       |                   视频画面的高度                    |                请严格填写传入画面的像素高度。                |
|  rotation   |  TRTCVideoRotation   |                       不填写                        | <ul style="margin:0"><li/>默认不填写。 <li/>如果需要对画面进行旋转，可以填写`TRTCVideoRotation_0`、`TRTCVideoRotation_90`、`TRTCVideoRotation_180`、`TRTCVideoRotation_270`。SDK  会根据这个值将视频顺时针旋转对应角度。例如一个竖屏画面，传入`TRTCVideoRotation_90`后，SDK 会旋转成横屏显示。</ul> |

#### 示例代码
在 [Demo](https://github.com/tencentyun/TRTCSDK/tree/master/iOS/TRTC-API-Example-OC/Advanced/LocalVideoShare) 文件夹中有一个叫做 `LocalVideoShareViewController.m` 的文件，它展示了如何从一个本地视频文件中读取出 NV12 格式的 PixelBuffer，并通过 SDK 进行后续处理。

```objectiveC
//组装一个 TRTCVideoFrame 并将其送给 trtcCloud 对象
TRTCVideoFrame* videoFrame = [TRTCVideoFrame new];
videoFrame.bufferType = TRTCVideoBufferType_PixelBuffer;
videoFrame.pixelFormat = TRTCVideoPixelFormat_NV12;
videoFrame.pixelBuffer = imageBuffer;
videoFrame.rotation = rotation;
videoFrame.timestamp = timeStamp;
        
[trtcCloud sendCustomVideoData:videoFrame];      
```
:::
::: Android\s平台
Android 平台有以下两种方案：
- **buffer 方案**：对接起来比较简单，但是性能较差，不适合分辨率较高的场景。
buffer 方案要求直接向音视频通话 TRTC SDK 塞入 byte[] 格式的数组，支持 i420 和 NV21 两种 YUV 格式。
<table>
<thead><tr><th>参数名称</th><th>参数类型</th><th>推荐取值</th><th>备注说明</th></tr></thead>
<tbody><tr>
  <td>pixelFormat</td>
  <td>int</td>
  <td>TRTC_VIDEO_PIXEL_FORMAT_I420 或<br> TRTC_VIDEO_PIXEL_FORMAT_NV21</td>
  <td>i420 和 NV21 是两种不同的 YUV 数据格式。</td>
</tr><tr>
  <td>bufferType</td>
  <td>int</td>
  <td>TRTC_VIDEO_BUFFER_TYPE_BYTE_ARRAY</td>
  <td>指明使用 data 的方式传递 YUV 数据。</td>
</tr><tr>
  <td>texture</td>
  <td>TRTCTexture</td>
  <td>buffer 方案不填写</td>
  <td>如果选择 i420 或者 NV21 两种格式，则此参数不填写。</td>
</tr><tr>
  <td>data</td>
  <td>byte[]</td>
  <td>YUV 格式的数据 buffer</td>
  <td>内部包装的是 Java 类型的内存，适合在 Java 层使用。</td>
</tr><tr>
  <td>buffer</td>
  <td>ByteBuffer</td>
  <td>buffer 方案不填写</td>
  <td>内部包装的是 C/C++ 类型的内存，适合在 JNI 层使用。</td>
</tr><tr>
  <td>width</td>
  <td>uint64_t</td>
  <td>视频画面的宽度</td>
  <td>请严格填写传入画面的像素宽度。</td>
</tr><tr>
  <td>height</td>
  <td>uint32_t</td>
  <td>视频画面的高度</td>
  <td>请严格填写传入画面的像素高度。</td>
</tr><tr>
  <td>timestamp</td>
  <td>long</td>
  <td>视频帧的采集时间</td>
  <td>可以填0，这样 SDK 会自定填充 timestamp 字段，但请<strong>均匀</strong>地控制 sendCustomVideoData 的调用间隔。</td>
</tr><tr>
  <td>rotation</td>
  <td>int</td>
  <td>不填写</td>
  <td><li>默认不填写。 </li><li>如果需要对画面进行旋转，可以填写0、90、180、270。SDK  会根据这个值将视频顺时针旋转对应角度。例如一个竖屏画面，传入90后，SDK 会旋转成横屏显示。</li></td>
</tr></tbody></table>

- **texture 方案**：对接需要一定的 OpenGL 基础，但是性能较好，尤其是画面分辨率较高的时候。
texture 需要向音视频通话 TRTC SDK 中传递 OpenGL 纹理，为了保证该方案能够正常运作，需要您提前设置好 OpenGL 环境，因此该方案的对接难度非常高。如果您没有学习过 OpenGL，建议直接使用我们提供的示例代码，它在 [Demo](https://github.com/tencentyun/TRTCSDK/blob/master/Android/TRTC-API-Example/Advanced/LocalVideoShare/src/main/java/com/tencent/trtc/mediashare) 中的 `mediashare` 文件夹下，包含以下文件：
<table><thead><tr><th>文件名</th><th>源码逻辑</th></tr></thead>
<tbody>

<tr><td>LocalVideoShareActivity.java</td>
<td>演示如何通过 TRTCloud 的 sendCustomVideoData 函数向 SDK 投喂视频纹理以及 通过 TRTCloud 的 sendCustomAudioData 函数向 SDK 投喂音频数据。</td>
</tr><tr>

<tr><td>Utils.java</td>
<td>封装了获取文件路径及媒体格式的函数。</td>
</tr><tr>

<td>helper</td>
<td>封装了 OpenGL 环境以及读取视频文件数据的函数，使其更好用一些。</td>
</tr><tr>

</tbody></table>

>! 对于 640 x 360 以上的分辨率，我们建议使用 texture 方案，以避免过高的 CPU 使用率。


#### 示例代码
`LocalVideoShareActivity.java` 中的代码原理比较复杂：
1. 代码首先初始化了 `mVideoFrameReadListener` 的成员变量，等待视频数据回调。
2. 代码接下来使用了一个叫 MediaFileSyncReader 的模块，调用 `start` 方法从一个本地视频文件中读取出一帧帧的视频画面。
3. 然后将视频数据通过 `onFrameAvailable` 回调，在这个回调函数里，我们将获得的 texture 通过 sendCustomVideoData 函数传递给 SDK：


<dx-codeblock>
::: java java
public void onFrameAvailable(EGLContext eglContext, int textureId, int width, int height, long timestamp) {
          TRTCCloudDef.TRTCVideoFrame videoFrame = new TRTCCloudDef.TRTCVideoFrame();
          videoFrame.texture = new TRTCCloudDef.TRTCTexture();
          videoFrame.texture.textureId = textureId;
          videoFrame.texture.eglContext14 = eglContext;
          videoFrame.width = width;
          videoFrame.height = height;
          videoFrame.timestamp = timestamp;
          videoFrame.pixelFormat = TRTCCloudDef.TRTC_VIDEO_PIXEL_FORMAT_Texture_2D;
          videoFrame.bufferType = TRTCCloudDef.TRTC_VIDEO_BUFFER_TYPE_TEXTURE;

          mTRTCCloud.sendCustomVideoData(videoFrame);
      }
:::
</dx-codeblock>
:::
::: Windows\s平台
在 Windows 平台上，仅支持 TRTCVideoPixelFormat_I420，我们建议参数格式如下：
<table>
<thead><tr><th>参数名称</th><th>参数类型</th><th>推荐取值</th><th>备注说明</th>
</tr></thead><tbody>
<tr>
  <td>videoFormat</td>
  <td>TRTCVideoPixelFormat</td>
  <td>TRTCVideoPixelFormat_I420</td>
  <td>仅支持TRTCVideoPixelFormat_I420。</td>
</tr><tr>
  <td>bufferType</td>
  <td>TRTCVideoBufferType</td>
  <td>TRTCVideoBufferType_Buffer</td>
  <td>仅支持 TRTCVideoBufferType_Buffer。</td>
</tr><tr>
  <td>textureId</td>
  <td>int</td>
  <td>不需要填写</td>
  <td>Windows平台不支持纹理</td>
</tr><tr>
  <td>data</td>
  <td>char *</td>
  <td>视频帧 buffer</td>
  <td>一帧I420数据。</td>
</tr><tr>
  <td>timestamp</td>
  <td>uint64_t</td>
  <td>0</td>
  <td>可以填0，这样 SDK 会自定填充 timestamp 字段，但请<strong>均匀</strong>地控制 sendCustomVideoData 的调用间隔。</td>
</tr><tr>
  <td>width</td>
  <td>uint64_t</td>
  <td>视频画面的宽度</td>
  <td>请严格填写传入画面的像素宽度。</td>
</tr><tr>
  <td>height</td>
  <td>uint32_t</td>
  <td>视频画面的高度</td>
  <td>请严格填写传入画面的像素高度。</td>
</tr><tr>
  <td>rotation</td>
  <td>TRTCVideoRotation</td>
  <td>画面旋转角度</td>
  <td><li>默认不填写。 </li><li>如果需要对画面进行旋转，可以填写<code>TRTCVideoRotation_0</code>、<code>TRTCVideoRotation_90</code>、<code>TRTCVideoRotation_180</code>、<code>TRTCVideoRotation_270</code>。SDK  会根据这个值将视频顺时针旋转对应角度。例如一个竖屏画面，传入<code>TRTCVideoRotation_90</code>后，SDK 会旋转成横屏显示。</li></td>
</tr></tbody></table>

#### 示例代码
在 [Demo](https://github.com/tencentyun/TRTCSDK/blob/master/Windows/DuilibDemo/sdkinterface/TRTCCloudCore.cpp) 文件中有一个叫做 `sendCustomVideoFrame` 的函数，它展示了如何从一个本地视频文件中读取出 I420 格式的 buffer，并通过 SDK 进行后续处理。

```C++
//组装一个 TRTCVideoFrame 并将其送给 trtcCloud 对象
TRTCVideoFrame frame;
frame.videoFormat = LiteAVVideoPixelFormat_I420;
frame.length = bufferSize;
frame.data = _video_buffer;
frame.width = _video_width;
frame.height = _video_height;
m_pCloud->sendCustomVideoData(TRTCVideoStreamTypeBig, & frame);      
```
:::
</dx-tabs>





## 自定义视频渲染

音视频通话 TRTC SDK 使用 OpenGL 进行视频画面的渲染，如果您是用在游戏开发中，或者需要在自己的界面引擎中嵌入音视频通话 TRTC SDK，那么就要自己渲染视频画面。

<dx-tabs>
::: iOS\s平台（包括\sMac）
通过 TRTCCloud 的 `setLocalVideoRenderDelegate` 和 `setRemoteVideoRenderDelegate` 可以设置本地和远程画面的自定义渲染回调，相关参数如下：

|  参数名称   |       参数类型       |            推荐取值             |                备注说明                |
| :---------: | :------------------: | :-----------------------------: | :------------------------------------: |
| pixelFormat | TRTCVideoPixelFormat |    TRTCVideoPixelFormat_NV12    |                   -                    |
| bufferType  | TRTCVideoBufferType  | TRTCVideoBufferType_PixelBuffer | iOS 中原生支持的视频帧格式，性能最佳。 |

[](id:example_ios)
#### 示例代码
如果 pixelFormat 选择了  TRTCVideoPixelFormat_NV12，bufferType 选择了 TRTCVideoBufferType_PixelBuffer，那么整个您可以很方便地将一帧 NV12 格式的 PixelBuffer 转成一副视频画面。在 Demo 文件夹中有一个叫做 `TestRenderVideoFrame.m` 的文件，其中就用如下的示例代码展示了如何使用该方法。

```objectiveC
- (void)onRenderVideoFrame:(TRTCVideoFrame *)frame 
                                userId:(NSString *)userId 
						        streamType:(TRTCVideoStreamType)streamType
{
    //userId是nil时为本地画面，否则为远端画面
    CFRetain(frame.pixelBuffer);
    __weak __typeof(self) weakSelf = self;
    dispatch_async(dispatch_get_main_queue(), ^{
        TestRenderVideoFrame* strongSelf = weakSelf;
        UIImageView* videoView = nil;
        if (userId) {
            videoView = [strongSelf.userVideoViews objectForKey:userId];
        }
        else {
            videoView = strongSelf.localVideoView;
        }
        videoView.image = [UIImage imageWithCIImage:[CIImage imageWithCVImageBuffer:frame.pixelBuffer]];
        videoView.contentMode = UIViewContentModeScaleAspectFit;
        CFRelease(frame.pixelBuffer);
    });
}
```
:::
::: Android\s平台
通过 TRTCCloud 的 `setLocalVideoRenderListener` 和 `setRemoteVideoRenderListener` 可以设置本地和远程画面的自定义渲染回调，相关参数如下：

|  参数名称   |       参数类型       |                           推荐取值                           |                           备注说明                           |
| :---------: | :------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| pixelFormat | TRTCVideoPixelFormat |                 TRTC_VIDEO_PIXEL_FORMAT_NV21                 |             自定义渲染暂时不支持 texture 方案。              |
| bufferType  | TRTCVideoBufferType  | TRTC_VIDEO_BUFFER_TYPE_BYTE_ARRAY 或<br> TRTC_VIDEO_BUFFER_TYPE_BYTE_BUFFER | BYTE_BUFFER 适合在 jni 层使用，BYTE_ARRAY 则可用于 Java 层的直接操作。 |

[](id:example_android)
#### 示例代码
<dx-codeblock>
::: Android java
    public void onRenderVideoFrame(String userId, int streamType, final TRTCCloudDef.TRTCVideoFrame frame) {
        mEglCore.makeCurrent();
        GLES20.glViewport(0, 0, mSurfaceSize.width, mSurfaceSize.height);
        GLES20.glBindFramebuffer(GLES20.GL_FRAMEBUFFER, 0);
        GLES20.glClearColor(0, 0, 0, 1.0f);
        GLES20.glClear(GLES20.GL_DEPTH_BUFFER_BIT | GLES20.GL_COLOR_BUFFER_BIT);
        mNormalFilter.onDraw(frame.texture.textureId, mGLCubeBuffer, mGLTextureBuffer);
        mEglCore.swapBuffer();
    }
:::
</dx-codeblock>
:::
::: Windows\s平台
通过 TRTCCloud 的 setLocalVideoRenderCallback 和 setRemoteVideoRenderCallback 可以设置本地和远程画面的自定义渲染回调，相关参数如下：

|  参数名称   |         参数类型          |          推荐取值           |                           备注说明                           |
| :---------: | :-----------------------: | :-------------------------: | :----------------------------------------------------------: |
| pixelFormat |   TRTCVideoPixelFormat    | TRTCVideoPixelFormat_BGRA32 | 支持回调 TRTCVideoPixelFormat_I420 或 TRTCVideoPixelFormat_BGRA32 像素格式的视频帧。 |
| bufferType  |    TRTCVideoBufferType    | TRTCVideoBufferType_Buffer  |           目前只支持 TRTCVideoBufferType_Buffer。            |
|  callback   | ITRTCVideoRenderCallback* |  ITRTCVideoRenderCallback*  |                       自定义渲染回调。                       |

[](id:example_windows)
#### 示例代码
如果 pixelFormat 选择了  TRTCVideoPixelFormat_BGRA32，bufferType 选择了 TRTCVideoBufferType_Buffer，那么整个您可以很方便地将一帧 BGRA32 格式的 PixelBuffer 转成一幅视频画面。在 Demo 中有一个叫做 `TXLiveAvVideoView.cpp` 的文件，其中就用如下的示例代码展示了如何使用该方法。

<dx-codeblock>
::: Window c++
void TXLiveAvVideoView::renderFitMode(HDC hDC, unsigned char * buffer, int width, int height, RECT& rcImage)
{
    Point origin;
    origin.X = m_rcItem.left, origin.Y = m_rcItem.top;
    int viewWith = m_rcItem.right - m_rcItem.left;
    int viewHeight = m_rcItem.bottom - m_rcItem.top;

    bool bReDrawBg = false;
    
    if (m_bmi.bmiHeader.biWidth != width || m_bmi.bmiHeader.biHeight != height)
    {
        memset(&m_bmi, 0, sizeof(m_bmi));
        m_bmi.bmiHeader.biSize = sizeof(BITMAPINFOHEADER);
        m_bmi.bmiHeader.biWidth = width;
        m_bmi.bmiHeader.biHeight = height;
        m_bmi.bmiHeader.biPlanes = 1;
        m_bmi.bmiHeader.biBitCount = 32;
        m_bmi.bmiHeader.biCompression = BI_RGB;
        bReDrawBg = true;
    }
    //开始处理渲染
    ::SetStretchBltMode(hDC, COLORONCOLOR);
    if (bReDrawBg)
        ::PatBlt(hDC, 0 + origin.X, 0 + origin.Y, viewWith, viewHeight, BLACKNESS);
    ::StretchDIBits(hDC, x + origin.X, y + origin.Y, width, height, 0, 0, width, height, m_argbRenderFrame.frameBuf, &m_bmi, DIB_RGB_COLORS, SRCCOPY);
}
:::
</dx-codeblock>
:::
</dx-tabs>

## 自定义音频采集

您可以通过 TRTCCloud 的 enableCustomAudioCapture 接口关闭 TRTC SDK 默认的声音采集流程。然后您可以使用 sendCustomAudioData 接口向音视频通话 TRTC SDK 填充您自己的声音数据。

在 `sendCustomAudioData` 接口中有一个叫 `TRTCAudioFrame` 的参数，它表示一帧 20ms 长度的音频数据：

- 通过 `sendCustomAudioData`  向 SDK 投送的数据必须是 PCM 格式的未经压缩的音频裸数据，不可以是 AAC 或者其他的压缩格式。
- sampleRate 和 channels 分辨代表声音采样率和声道数，请保持跟传入的 PCM 数据严格一致。
- 每帧音频数据的时长推荐是20ms，我们可以计算一下，如果 sampleRate 为48000，声道数是1（单声道），那么每次调用 sendCustomAudioData 时传入的 buffer 长度应该是：48000 * 0.02s * 1 * 16bit = 15360bit = 1920字节。
- timestamp 可以为0，当 timestamp 为0时，SDK 会自动填充音频时间戳。因此，为了确保音频时间戳的稳定，请**均匀**地调用 `sendCustomAudioData`，保持20ms一次的频率，否则会导致声音出现断断续续的效果。

>! 使用 `sendCustomAudioData` 有可能会导致回声抵消（AEC）的功能失效。

## 获取音频原数据

声音模块是一个高复杂度的模块，SDK 需要严格控制声音设备的采集和播放逻辑。在某些场景下，当您需要获取远程用户的音频数据或者需要获取本地麦克风采集到的音频数据时，可以通过 TRTCCloud 对应的不同平台的接口，来给 SDK 集成三个回调函数。

<dx-alert infotype="explain" title="TRTCCloud 对应的不同平台的接口分别为：">
<ul style="margin:0"><li/><b>iOS：</b>setAudioFrameDelegate。
<li/><b>Android：</b>setAudioFrameListener。
<li/><b>Windows：</b>setAudioFrameCallback。
</ul>
</dx-alert>

| 接口                  | 说明                                                         |
| --------------------- | ------------------------------------------------------------ |
| onCapturedAudioFrame  | 获取本地麦克风采集到的音频源数据。在非自定义采集模式下，SDK 会负责麦克风的声音采集工作，但您可能也需要拿到 SDK 采集到的声音源数据，通过此回调函数就可以获取。 |
| onPlayAudioFrame      | 该函数会回调每一路远程用户的声音数据，这是混音前的数据。如果您要对某一路的语音进行语音识别，就可以用到这个回调。 |
| onMixedPlayAudioFrame | 各路音频数据混合后，在送到扬声器播放之前，会通过此函数回调出来。 |

>!
- 不要在上述回调函数中做任何耗时操作，建议直接拷贝，并通过另一线程进行处理，否则会导致声音断断续续或者回声抵消（AEC）失效的问题。
- 上述回调函数中回调出来的数据都只允许读取和拷贝，不能修改，否则会导致各种不确定的后果。





