## 1. 为什么会卡顿
![](//mc.qcloudimg.com/static/img/b41b15c344f8c34011e4ee0e55db21e5/image.png)

卡顿的原因无外乎三种情况：
- **出流不畅（A）**
主播端手机性能较差，或者使用时间太久的 Android 系统，都可能影响主播出流的流畅度。正常来说，1 秒钟15帧以上的视频流才能保证观看的流畅度，如果FPS在 10 帧以下，可以判定为出流不畅。出流不畅会导致 <font color='red'>全部观众</font> 的观看体验都很卡顿。

- **上行阻塞（B）**
就是主播的手机在源源不断地产生音视频数据，但是手机的上行带宽却不够用，导致产生的数据都被堆积在主播端推不出去了，上行阻塞会导致 <font color='red'>全部观众</font> 的观看体验都很卡顿。

- **下行不佳（C）**
就是观众的下行带宽不足，或者网络很波动。比如直播流的码率是1Mbps 的，也就是每秒钟有 1M 比特的数据流要下载下来。但如果观众端的带宽不够，就会导致观众端体验非常卡顿。 下行不佳只会影响当前网络环境下的观众。

## 2. 主播端的优化
**出流不畅** 或 **上行阻塞**，对于观看体验影响都非常大，会导致所有观众看到的视频都是卡顿的。据统计，视频云客户群 80% 以上的直播间卡顿问题，均是由于 RTMP 推流质量不佳所导致的！

那么怎么评估推流质量的好坏呢？

### 2.1 SDK 的质量监控
RTMP SDK 提供了一种状态反馈机制，每 1-2 秒就会将内部各种状态参数反馈出来，我们可以通过注册 **TXLivePushListener** 监听器来获取这些状态。

![](//mc.qcloudimg.com/static/img/48fd46af4e17b0299fd00a0e661a16f0/image.png)

|  推流状态                   |  含义说明                    |   
| :------------------------  |  :------------------------ | 
| NET_STATUS_CPU_USAGE     |当前进程的CPU使用率和本机总体的CPU使用率|
| NET_STATUS_VIDEO_WIDTH  |当前视频的宽度（单位：像素值）    |
| NET_STATUS_VIDEO_HEIGHT|当前视频的高度（单位：像素值）    |
|	NET_STATUS_NET_SPEED     | 当前的发送速度（单位：kbps）|
|	NET_STATUS_VIDEO_BITRATE | 当前视频编码器输出的比特率，也就是编码器每秒生产了多少视频数据，单位： kbps|
|	NET_STATUS_AUDIO_BITRATE | 当前音频编码器输出的比特率，也就是编码器每秒生产了多少音频数据，单位： kbps|
|	NET_STATUS_VIDEO_FPS     | 当前视频帧率，也就是视频编码器每条生产了多少帧画面|
|	NET_STATUS_CACHE_SIZE    | 音视频数据堆积情况，这个数字超过个位数，即说明当前上行带宽不足以消费掉已经生产的音视频数据|
|	NET_STATUS_CODEC_DROP_CNT  |全局丢包次数，为了避免延迟持续恶性堆积，SDK在数据积压超过警戒线以后会主动丢包，丢包次数越多，说明网络问题越严重。|
| NET_STATUS_SERVER_IP     | 连接的推流服务器的IP |

### 2.2 上行阻塞的评判
- **2.2.1 : BITRATE 与 NET_SPEED 的关系**
 BITRATE( = VIDEO_BITRATE + AUDIO_BITRATE ) 指的是编码器每秒产生了多少音视频数据要推出去，NET_SPEED 指的是每秒钟实际推出了多少数据，所以如果 BITRATE == NET_SPEED 的情况是常态，则推流质量会非常良好；而如果 BITRATE >= NET_SPEED 这种情况的持续时间比较长，推流质量就很难有什么保障。

- **2.2.2 : CACHE_SIZE 和 DROP_CNT 的数值**
BITRATE >= NET_SPEED 的情况一旦出现，编码器产生的音视频数据就会在主播的手机上积压起来，积压的严重程度以 CACHE_SIZE 这个状态值展示出来，如果 CACHE_SIZE 超过警戒线，SDK 会主动丢弃一些音视频数据，从而触发 DROP_CNT 的增长。

 下图所示就是一个典型的上行阻塞，途中 CACHE_SIZE 始终在 红色警戒线 以上，说明上行网络不足以满足数据的传输需求，也就是上行阻塞严重：

![](//mc.qcloudimg.com/static/img/319d6197da603ca15ffc6e2afd778e48/image.png)

上面看到的图表是源于我们实验测试用的内部数据分析系统，如果您有同样的分析需求，可以在[直播控制台](https://console.qcloud.com/live)的质量监控系统里看到类似的图表，这里的图表的格式更加简明，对其理解不需要太多专业的音视频基础知识。
![](//mc.qcloudimg.com/static/img/e241222c0591e6b5ffa41738a8a35d62/image.png)

### 2.3 推流不畅的评判
- **2.3.1 VIDEO_FPS 的大小** 
正常来说，1 秒钟15帧以上的视频流才能保证观看的流畅度，如果FPS在 10 帧以下，可以判定为出流不畅。出流不畅90%以上的原因都跟下面这个因素有关。

- **2.3.2 CPU_USAGE 的大小** 
RTMP SDK 的测试团队要求，在主流手机上的 CPU 使用率要控制在 50% 以下，尤其是在开启硬件编码的情况下。比如在小米3这款机型上，开启硬件加速后，720p超清画质推流也不过30%的CPU使用率。

然而，硬件配置较差的低端手机，或者服役时间太久的Android系统，都可能导致计算能力不满足 RTMP SDK 的最低要求，另一方面，一款直播 APP 中使用CPU的不可能只有RTMP SDK，弹幕、飘星、文本消息互动等等，都有可能会消耗一定的CPU，这些都是不可避免的。

如果系统的整体CPU使用率超过 80%，那么视频的采集和编码都有可能会有影响；如果CPU使用率达到 100%，那么主播端本身就已经卡的一塌糊涂了，观众端要有流畅的观看体验显然是不可能的。


### 2.4 针对性优化方案
- **2.4.1 主动提示主播**
对于注重清晰度的场景下，通过合适的 UI 交互提示主播 **“当前网络质量很糟糕，建议您拉近离WiFi的距离”** 是最好的选择。RTMP SDK 的推流功能文档中有涉及 **事件处理** 的介绍，您可以利用它来做到这一点。

 推荐的做法是：如果你的APP在短时间内连续收到  RTMP SDK  的多个 **PUSH_WARNING_NET_BUSY** 事件，则提示主播网络关注一下当前网络质量，因为对于上行阻塞这种情况而言，主播本人是没办法通过视频的表现感知到的，只能通过观众的提醒或者 APP 的提醒来了解。

- **2.4.2 启用流控辅助**
有些城市里的运营商带宽，会限制上行网速，比如最常见的限制就是 512 kbps 这一档，所以如果您要规避这种情况，推荐开启网络自适应，参考文档见 [iOS平台](https://www.qcloud.com/document/product/454/7884#4.-.E6.99.BA.E8.83.BD.E6.8E.A7.E9.80.9F) &  [Android平台](https://www.qcloud.com/document/product/454/7890#4.-.E6.99.BA.E8.83.BD.E6.8E.A7.E9.80.9F) 。

| 设置项 | 中文含义 | 建议值 |
|---------|------------|--------------|
| MinVideoBitrate | 最小码率 | 400 |
| MinVideoBitrate | 最大码率 | 1000（推荐根据具体分辨率而定） |
| AutoAdjustBitrate | 码率自适应 | 开启 |
| setAutoAdjustStrategy | 码率调整策略 | AUTO_ADJUST_BITRATE_STRATEGY_2 |


## 3. 播放端的优化
![](//mc.qcloudimg.com/static/img/9ccfcf56c0993232cc5637f306c21ba5/image.png)

### 3.1 卡顿 & 延迟
如上图，下行网络的波动或者下行带宽不沟通，都会导致在播放过程中出现一段段的**饥饿期**—— APP这段时间内拿不到可以播放的音视频数据。如果想要让观看端的视频卡顿尽量少，就要尽可能地让APP缓存足够多的视频数据，以保证它能平安度过这些“饥饿期”。

但是，APP缓存太多的音视频数据，会引入一个新的问题 —— **高延迟**，这对互动性要求高的场景是很坏的消息。同时，如果不做延迟修正和控制，卡顿引起的延迟会有**累积效应**，就是播放时间越久，延迟越高。延迟修正做得好不好是衡量一款播放器是否足够优秀的关键指标。

所以，**延迟和流畅是一架天平的两端**，如果过分强调低延迟，就会导致轻微的网络波动即产生明显的播放端卡顿。反之，如果过分强调流畅，就意味着引入大量的延迟（典型的案例就是HLS(m3u8)通过引入10-30秒的延迟来实现流畅的播放体验)。

### 3.2 针对性优化方案
为了能够让您无需了解过多流控处理知识就能优化出较好的播放体验，腾讯云 RTMP SDK 经过多个版本的改进，优化出一套自动调节技术，并在其基础上推出了三种比较优秀的延迟控制方案：

- **自动模式**：如果您不太确定您的主要场景是什么，可以直接选择这个模式。
>把 TXLivePlayConfig 中的 setAutoAdjustCache 开关打开，即为自动模式.在该模式下，播放器会根据当前网络情况，对延迟进行自动调节（默认情况下播放器会在1s - 5s 这个区间内自动调节延迟大小，您可以通过setMinCacheTime 和 setMaxCacheTime对默认值进行修改），以保证在足够流畅的情况下尽量降低观众跟主播端的延迟，确保良好的互动体验。

- **极速模式**：主要适用于**秀场直播**等互动性高，因而对延迟要求比较苛刻的场景。
> 极速模式设置方法是  **setMinCacheTime = setMaxCacheTime = 1s**  ， 而您也发现了，自动模式跟极速模式的差异只是MaxCacheTime 有所不同 （极速模式的 MaxCacheTime 一般比较低，而自动模式的MaxCacheTime 则相对较高 ），这种灵活性主要得益于SDK内部的自动调控技术，可以在不引入卡顿的情况下自动修正延时大小，而MaxCacheTime 反应的就是调节速度：MaxCacheTime的值越大，调控速度会越发保守，当然卡顿概率就会越低。
 
- **流畅模式**：主要适用于**游戏直播** 等大码率高清直播场景。
> 当把播放器中的 setAutoAdjustCache 开关关闭，即为流畅模式，在该模式下，播放器采取的处理策略跟Adobe FLASH内核的缓存出策略如出一辙：当视频出现卡顿后，会进入 loading 状态直到缓冲区蓄满，之后进入playing状态，直到下一次遭遇无法抵御的网络波动。默认情况下，缓冲大小为5s，您可以通过setCacheTime进行更改。
> 
> 在延迟要求不高的场景下，这种看似简单的模式会更加可靠，因为该模式本质上就是通过牺牲一点延迟来降低卡顿率。





