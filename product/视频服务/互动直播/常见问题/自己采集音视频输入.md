## 自定义音频数据

### 音频本地处理流程

以下是音频本地处理流程图，用户可以其中任意环节对数据进行拦截并作相应的处理。

**观众侧：**

![](https://zhaoyang21cn.github.io/iLiveSDK_Help/readme_img/audio_member.jpg)

**主播侧：**

![](https://zhaoyang21cn.github.io/iLiveSDK_Help/readme_img/audio_host.jpg)

### 自定义音频采集流程

1. 主播或上麦观众需要在腾讯云后台 spear 角色配置将音频场景设置为“**开播**”（开播场景会占用本地录音权限），也可以设置为“**观看**”（设置为“观看”不会占用本地音频设备）。
2. 进房间时 mic 和 speaker 都要打开。
3. 进房间成功后调用接口 AVAudioCtrl.hangeAudioCategory 切换至观看场景（第一步如果设置为“**观看**”场景此步省略）。
4. 调 enableExternalAudioDataInput 开启自定义采集音频。
5. 调 fillExternalAudioFrame 将外部采集的音频塞给 AVSDK。
6. 以上接口都需在主线程调用。


### Android 混音

**注册回调：**

接口名|接口描述
-- | --
registAudioDataCallbackWithByteBuffer|注册具体数据类型的回调函数

参数类型|说明
--|--
int|音频数据类型（参考上图）
RegistAudioDataCompleteCallbackWithByteBuffer|指向 App 定义的音频数据回调函数

```java
ILiveSDK.getInstance().getAvAudioCtrl().registAudioDataCallbackWithByteBuffer(
    AVAudioCtrl.AudioDataSourceType.AUDIO_DATA_SOURCE_MIXTOSEND, mAudioDataCompleteCallbackWithByffer);
```

**添加需要混入的音频数据：**

```java
private AVAudioCtrl.RegistAudioDataCompleteCallbackWithByteBuffer mAudioDataCompleteCallbackWithByffer = 
      new AVAudioCtrl.RegistAudioDataCompleteCallbackWithByteBuffer() {
        @Override
        public int onComplete(AVAudioCtrl.AudioFrameWithByteBuffer audioFrameWithByteBuffer, int srcType) {
            if (srcType==AudioDataSourceType.AUDIO_DATA_SOURCE_MIXTOSEND) {
                synchronized (obj){
                  /*************************************************
                    将要混入的音频数据写入 audioFrameWithByteBuffer 中
                  *************************************************/
                }
            }
            return AVError.AV_OK;
        }
    };
```

### iOS 混音

**音频透传**，主要用于在直播中对 Mic 采集到的数据作再加工处理，一般用于在直播间内添加背景音等，其对透传的音频数据有格式要求，默认使用的音频格式为 `QAVAudioFrameDesc = {48000, 2, 16}`。通常的有下面两种使用方法：麦克风透传和扬声器透传数据。

- **麦克风透传：**开麦克风端（有上行音频能力端）能听到，其他人可听到：以下代码为设置麦克风透传。

 ```
// 设置音频处理回调
[[[ILiveSDK getInstance] getAVContext].audioCtrl setAudioDataEventDelegate:self];
// 注意为 QAVAudioDataSource_MixToSend
[[[ILiveSDK getInstance] getAVContext].audioCtrl registerAudioDataCallback:QAVAudioDataSource_MixToSend];
[[[ILiveSDK getInstance] getAVContext].audioCtrl setAudioDataFormat:QAVAudioDataSource_MixToSend desc:pcmdesc];
```

- **扬声器透传数据：**开扬声器端配置，只有自己听到，其他人听不到：以下代码为设置扬声器透传。

 ```
 // 设置音频处理回调
 [[[ILiveSDK getInstance] getAVContext].audioCtrl setAudioDataEventDelegate:self];
 // 注意为 QAVAudioDataSource_MixToPlay
 [[[ILiveSDK getInstance] getAVContext].audioCtrl registerAudioDataCallback:QAVAudioDataSource_MixToPlay];
 [[[ILiveSDK getInstance] getAVContext].audioCtrl setAudioDataFormat:QAVAudioDataSource_MixToPlay desc:pcmdesc];
```

**回调处理：**注意三个回调中的注释。

```
- (QAVResult)audioDataComes:(QAVAudioFrame *)audioFrame type:(QAVAudioDataSourceType)type
{
    // 主要用于保存直播中的音频数据
    return QAV_OK;
}
- (void)handle:(QAVAudioFrame **)frameRef withPCM:(NSData *)data offset:(NSInteger *)offset
{
	// 演示如何将透传的数据添加到 QAVAudioFrame
    const QAVAudioFrame *aFrame = *frameRef;
    NSInteger off = *offset;
    NSInteger length = [aFrame.buffer length];
    if (length)
    {
        NSMutableData *pdata = [NSMutableData data];
        const Byte *btyes = [data bytes];
        while (pdata.length < length)
        {
            if (off + length > data.length)
            {
                const Byte *byteOff = btyes + off;
                [pdata appendBytes:byteOff length:data.length - off];
                off = 0;
            }
            else
            {
                const Byte *byteOff = btyes + off;
                [pdata appendBytes:byteOff length:length];
                off += length;
            }
        }
        if (pdata.length == length)
        {
            *offset = off;
            
            const void *abbytes = [aFrame.buffer bytes];
            memcpy((void *)abbytes, [pdata bytes], length);
        }
    }
}
- (QAVResult)audioDataShouInput:(QAVAudioFrame *)audioFrame type:(QAVAudioDataSourceType)type
{
    // 混音输入（Mic 和 Speaker）的主要回调
    
    // 麦克风透传处理
    if (type == QAVAudioDataSource_MixToSend)
    {
    	// self.micAudioTransmissionData 为要透传的音频数据，默认使用 QAVAudioFrameDesc = {48000, 2, 16}，外部传入数据时，注意对应，外部传入的时候，注意相关的参数
        if (self.micAudioTransmissionData)
        {
            NSInteger off = self.micAudioOffset;
            [self handle:&audioFrame withPCM:self.micAudioTransmissionData offset:&off];
            self.micAudioOffset = off;
        }
    }
    // 扬声明器透传处理
    else if (type == QAVAudioDataSource_MixToPlay)
    {
    // self.speakerAudioTransmissionData 为要透传的音频数据，默认同样使用 QAVAudioFrameDesc = {48000, 2, 16}，外部传入数据时，注意对应，外部传入的时候，注意相关的参数
        if (self.speakerAudioTransmissionData)
        {
            NSInteger off = self.speakerAudioOffset;
            [self handle:&audioFrame withPCM:self.speakerAudioTransmissionData offset:&off];
            self.speakerAudioOffset = off;
        }
    }
//    NSLog(@"%@", audioFrame.buffer);
    return QAV_OK;
}

- (QAVResult)audioDataDispose:(QAVAudioFrame *)audioFrame type:(QAVAudioDataSourceType)type
{
    // 主要用作作变声处理
    return QAV_OK;
}
```

**退出直播间，取消透传回调：**

```
// 取消所有音频透传处理
[[[ILiveSDK getInstance] getAVContext].audioCtrl unregisterAudioDataCallbackAll];
[[[ILiveSDK getInstance] getAVContext].audioCtrl setAudioDataEventDelegate:nil];
// 或调用 AVSDK 接口取消不同类型的透传
// 方法详见 QAVSDK.framework 中的 QAVAudioCtrl
/*!
 @abstract      反注册音频数据类型的回调
 @discussion    要注册监听的音频数据源类型，具体参考 QAVAudioDataSourceType。
 @param         type            要反注册监听的音频数据源类型，具体参考 QAVAudioDataSourceType
 @return        成功返回 QAV_OK, 其他情况请参照 QAVResult。
 @see           QAVAudioDataSourceType QAVResult
 */
- (QAVResult)unregisterAudioDataCallback:(QAVAudioDataSourceType)type;
```



## 自定义视频采集

### 自定义视频流程图

![](https://zhaoyang21cn.github.io/iLiveSDK_Help/readme_img/custom_flow.png)

### Android

- 打开摄像头，同时需要调用 enableExternalCapture 做一些准备。

- 获取原始视频数据，加工处理。接收到系统回吐出的原始数据，用户就可以对其做预处理；例如美白、美颜和人脸识别等；预处理之后的画面需要用户自己完成渲染，与 iLiveSDK 无直接联系。

接口名|接口描述
-- | --
enableExternalCapture|开启/关闭外部视频捕获设备

参数类型|说明
--|--
boolean|true 表示开启，false 表示关闭
boolean|true 表示开启本地渲染，false 表示关闭
EnableExternalCaptureCompleteCallback|指向 App 定义的回调函数

```java
ILiveSDK.getInstance().getAvVideoCtrl().enableExternalCapture(false, true
      new AVVideoCtrl.EnableExternalCaptureCompleteCallback(){
@Override
protected void onComplete(boolean enable, int result) {
super.onComplete(enable, result);
}
});
```

- 获取原始视频数据，加工处理
- 上传视频数据

接口名|接口描述
:--|:--:
fillExternalCaptureFrame|输入从外部视频捕获设备获取的视频图像到 SDK

参数类型|说明
:--|:--:
byte 数组|图像数据
int|图像数据长度
int|图像的 byteperRow。RGB32 图像专用，一般为宽度的4倍，特殊分辨率图像需要注意
int|图像宽度
int|图像高度
int|图像渲染角度。角度可以是 0、1、2、3，含义分别为图像需要顺时针旋转 0、90、180、270度才能正立
int|图像颜色格式，具体值参考 EXTERNAL_FORMAT_I420、EXTERNAL_FORMAT_RGBA 等值
int|视频源类型。当前仅支持 VIDEO_SRC_TYPE_CAMERA

```java
// 图像需要旋转 90 度
ILiveSDK.getInstance().getAvVideoCtrl().fillExternalCaptureFrame(data, data.length, 0,
    mCameraSize.width, mCameraSize.height, 3, AVVideoCtrl.COLOR_FORMAT_I420, AVView.VIDEO_SRC_TYPE_CAMERA);
```


**自定义预处理视频数据：**

![](https://mc.qcloudimg.com/static/img/0baaafa05e5549ff30f584ac9424f756/11.png)

- 拦截 AVSDK 相机数据

| 接口名|  接口描述  |
|---------|---------|
| **setLocalVideoPreProcessCallback** | 设置本地摄像头视频的前处理函数|

**实现代码：**

```
/*
函数说明：
a,预处理相机数据后，一般只需要将处理后的数据重新写回 VideoFrame.data ，并且需要保持视频数据格式和大小不变；
b,其他的参数一般不需要修改

返回值：true： 成功    false: 失败
*/
boolean bRet = ILiveSDK.getInstance().getAvVideoCtrl().setLocalVideoPreProcessCallback(new AVVideoCtrl.LocalVideoPreProcessCallback(){

                    @Override
                    public void onFrameReceive(AVVideoCtrl.VideoFrame var1) {
						Log.d("SdkJni", "base class SetLocalPreProcessCallback.onFrameReceive");
					}
}
```

**回调数据类型 AVVideoCtrl.VideoFrame 参数说明：**

| 参数|  含义  |
|---------|---------|
| **byte[] data** | 视频数据（a,目前 Android 只支持 I420 数据输出;b,对数据预处理后，数据仍然塞回 data；只要视频格式保持一致，AVSDK 就可以正常的预览和编码）|
| **int dataLen** | 视频数据长度|
| **int width** | 视频宽|
| **int height** | 视频高|
| **int rotate** | 视频图像角度。角度可以是0、1、2、3，含义分别为图像需要顺时针旋转0、90、180、270度才能正立|
| **int videoFormat** | 视频格式，目前支持 I420、NV21、NV12、RGB565、RGB24、ARGB；默认为 0|
| **String identifier** | 房间成员 identifie|
| **int srcType** | 视频采集来源 NONE = 0 CAMERA = 1 SCREEN = 2 MEDIA = 3|
| **long timeStamp** | 视频帧的时间戳，SDK 内部会自动填写好，utc 时间，0 为无效值|

- 预处理视频数据：拦截到 AVSDK 回吐的相机数据，用户就可以对其做预处理；例如美白、美颜和人脸识别等；预处理之后的数据，通过 AVVideoCtrl.VideoFrame.data 塞回 AVSDK；AVSDK 会自动渲染，编码。

- 取消拦截 AVSDK 相机数据

**实现代码：**

```
// 返回 true： 成功    false: 失败
boolean bRet = ILiveSDK.getInstance().getAvVideoCtrl().setLocalVideoPreProcessCallback(null);
```

### iOS

**自定义采集画面** 主要用于预处理原始数据，比如用户需要人脸识别、画面美化和动效处理等，如下是通过自定义采集画面后，增加动效效果图。

>? 动效效果是用户自己增加的，和本文档无关，用户可以对原始数据做任何预处理（不仅是动效）。

![](http://mc.qcloudimg.com/static/img/f532b2ba735514ed2faff41cf805a817/image.png)


- **流程说明：**首先请记住，若要使用自定义采集画面，则采集画面的过程与 iLiveSDK 没有任何关系，完全不依赖 iLiveSDK，自定义采集画面的流程中，iLiveSDK 的作用是透传数据以及渲染远程数据。而本文介绍的流程是：**【自定义采集画面】>【画面传入 iLiveSDK】>【远程端收到画面帧渲染】** 的整个过程，流程图如下：

![](http://mc.qcloudimg.com/static/img/5311e0e74ef71db124c291be01f8b5da/image.png)


- **采集前准备：**进入房间之后，采集画面之前。

|接口|描述|
|---|---|
|enableExternalCapture:|打开(关闭)外部视频捕获设备,自定义采集时，需要打开，返回 QAV_OK 表示执行成功。用了此方法之后，**不能**再调用 iLiveSDK 的打开摄像头接口，且 iLiveSDK 的美白，美颜将失效|

|参数类型|参数名|说明|
|---|---|---|
|BOOL|isEnableExternalCapture|是否打开外部视频捕获设备，自定义采集时传 YES|
|BOOL|shouldRender|SDK 是否渲染输入流视频数据，YES 表示会，NO 表示不会|

**示例：**

```
    QAVContext *context = [[ILiveSDK getInstance] getAVContext];
    [context.videoCtrl enableExternalCapture:YES shouldRender:NO];
```

- **自定义采集：**自定义采集使用的是系统层级接口，和 iLiveSDK 没有直接联系，简单列下需要用到的系统类和方法。

|系统类或方法|描述|
|---|---|
|AVCaptureSession|采集画面|
|AVCaptureDeviceInput|画面输入流|
|AVCaptureVideoDataOutput|画面输出流|
|captureOutput: didOutputSampleBuffer: fromConnection:|采集画面回调函数，采集到的画面将从这里回吐，用户可在这个接口作预处理|

- **采集后处理：**接收到系统回吐出的原始数据（`CMSampleBufferRef` 类型数据），用户就可以对其做预处理，例如美白、美颜和人脸识别等，预处理之后的画面需要用户自己完成渲染，与 iLiveSDK 无直接联系。

- **ILiveSDK 透传：**

|接口|描述|
|---|---|
|fillExternalCaptureFrame:|向音视频 SDK 传入捕获的视频帧，返回 QAV_OK 表示执行成功|

|参数类型|参数名|说明|
|---|---|---|
|QAVVideoFrame|frame|AVSDK 画面帧类型，用户需将自定义采集的画面转换成 QAVVideoFrame 类型|

**示例：**

```
    QAVVideoCtrl *videoCtrl = [[ILiveSDK getInstance] getAvVideoCtrl].videoCtrl;
    QAVResult result = [videoCtrl fillExternalCaptureFrame:frame];
```

- **远端渲染：**

|接口|描述|
|---|---|
|OnVideoPreview:|远程画面回调，接收远程帧数据，再使用 iLiveSDK 的渲染接口渲染，用户只需要添加一个渲染区域即可。渲染详情请参考 [新版随心播](https://github.com/zhaoyang21cn/ILiveSDK_iOS_Demos/tree/master)|

|参数类型|参数名|说明|
|---|---|---|
|QAVVideoFrame|frameData|AVSDK 画面帧类型|

>?
> - 如果渲染自定义采集的画面使用了 OpenGL，则不能使用 iLiveSDK 中的渲染，否则会 Crash。也就是说，此时界面上应该有两个 view，一个渲染自定义采集的画面，另一个渲染 QAVVideoFrame 对象。
> - 转换成 QAVVideoFrame 时，属性 color_format 必需填写 AVCOLOR_FORMAT_NV12，srcType 属性必须填写 QAVVIDEO_SRC_TYPE_CAMERA。
