
用户能将模型保存至模型仓库，在模型仓库中，用户可进行模型部署，并将模型部署为 Restful API，还可进行离线批量预测、导入第三方模型等服务，下文将分别介绍：

### 模型部署
**将模型保存至模型仓库页面进行部署**
1. 右键单击【模型】组件，选择【保存】到模型仓库。
<img src="https://main.qcloudimg.com/raw/f2b68d6f8d6ec4649ea68b085758454e.png" style="zoom:67%;" />
2. 输入模型名称。
3. 页面将跳转至模型仓库页面，单击【部署】。
![](https://main.qcloudimg.com/raw/5fed45dd1f6074792083793c77a298b8.png)
4. 进行部署设置（需要选择运行环境、以及模型资源配置）。
![](https://main.qcloudimg.com/raw/8161059443f22027cd48a1474261fda9.png)
模型部署完成后，可在模型服务的操作列中，进行如下操作：
- **测试**：页面内对服务进行小数量级的测试。支持的任务类型为深度学习（图像分类、目标识别）以及机器学习。
  - 图像分类和图像目标识别的数据支持用户通过本地上传。
  - 机器学习任务的数据支持的提交格式为 JSON（用户需要先在任务流界面右击组件选择导出模型并找到对应 txt 文件）。
-  **升级/回退**：用户可对实例进行部分/全量的版本升级与回退。
-  **负载均衡**：单击列表的流量数，分配此服务各个版本的流量百分比，新建的模型版本默认流量为0。
-  **监控**：用户可在界面上查看某一段时间的响应、请求、使用率等情况。
-  **删除**：在弹框中确认删除后模型将不再对外进行服务。
  
### 模型服务调用方式
目前，智能钛支持三种类型的模型部署：

- angel：腾讯自研的 [Angel 框架](https://github.com/Angel-ML/serving) 模型服务。
- pmml：通用标准的 [PMML](https://www.ibm.com/developerworks/cn/opensource/ind-PMML1/index.html ) 模型服务。
- pb： 业界流行的 [TensorFlow Serving](http://www.tensorfly.cn/) 模型服务。

#### 调用命令
模型在智能钛页面部署后，可以直接生成采用 POST 方法的服务接口，以供外界调用。这里使用 curl 工具为例，展示用户如何调用服务接口：

```bash
curl -v \
-H "Content-Type: application/json" \
-H "Authorization: ${accessKey}" \
-X POST \
-d '${data}' \
${hostUri}/v1/models/m:predict
```

**变量含义**

- accessKey：从模型服务界面获得的密钥，例如 `a15fea3033ba4496d87cfa74bafdae8a1`。
- hostUri：从模型服务界面获得的链接，例如 `zbejoOAJ.ms.ti.tencent-cloud.com`。
- data：推理服务的输入数据，json 格式，例如 `{"instances": [{"x1":6.2, "x2":2.2, "x3":1.1, "x4":1.2}]}`。

具体调用接口时，请替换上文中的变量。data 字段为模型自身定义，请部署服务时了解模型所要求的数据结构。

#### 请求实例

**调用 PMML 模型部署的服务**

下载 PMML 模型文件 [LogisticRegressionIris](https://models-prod-1256322946.cos.ap-chengdu.myqcloud.com/demo/pmml/LogisticRegressionIris.pmml) 并在【模型仓库】页面上传和部署。部署成功后，获取正确的密钥和网址，参考下面例子进行访问：

```bash
$ curl  -H "Content-Type: application/json" \
-H "Authorization: afae3d02e2dc04e78a95a44d4ba3b1e8e" \
-d '{"instances": [{"x1":6.2, "x2":2.2, "x3":1.1, "x4":1.2}]}' \
-X POST zbejoOAJ.ms.ti.tencent-cloud.com/v1/models/m:predict
```

返回：

```json
{
    "predictions": [
        {
            "probability(-1)": "0.9135737801209646",
            "probability(1)": "0.08642621987903544",
            "y": "-1"
        }
    ]
}
```

**调用 TensorFlow 模型部署的服务**

下载 TensorFlow 模型文件 [flower.zip](https://models-prod-1256322946.cos.ap-chengdu.myqcloud.com/demo/tfserving/flower.zip) 并在【模型仓库】页面上传和部署。部署成功后，获取正确的密钥和网址，并使用我们提供好的 [image](https://models-prod-1256322946.cos.ap-chengdu.myqcloud.com/demo/tfserving/image)（base64 编码后的图片文件）文件，参考下面例子进行访问：

```bash
# 获得图片
wget "https://models-prod-1256322946.cos.ap-chengdu.myqcloud.com/demo/tfserving/image"
# 调用服务
curl -H "Content-Type: application/json" \
-H "Authorization: a660eae44640f4a47859357ea934ff907" \
-X POST bacJSOyq.ms.ti.tencent-cloud.com/v1/models/m:predict -d "@image"
```

返回：

```json
{
    "predictions": [
        {
            "probabilities": [5.39646e-07, 1.25254e-05, 0.99991, 3.78366e-06, 7.35332e-05],
            "classes": "roses",
            "class_idx": 2
        }
    ]
}
```

### 参考
[Angel 模型服务文档](https://github.com/Angel-ML/serving/blob/master/docs/serving_doc.md)
[TensorFlow 模型服务文档](http://www.tensorfly.cn/)

### 离线批量预测
1. 进入【模型仓库】页面，选择相应的模型单击【离线批量预测】。
![](https://main.qcloudimg.com/raw/1f3ea805b7b175940dc7903d615c3f17.png)
2. 在跳转的离线批量预测页面对**作业类型**、**调度周期**与**开始时间**进行设置。并对预处理环节进行编辑，如果删除预处理节点，请重新在组件间完成连线。
3. 成功开启作业后，您将进入批量预测作业列表页面，可以对所有作业进行查看和管理。
作业的运行状态有三种：运行中、已完成、已失败。单击**运行中**，可以打开任务流视图，查看离线预测作业的运行详情。


### 导入模型
1. 进入【模型仓库】页面，单击右上角【导入模型】按钮。
2. 填写 COS 路径、模型名称、选择模型类型等信息。
3. 单击【确定】，导入模型。




