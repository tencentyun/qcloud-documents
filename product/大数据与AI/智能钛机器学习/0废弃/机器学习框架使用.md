智能钛机器学习平台支持运行用户自定义的机器学习代码，使用机器学习框架，您可以做如下操作：
  - 运行自定义代码
  - 部署使用机器学习框架生成的模型
目前机器学习框架有 spark，pyspark，spark-on-angel 三种。其中 spark 组件和 spark-on-angel 组件需要用户上传自己的 jar 包，pyspark 组件上传 Python 文件。

下面以 spark 组件为例，介绍如何在平台上运行自己的 spark 代码，生成并部署模型。可点击 [组件 demo](https://tio.cloud.tencent.com/ml/platform.html?projectId=28&flowId=88) 进行查看。
![](https://main.qcloudimg.com/raw/0e79bb28c4c0c8067fdf11d396d1b01c.png)

## 运行自定义代码
代码取自官方 example，利用 [spark 框架计算圆周率](https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala)

### 代码示例

```scala
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples

import scala.math.random

import org.apache.spark.sql.SparkSession

/** Computes an approximation to pi */
object SparkPi {
  def main(args: Array[String]) {
    val spark = SparkSession
      .builder
      .appName("Spark Pi")
      .getOrCreate()
    val slices = if (args.length > 0) args(0).toInt else 2
    val n = math.min(100000L * slices, Int.MaxValue).toInt // avoid overflow
    val count = spark.sparkContext.parallelize(1 until n, slices).map { i =>
      val x = random * 2 - 1
      val y = random * 2 - 1
      if (x*x + y*y <= 1) 1 else 0
    }.reduce(_ + _)
    println(s"Pi is roughly ${4.0 * count / (n - 1)}")
    spark.stop()
  }
}
// scalastyle:on println
```

平台内置的 spark 版本是2.4，所以用户在本地打包时请引入 spark 2.4相关的依赖。选择 sbt 或者 maven 作为打包工具均可。假如此时打包后的 jar 包叫 pi-1.0.jar。

### 参数配置

#### 组件参数

- 将 spark 组件拖至画布内，点击右侧的【作业 Jar 包】，上传本地编译后的 pi-1.0.jar
- 主类名与代码保持一致，即【包名 + 类名】。
- 程序参数填入用户自定义参数，例如这里的 100， 可以在代码中的 args[0] 中读到
- 配置文件，代码中可通过 getResourceAsStream('xxx.txt') 获取该资源文件流。
![2](https://main.qcloudimg.com/raw/915ea35bbc5564db78b69ad61db0c8f3.png)

#### 资源参数

资源参数，可以根据自身代码调整分配的资源

- num-executors：spark executor 的数量
- driver-memory：spark driver 的内存大小（单位 G）
- executor-cores：spark executor 的 CPU 核数
- executor-memory：spark executor 的内存大小（单位G）
- spark-conf：spark 的配置参数，例如 spark.shuffle.service.enabled=false，用空格或者回车分割多个 conf

![3](https://main.qcloudimg.com/raw/7b7d9c50c5d9f14e7c427d5327c407f4.png)


### 查看运行状态和结果

在算子上单击右键，选择【spark控制台】查看 spark 相关日志。如果程序在运行中，可以查看 driver 和 executor 的日志，程序结束后只能查看 driver 的日志。
对于 spark-on-angel 任务，在运行中可以查看 driver master executor ps 的日志，程序结束后可以查看 driver 和 master 的日志。
<img src="https://main.qcloudimg.com/raw/fdb71ce371ae1bb1415e9a4010a7c3c3.png" style="zoom:67%;" />

在日志中查看计算 pi 的结果

![5](https://main.qcloudimg.com/raw/10eece9304f6c0bb976c0acdad86532c.png)



## 生成并部署 PMML 模型

### 生成模型

智能钛平台支持 PMML / Tfserving（pb）/ angel 三种类型的模型部署。下面以 spark 为例，代码中生成 ML 模型后，需要利用 jpmml 库将原模型转成 PMML 格式。可点击查看 [jpmml 支持的 spark 操作](https://github.com/jpmml/jpmml-sparkml)。

如果使用 maven 作为编译工具，可以引入以下依赖

```
<dependency>
    <groupId>org.jpmml</groupId>
    <artifactId>jpmml-sparkml</artifactId>
    <version>1.5.4</version>
</dependency>
```

下面这个例子，将 VectorAssembler 和 DecisionTreeClassifier 载入 pipeline 后，转换成 PMML 模型以 DateFrame 的格式存储下来

```scala
package com.tencent.tione

import org.apache.commons.io.output.ByteArrayOutputStream
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.classification.DecisionTreeClassifier
import org.apache.spark.ml.feature.VectorAssembler
import org.apache.spark.sql.{SaveMode, SparkSession}
import org.jpmml.model.MetroJAXBUtil
import org.jpmml.sparkml.PMMLBuilder

object test {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("Spark Pi")
      .getOrCreate()

    val data = spark.read
      .format("csv")
      .option("header", true)
      .option("inferSchema", "true")
      .load(args(0))

    val assembler = new VectorAssembler()
      .setInputCols(Array(
        "Alcohol",
        "Malic_acid",
        "Ash",
        "Alcalinity_of_ash",
        "Magnesium",
        "Total_phenols",
        "Flavanoids",
        "Nonflavanoid_phenols",
        "Proanthocyanins",
        "Color_intensity",
        "Hue",
        "f diluted_wines",
        "Proline"))
      .setOutputCol("features")

    val dt = new DecisionTreeClassifier()
      .setLabelCol("Label")
      .setFeaturesCol("features")

    val pipeline = new Pipeline()
      .setStages(Array(assembler, dt))

    val model = pipeline.fit(data)

    val pmml = new PMMLBuilder(data.schema, model).build()
    val baos = new ByteArrayOutputStream()
    MetroJAXBUtil.marshalPMML(pmml, baos)
    import spark.implicits._
    val writer = spark.createDataset(Seq(baos.toString)).write
    writer.mode(SaveMode.Overwrite).text(args(1))

  }
}
```


>!打包时需要把 jpmml 库打包进去，平台镜像中并不包含 pmml 库。打包时不要将 spark 等依赖打进 jar，一是会导致 jar 包体积膨胀，二是可能导致 jar 包的 spark 依赖和镜像中的 spark 版本冲突。如果使用 maven 作为编译工具，建议将 spark 等依赖的 scope 设置为 provided。

![6](https://main.qcloudimg.com/raw/2fda32327246f475426de1697756cb27.png)

如图所示，组件读取公共数据集中的 wine.csv 文件，然后将 PMML 模型输出到 cos 根路径的 spark_pmml_model 文件夹内（即下图的 txt 文件）。在 cos 上拿到模型文件后，可以导入平台的模型仓库，进行在线部署。

>!若部分存量用户所选存储桶不在【重庆地域】，请先至存储桶中将数据下载到本地。

### 部署及预测

模型的部署和预测分为两步：
 - 导入模型
 - 模型部署及预测

此处以 spark 框架生成的模型为例进行模型服务部署和在线预测功能展示，详细文档请查看 [模型仓库及模型服务部署](https://cloud.tencent.com/document/product/851/35158)。

#### 导入模型

前往【模型仓库】页面，点击【导入模型】，按照页面弹窗提示输入所需要的信息。
本案例中，需要上传.txt 格式的文件。由于目前导入模型的文件格式仅支持.pmml、.pb、.zip，.txt 的文件需要先压缩成.zip 格式，上传后将显示 COS 路径。
模型类型，选择后的结果将作为模型标签以区分模型仓库中的模型。

![](https://main.qcloudimg.com/raw/ca58cfc3c0f4f265fb163225fabeba14/201910091046.png)

成功导入后，模型将在【模型服务】中展示。


#### 模型部署及预测

进入【模型服务】页面，点击 sparkmodel 【部署】按钮，输入模型服务名称及运行环境与资源，模型将成功部署。

在此模型的【操作】栏中选择【更多】-【测试】-【机器学习】，输入符合规范的 json 代码即可在线预测。

例如本案例中输入：

`{"instances":[{"Alcohol":1.2,"Malic_acid":1.2,"Alcalinity_of_ash":1.1,"Flavanoids":1.2,"Hue":2.2,"f diluted_wines":1.1,"Proline":2}]}`

其中：`{"instances":[{"key1": ,"key2": ,"key3": ,"key4": ,"key5": ,"key6": ,"key7": }]}`为格式要求，具体数值内容用户可根据实际情况自行输入。

![](https://main.qcloudimg.com/raw/7c6a76ae59316c0d9337ee615171c4f8.png)

## 其他

pyspark 的用户脚本语言为 Python，需要上传 py 脚本和选择 Python 版本，其余配置与 spark 任务一致。
