
## Dummy

模块将低维的特征做**特征交叉** ，并将特征转换成高维的 One-Hot 特征。Dummy 模块中包含两个阶段，**特征交叉**和**特征 One-Hot**。**特征交叉** 根据 json 配置文件，对指定的特征字段做交叉，生成 feature name 组成的特征；**特征 One-Hot** 将 feature name 编码成全局统一、连续的 index。

**输入**
- 数据形式：Dense 或 libsvm。
- 格式：| features |
- 说明：
- 数据中的0和-1是两个特殊值，0表示默认值，-1表示非法值；读取数据过程中将会过滤掉这两个特殊值；因此特征的数值表示应该避开0和-1。
- Dense 数据支持多值特征（该特征可以包含多个 value），每个 value 以“|”分割。例如，某特征是“喜欢的游戏”，该特征对应的值应该是多个游戏名，即游戏列表。
- 必须包含 target 字段。训练数据的 target 字段是0或1的 label 值，预测数据的 target 字段是每条数据的标识 ID。

**输出**

输出包含三个目录：featureCount、featureIndexs 和 instances。
- featureCount：Dummy 后的特征空间维度，一个 Int 值。
- featureIndexs：feature name 到 feature index 的映射关系，中间用“:”分隔。
- instances：One-Hot 后的 Dummy 格式的样本数据。

**参数**

- 并行数：训练数据的分区数、Spark 的并行数。
- 特征生成配置：特征交叉的配置文件有两个对象“fields”和“feature_cross”，"fields"对象保存着输入数据每个字段对应的 name 和 index；“feature_cross”对象是生成特征的配置，其中“id_features”是指生成单个特征，“comb_features”是指交叉的特征，dependencies 是指交叉的特征，用逗号分隔，可以指定多个。
- 说明：必须包含 target 字段。若某个特征是多值特征，"id_features"会生成多个特征，"comb_features"也会多次与其他的 dependencies 做交叉。
- 示例：以下为示例数据的配置和特征交叉阶段生成的中间结果。

```json
	{
  	"fields": [
  	  {
  	    "name": "target",
  	    "index": "0"
  	  },
  	  {
  	    "name": "f1",
  	    "index": "1"
  	  },
  	  {
   	    "name": "f2",
  	    "index": "2"
 	   },
 	   {
   	    "name": "f3",
  	    "index": "3"
 	   }
 	 ],
        "feature_cross": {
 	 "id_features": [
           {
            "name": "f1"
           },
           {
            "name": "f3"
           }
   	  ],
  	 "comb_features": [
          {
            "name": "f1_f2",
            "dependencies": "f1,f2"
          },
          {
            "name": "f2_f3",
            "dependencies": "f2,f3"
          }
        ]
     }
  }
```

- featureIndex：指定了特征 One-Hot 时，使用的 feature name 到 index 之间的映射。该参数为选填，当需要基于上次特征 Dummy 的结果做增量更新或者预测数据的 Dummy 时，需要指定该参数，以保证两次 Dummy 的 index 一致。
- 负样本抽样率：大部分应用场景中，负样本比例太大，可通过该参数对负样本采样。

## PCA

主成分分析（Principal Component Analysis，PCA）是一种统计学的特征降维方法，将数据从原来的坐标系投影到新的坐标系，通过每个维度的方差大小来衡量该维度的重要性。从中选取重要性排在前 K 个的特征作为新的特征，达到数据降维的目的。

**输入**

- 数据形式：Dense 或 Libsvm。
- 格式：| 参与计算的 features | 不参与计算的 features | 
- 参与计算的 features：可通过**选择特征列**指定。
- 不参与计算的 features：可包括不参与计算的特征，如果存在则保留在输出中，对于 Libsvm 格式的数据，也包括了 label 列，该 label 仅用于标识样本 ID。

**输出**

- 格式：| 不参与计算的 features | PCA 结果列 |
- PCA 结果列：选取的前 K 个新特征。

**参数**

- 选择特征列：表示需要计算的特征所在列，例如“1-12,15”，表示取特征在表中的1到12列，15列，从0开始计数。
- 提取主成份维度：降维后的特征维度。
- 并行数：训练数据的分区数、Spark 的并行数。
- 抽样率：输入数据的采样率。

## Normalizer

该模块提供了特征正则化，将各维特征正则化成[0, 1]阈值，可以避免不同特征值的分布域不一致而影响算法的决策。该算法的计算过程：首先计算每行所选特征的 p 阶范数，然后将该行所选的每个特征值与该范数相除的结果作为最终的正则化结果。当某行特征的 p 阶范数为0.0时，该行特征的正则化结果为该行特征的原始值。

**输入**
- 数据形式：Dense 或 Libsvm。
- 格式：| 参与计算的 features | 不参与计算的 features | 
- 参与计算的 features：可通过**选择特征列**指定。
- 不参与计算的 features：可包括不参与计算的特征，如果存在则保留在输出中，对于 Libsvm 格式的数据，也包括了 label 列，该 label 仅用于标识样本 ID。

**输出**
- 格式：| 不参与计算的 features | 正则化的结果 |
- 正则化的结果：被选取的特征的正则化结果。

**参数**
- 选择特征列：表示需要计算的特征所在列，例如“1-12,15”，表示取特征在表中的1到12列，15列，从0开始计数。
- p： p-norm 参数，默认为 2，表示用 L2-Norm 做正则化。
- 并行数：训练数据的分区数、Spark 的并行数。
- 抽样率：输入数据的采样率。

## Scaler

Scaler 模块集成了最大最小值归一化（MinMaxScaler）和标准化（StandardScaler）两种方式，用户可通过特征配置文件来指定某个特征的归一化方法。

**MinMaxScaler 算法**对特征数据进行统一的归一化处理，默认归一化后的特征数值范围在[0,1]，用户也可以指定归一化后的取值范围为[min,max]。
该归一化的计算公式为：((x - EMin) / (EMax - EMin)) * (max - min) + min
其中，x 表示需要归一化的特征值，EMin 表示该特征下的最小值，EMax 表示该特征下的最大值，min 与 max 为用户设定的归一化后的数值范围。**当某列特征的最大最小值相等时，该列所有数值归一化为 0.5 \* (max - min) + min**。

**StandardScaler 算法**主要对特征进行标准化处理，原始特征数据通过该算法的转化将成为方差为1，均值为0的新的特征。
该标准化的计算公式为：(x - Mean) / Var
其中，Mean 代表该特征的平均值，Var 表示该特征的样本标准差。

- 如果 Var 为0，则 x 标准化后的结果直接为0.0。
- 不需要均值化处理：此时算法只做方差处理，即：x / Var。
- 不需要方差处理：x 直接取值(x - Mean)。

**输入**
- 数据形式：Dense。
- 格式：| 参与计算的 features | 不参与计算的 features | 
- 参与计算的 features：通过**特征配置文件**指定。
- 不参与计算的 features：可包括不参与计算的特征，如果存在则保留在输出中。

**输出**
- 格式：与输入数据格式一致。
- 说明：特征排列顺序与输入数据的一致，根据特征配置对数据进行归一化或标准化处理，没有配置的特征值保持不变。

**参数**
- 并行数：训练数据的分区数、Spark 的并行数。
- 抽样率：输入数据的采样率。
- 特征配置文件名：从 TI-ONE 页面上传配置文件。

示例：以下是一个该模块的 JSON 格式的特征配置文件样例。

```json
{
    "minmax":[
      {
        "colStr": "1-2",
        "min": "0",
		"max":"1"
      },
      {
		"colStr":"5",
        "min":"-1",
        "max": "1"
       }
       ],
		"standard":[
       {
        "colStr": "3,6-7",
        "std": "true",
		"mean":"false"
       },
       {
        "colStr": "8,9",
		"std":"true",
		"mean":"true"
       }
       ]
}
```

**特征配置文件参数**

- minmax 和 standard：分别代表了相应的归一化模块 MinMaxScaler 和 StandardScaler。
- colStr：需要做相应处理的特征 ID，取值根据该特征在原始表的所在列，从0开始计数。多个特征可用“，”隔开，还可用“-”标识特征的起始到结束列，例如“1 - 20”表示第1列到第20列。
- min：归一化后的最小值。
- max：归一化后的最大值。
- std：是否需要做方差的标准化。
- mean：是否需要做均值的标准化。

## Discrete

Discrete 算法用于对特征数据进行离散化处理。离散方法包括等频和等值离散方式，等频离散法按照特征值从小到大的顺序将特征值划分到对应的桶中，每个桶中容纳的元素个数是相同的；等值离散法根据特征值中的最小值和最大值确定每个桶的划分边界，从而保证每个桶的划分宽度在数值上相等。

**输入**
- 数据形式：Dense。
- 格式：| 参与计算的 features | 不参与计算的 features | 
- 参与计算的 features：可通过**特征配置文件**指定。
- 不参与计算的 features：可包括不参与计算的特征，如果存在则保留在输出中。

**输出**
- 离散化结果：
  - 格式：与输入数据格式一致。
  - 说明：特征的排列顺序与输入数据一致，根据特征配置对数据进行离散化处理的结果，没有配置的特征值保持不变。
- 离散特征边界：每个特征进行离散化时使用的边界值：
  - 格式：| index | bound1 bound2 bound3 ...|
  - 说明：以空格连接各字段。
  - index：离散化的特征 ID，从0开始计数。
  - boundx：每个特征对应的离散边界值，各边界值以空格连接。

**参数**
- 并行数：训练数据的分区数、Spark 的并行数。
- 抽样率：输入数据的采样率。
- 特征配置文件名：从智能钛机器学习平台页面上传配置文件。

示例：以下是一个该模块的 JSON 格式的特征配置文件示例，特征配置之间的顺序没有要求，如果某些特征不需要离散配置，则不写在配置中。该配置文件中的“feature”不可更改，用户只需对如下参数进行修改即可。

```json
{
    "feature": [
      {
        "id": "2",
        "discreteType": "equFre",
        "numBin": "3"
      },
      {
		"id": "5",
        "discreteType": "equVal",
        "numBin": "3",
		"min": "0",
		"max": "100"
       },
       {
        "id": "0",
        "discreteType": "equVal",
        "numBin": "2",
		"min":"-1"
      }
    ]
}
```

**特征配置文件参数**

- id：表示特征 ID，该 ID 是输入数据中特征所在的列号，从0开始计数。
- discreteType：离散化的类型，“equFre”表示等频离散，“equVal”表示等值离散。
- numBin：离散化的桶数，在等频离散方法中，如果桶数设置过大，每个桶中的元素个数过少，导致离散边界中有重复的点，则出错。
- min：针对等值离散的配置，限定了特征值的最小值，如果特征值中有比这个值还小的则出错。如果没有需要可不填，同下面的“max”。
- max：针对等值离散的配置，限定了特征值的最大值，如果特征值中有比这个值还大的则出错，没有需要可不填。


