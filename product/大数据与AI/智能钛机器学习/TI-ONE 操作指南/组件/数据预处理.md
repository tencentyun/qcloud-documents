除了在程序中对数据做预处理，在上游系统对数据作SQL操作，平台的PySpark组件也可以实现简单的SQL处理。


> Spark组件是面向使用Python的Spark用户，用户通过Python编写Spark应用程序，通过PySpark组件完成部署，也支持pyspark的sql功能，本文有部分使用方法介绍（更多用法请参考社区指引:[Spark SQL, DataFrames and Datasets Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html)）。

和标准的Spark相比，pySpark支持上传Python脚本和实时修改，更加的灵活，而且支持SQL功能，所以我们推荐用来进行数据预处理。

### **从左侧组件列表里拖拽出一个PySpark节点**


![](https://main.qcloudimg.com/raw/b3347ed7c805332139b03fd5f8edfb90.png)


### **单击任务节点，会从右侧弹出配置框**


![](https://main.qcloudimg.com/raw/bb1a61a1165ed1edb03015e86eac9161.png)

### 1. 入门示例

首先通过一个简单的例子来看看如何使用python写spark应用程序，如下代码显示通过`range(1, 4)`初始化构造弹性分布式数据集`RDD`，经过transformation方法`map`转换后得到一种新的形式，最后调用action方法`collect`收集结果并打印。

 ![](https://main.qcloudimg.com/raw/3692e0cab38f5b68d462c26f707f2eea.png)

### 2. RDD Style

下面代码显示基于RDD API，从COS上读取原始数据，经过一系列转换处理后，得到每条记录的前4列，然后通过action方法`take(2)`仅拿出两条记录打印出来看看处理效果，如果处理正确，可以通过`saveAsTextFile`将结果保存到COS结果目录中。

![](https://main.qcloudimg.com/raw/3692e0cab38f5b68d462c26f707f2eea.png)

### 3. DataFrame Style

上述基于RDD的代码可能对数据分析人员来说并不是很好懂，用过scikit-learn的同学可能会对dataframe更熟悉，下面代码显示直接通过spark的提供的csv接口加载源数据得到dataframe，并打印schema信息，选出”age”和”marital”两列，显示4条记录。

![](https://main.qcloudimg.com/raw/2bbefdaf984253359880c8b6558921bc.png)

从以上代码可以看到，通过csv直接加载得到的dataframe，”age”列是`string`类型，我们可以使用`cast`表达式将其转换为`int`类型。
![](https://main.qcloudimg.com/raw/4216b3940324163a70c7fe4c3bd73415.png)

基于dataframe，可以使用sql算子灵活方便地处理数据，下面代码显示使用`groupBy`对数据进行分组计算。
![](https://main.qcloudimg.com/raw/c83d93d8ce4c26e2e2092c95c4698eb2.png) 

当然，你还可以像如下代码显示一样直接写sql。 
![](https://main.qcloudimg.com/raw/2e71e389507e5420b4fa2ac3ea120bce.png) 
