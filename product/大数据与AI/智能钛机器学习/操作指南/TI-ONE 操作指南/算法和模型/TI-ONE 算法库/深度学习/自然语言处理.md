
## 【2.0】Fasttext

**算法说明**
Fasttext 是一种简单有效的句子分类算法， 通过词向量以及 ngram 向量的平均值计算出句子的向量表示，再通过全连接层网络对句子进行分类 [参考文档](https://arxiv.org/pdf/1607.01759.pdf)。


**输入参数**
- 训练数据：每一行为一个句子，词与词之间用空格分隔，句子之间以及句子和标签之间用特定分隔符分隔（句子放置于分隔符左侧，标签放置于分隔符右侧），分隔符在算法参数中可以设置。
- 验证数据：格式同训练数据。
- 模型目录：存放模型文件和日志文件的目录（如果目录中已有模型文件，下次训练时将加载目录中最新的模型文件。因此，如果改变了网络结构或更换了数据，请更换或清空模型目录）。

**算法参数**
- 分隔符：用于分隔句子和标签的分隔符。
- 词向量维度：网络中词向量的维度。
- 批处理大小：即训练的 batch_size。
- 训练 epoch 数：训练数据的训练次数。
- 学习率：即 learning_rate。
- 是否使用预训练好的词向量：如设为 True，可填写词向量文件路径。词向量文件格式与 glove 词向量官方格式相同。**如果使用预训练好的词向量，预训练词向量的维度应等于参数【词向量维度】的值**。

**实例生成**
1. 使用数据节点，上传数据，数据格式请参考【训练数据】部分。
2. （可选）如数据只有一份，可以使用【输入】>【数据转换】中的【文本数据切分】节点，将上传的数据按比例分为训练数据和验证数据。
3. 将两份数据分别连接到 Fasttext 节点的两个输入桩，单击【运行】开始训练。

## 【2.0】LSTM 句子分类

**算法说明**
LSTM 句子分类算法首先使用双向 LSTM 网络 [参考文档](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320&rep=rep1&type=pdf) 产生要分类的句子的向量表示，再通过全连接层网络对句子进行分类。


**输入参数**
- 训练数据：每一行为一个句子，词与词之间用空格分隔，句子之间以及句子和标签之间用特定分隔符分隔（句子放置于分隔符左侧，标签放置于分隔符右侧），分隔符在算法参数中可以设置。
- 验证数据：格式同训练数据。
- 模型目录：存放模型文件和日志文件的目录（如果目录中已有模型文件，下次训练时将加载目录中最新的模型文件。因此，如果改变了网络结构或更换了数据，请更换或清空模型目录）。

**算法参数**
- 分隔符：用于分隔句子和标签的分隔符。
- 词向量维度：网络中词向量的维度。
- LSTM 维度：网络中句子的向量表示的维度。
- 批处理大小：即训练的 batch_size。
- 训练 epoch 数：训练数据的训练次数。
- 学习率：即 learning_rate。
- 是否使用预训练好的词向量：如设为 True，可填写词向量文件路径。词向量文件格式与 glove 词向量官方格式相同。**如果使用预训练好的词向量，预训练词向量的维度应等于参数【词向量维度】的值**。
  
**实例生成**
1. 使用数据节点上传数据，数据格式请参考【训练数据】部分。
2. （可选）如数据只有一份，可以使用【输入】>【数据转换】中的【文本数据切分】节点，将上传的数据按比例分为训练数据和验证数据。
3. 将两份数据分别连接到 LSTM-classifier 节点的两个输入桩，单击【运行】开始训练。

## 【2.0】TextCNN
**算法说明**
TextCNN 使用卷积神经网络产生句子的向量表示，再通过全连接层网络对句子进行分类 [参考文档](https://arxiv.org/pdf/1408.5882.pdf)。


**输入参数**
- 训练数据：每一行为一个句子，词与词之间用空格分隔，句子之间以及句子和标签之间用特定分隔符分隔（句子放置于分隔符左侧，标签放置于分隔符右侧），分隔符在算法参数中可以设置。
- 验证数据：格式同训练数据。
- 模型目录：存放模型文件和日志文件的目录（如果目录中已有模型文件，下次训练时将加载目录中最新的模型文件。因此，如果改变了网络结构或更换了数据，请更换或清空模型目录）。

**算法参数**
- 分隔符：用于分隔句子和标签的分隔符。
- 词向量维度：网络中词向量的维度。
- 各层网络卷积核大小：即 kernel_size。
- 各层网络卷积核个数：即通道数。
- 批处理大小：即训练的 batch_size。
- 训练 epoch 数：训练数据的训练次数。
- 学习率：即 learning_rate。
- 是否使用预训练好的词向量：如设为 True，可填写词向量文件路径。词向量文件格式与 glove 词向量官方格式相同。**如果使用预训练好的词向量，预训练词向量的维度应等于参数【词向量维度】的值**。

**实例生成**
1. 使用数据节点上传数据，数据格式请参考【训练数据】部分。
2. （可选）如数据只有一份，可以使用【输入】>【数据转换】中的【文本数据切分】节点，将上传的数据按比例分为训练数据和验证数据。
3. 将两份数据分别连接到 TextCNN 节点的两个输入桩，单击【运行】开始训练。



## 【2.0】BiLSTM-CRF
**算法说明**
BiLSTM-CRF 是一种常用的序列标注模型，能用于分词，词性标注，命名实体识别等序列标注任务。模型使用双向 LSTM 网络产生句子中的各个词语的向量表示，并据此计算词语标签的概率分布，然后使用 CRF 计算总概率最大的标签序列 [参考文档](https://arxiv.org/pdf/1508.01991.pdf)。

**输入参数**
- 训练数据：文本文件，其中每一行是一个句子及其各个词语的对应标签，句子和标签之间由特定分隔符分隔（默认为 label），句子中的各个词语之间，以及各个标签之间用空格分隔。由于算法对句子中的每个词语进行标注，词语个数必须等于标签个数，如词语1 词语2 词语3 abel 标签1 标签2 标签3。
- 验证数据：文本文件，格式与训练数据相同。
- 批处理大小：即算法 batch_size。
- 学习率：即算法的 learning_rate。
- 训练次数：即将训练数据训练的 epoch 数。
- 词向量维度：即每个词语向量表示的维度。
- LSTM 维度：即每个词语 LSTM 向量表示的维度。
- 使用预训练好的词向量模型：如设为 True，可填写词向量文件路径。词向量文件格式与 glove 词向量官方格式相同。**如果使用预训练好的词向量，预训练词向量的维度应等于参数【词向量维度】的值**。

**输出参数**
日志目录：即存放 events 文件目录的路径，可以用此路径启动 tensorboard 查看训练情况。

**实例生成**
1. 使用数据节点，上传数据，数据格式请参考【训练数据】部分。
2. （可选）如数据只有一份，可以使用【输入】>【数据转换】中的【文本数据切分】节点，将上传的数据按比例分为训练数据和验证数据。
3. 将两份数据分别连接到 BiLSTM-CRF 节点的两个输入桩，单击【运行】开始训练。



## 【2.0】词频统计

**算法说明**
词频统计节点统计文本中词语的出现频率，并从高到低排列。

#### 参数设置

**输入参数**
输入数据：要统计词频的文本。

**输出参数**
输出文件：词频统计结果。

#### 实例生成
1. 使用数据节点，上传数据，数据格式见上文【文本数据输入】部分。
2. 将输入数据连接到【词频统计】节点的输入桩，单击【运行】按钮开始运行。

## 【2.0】Glove
**算法说明**
Glove 是一种经典的词向量算法，能够从大量文本中学习出各个词语的向量表示。这一向量表示可以用作其它深度学习模型的初始值 [参考文档](http://www.aclweb.org/anthology/D14-1162)。

**输入参数**
- 训练数据：文本文件，词语之间使用空格分隔。
- 验证数据：（可选）包含四元组的数据集，如 word2vec 官方 questions-words.txt，用于评价生成的词向量的质量。

**输出参数**
保存路径：生成的词向量文件的保存路径。生成的文件为文本文件，每行为一个词语及其向量表示，词语和向量之间，向量中的各个数字之间使用空格分隔。

**算法参数**
- 词向量维度：要训练的词向量维度。
- 窗口大小：计算共现矩阵时使用的 window_size 参数。
- 最小出现次数：只训练出现次数大于这一次数的词语的词向量。
- 训练 epoch 数：训练数据的训练次数。
- 学习率：即 learning_rate 参数。
- 批处理大小：即训练的 batch_size。
- 最大词汇表大小：如果训练数据中单词总数超过这一大小 n，只训练出现频率最高的 n 个词的词向量。

**实例生成**
1. 使用数据节点上传数据，数据格式请参考【输入参数】部分。
2. 将数据节点连接到 glove 节点，配置好保存路径，单击【运行】按钮开始训练。



## 【2.0】Word2Vec
**算法说明**
Word2Vec 是一种经典的词向量算法，能够从大量文本中学习出各个词语的向量表示。这一向量表示可以用作其它深度学习模型的初始值 [参考文档](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)。

**输入参数**
- 训练数据：文本文件，词语之间使用空格分隔。
- 验证数据：（可选）包含四元组的数据集，如 word2vec 官方 questions-words.txt，用于评价生成的词向量的质量。

**输出参数**
保存路径：生成的词向量文件的保存路径。生成的文件为文本文件，每行为一个词语及其向量表示，词语和向量之间，向量中的各个数字之间使用空格分隔。

**算法参数**
- 词向量维度：要训练的词向量维度。
- 窗口大小：skip-gram 算法中的 window_size 参数。
- 最小出现次数：只训练出现次数大于这一次数的词语的词向量。
- 训练 epoch 数：训练数据的训练次数。
- 学习率：即 learning_rate 参数。
- 批处理大小：即训练的 batch_size。
- 负采样个数：skip-gram 算法中的负样本个数。

**实例生成**
1. 使用数据节点上传数据，数据格式请参见【输入参数】部分。
2. 将数据节点连接到 word2vec 节点，配置好保存路径，单击【运行】开始训练。

## 【2.0】BERT 句子分类

**算法说明**
BERT 句子分类算法首先使用BERT网络 [论文链接](https://arxiv.org/pdf/1810.04805.pdf) 产生要分类的句子的向量表示，再通过全连接层网络对句子进行分类。

**参数设置**

**输入参数**
- 训练数据：每一行为一个句子或两个句子，词与词之间用空格分隔，句子之间以及句子和标签之间用特定分隔符分隔（句子放置于分隔符左侧，标签放置于分隔符右侧），分隔符在算法参数中可以设置。
- 验证数据：格式同训练数据。
- 模型目录：存储模型文件和 events 文件的路径，可以从该路径启动 tensorboard。
- bert 参数目录：存储预训练的 bert 参数的目录，对于中文和英文，可以分别填写`${ai_dataset_lib}/BERT/chinese_L-12_H-768_A-12`和`${ai_dataset_lib}/BERT/uncased_L-12_H-768_A-12`，或使用自行上传的地址。

  

**算法参数**

- 标签分隔符：用于分隔句子和标签的分隔符
- 批处理大小：即训练的 batch_size。
- 训练 epoch 数：训练数据的训练次数。
- 学习率：即 learning_rate。
- 句子分隔符：对于输入为两个句子的数据，分隔两个句子的分隔符
- 最大句子长度：BERT 要求固定的输入长度，因此不足这一长度的句子将被补齐，超过的将被截断。

  
**实例生成**
1. 使用数据节点，上传数据，数据格式见上文【训练数据】部分。
2. （可选）如数据只有一份，可以使用【输入】-【数据转换】中的【文本数据切分】节点，将上传的数据按比例分为训练数据和验证数据。
3. 将两份数据分别连接到 BERT 文本分类节点的两个输入桩。单击【运行】开始训练。


## 【2.0】中文文本摘要

**算法说明**
中文文本摘要首先使用 BERT 网络 [论文链接](https://arxiv.org/pdf/1810.04805.pdf) 产生文中句子的向量表示，再通过 TextRank 算法抽取权重较高的句子。

**参数设置**

**输入参数**
文本数据输入：要抽取摘要的文本，要求每行为一个句子。

**输出参数**
输出数据：抽取后的摘要。

**算法参数**
摘要句子数目：从文本中选为摘要的句子数量。

  

**实例生成**
1. 使用数据节点，上传数据，数据格式见上文【文本数据输入】部分。
2. 将输入数据连接到【中文文本摘要】节点的输入桩，单击【运行】按钮开始运行。



## 【2.0】中文关键词抽取

**算法说明**
中文关键词抽取算法使用 TF - IDF 抽取输入文本中的关键词。



**参数设置**

**输入参数**
文本数据输入：要抽取关键词的文本，无需预先分词。

**输出参数**
输出数据：抽取后的关键词。

**算法参数**
关键词个数：从文本中抽取的关键词数量。

  

**实例生成**
1. 使用数据节点，上传数据，数据格式见上文【文本数据输入】部分。
2. 将输入数据连接到【中文关键词抽取】节点的输入桩，单击【运行】按钮开始运行。



