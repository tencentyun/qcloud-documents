## 分词
#### 算法说明
分词指将中文句子划分成词语，词语之间用空格分隔。

#### 输入参数
输入数据：未分词的中文文本，每行为一个句子。

#### 输出参数
输出数据：分词后的中文文本，每行为一个句子，词与词之间用空格分隔。

#### 算法参数
标签分隔符：（可选）如行中有标签分隔符，则只对标签分隔符左侧的文本进行分词。

####   实例生成
1. 使用数据节点上传数据，数据格式请参考【输入参数】部分。
2. 将数据节点连接到分词节点，配置好输出数据路径和标签分隔符，单击【运行】开始分词。



## 去除停用词

#### 算法说明
去除文本中的停用词，文本需要预先分词。

#### 输入参数
- 输入数据：分词后的中文文本，每行为一个句子，词语之间用空格分隔。
- 停用词表：要去除的停用词列表，每行为一个词。

#### 输出参数
输出数据：去除停用词后的中文文本，每行为一个句子。

#### 算法参数
标签分隔符（可选）：如行中有标签分隔符，则只对标签分隔符左侧的文本进行去停用词。

#### 实例生成
1. 使用数据节点，上传输入数据和停用词表数据，数据格式请参考【输入参数】部分。
2. 将数据节点连接到去停用词词节点，配置好输出数据路径和标签分隔符，单击【运行】开始去停用词。



## LSTM 文本分类

#### 算法说明
LSTM 文本分类算法首先使用双向 LSTM 网络（[参考文档](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320&rep=rep1&type=pdf)）产生要分类的句子的向量表示，再通过全连接层网络对句子进行分类。

[算法示例](https://tio.cloud.tencent.com/ml/platform.html?projectId=4265&flowId=13365)
#### 输入参数
- 训练数据：每一行为一个句子，词与词之间用空格分隔，句子和标签之间用特定分隔符分隔（分隔符在算法参数中可以设置）。
- 验证数据：格式同训练数据。
- logs 存储路径：存储 events 文件的路径，可以从该路径启动 tensorboard。

#### 算法参数
- 分隔符：用于分隔句子和标签的分隔符。
- 词向量维度：网络中词向量的维度。
- LSTM 维度：网络中句子的向量表示的维度。
- 批处理大小：即训练的 batch_size。
- 训练 epoch 数：训练数据的训练次数。
- 学习率：即 learning_rate。
- 是否使用预训练好的词向量：如设为 True，可填写词向量文件路径。词向量文件格式与 glove 词向量官方格式相同。**如果使用预训练好的词向量，预训练词向量的维度应等于参数【词向量维度】的值**。
  
#### 	实例生成
1. 使用数据节点上传数据，数据格式请参考【训练数据】部分。
2. （可选）如数据只有一份，可以使用【输入】>【数据转换】中的【NLP 数据切分】节点，将上传的数据按比例分为训练数据和验证数据。
3. 将两份数据分别连接到 LSTM-classifier 节点的两个输入桩，单击【运行】开始训练。



## Fasttext

#### 算法说明
Fasttext 是一种简单有效的句子分类算法， 通过词向量以及 ngram 向量的平均值计算出句子的向量表示，再通过全连接层网络对句子进行分类（[参考文档](https://arxiv.org/pdf/1607.01759.pdf)）。

[算法示例](https://tio.cloud.tencent.com/ml/platform.html?projectId=4265&flowId=13345)
#### 输入参数
- 训练数据：每一行为一个句子，词与词之间用空格分隔，句子和标签之间用特定分隔符分隔（分隔符在算法参数中可以设置）。
- 验证数据：格式同训练数据。
- logs 存储路径：存储 events 文件的路径，可以从该路径启动 tensorboard。

#### 算法参数
- 分隔符：用于分隔句子和标签的分隔符
- 词向量维度：网络中词向量的维度。
- 批处理大小：即训练的 batch_size。
- 训练 epoch 数：训练数据的训练次数。
- 学习率：即 learning_rate。
- 是否使用预训练好的词向量：如设为 True，可填写词向量文件路径。词向量文件格式与 glove 词向量官方格式相同。**如果使用预训练好的词向量，预训练词向量的维度应等于参数【词向量维度】的值**。
- 是否使用 ngrams：如设为 True，在计算句子向量表示时将引入 ngrams 以建模词序信息。需要配置使用的 n 值，ngrams 进行 hash 后的 bucket 数和 ngram 向量表示的维度。

#### 实例生成
1. 使用数据节点，上传数据，数据格式请参考【训练数据】部分。
2. （可选）如数据只有一份，可以使用【输入】>【数据转换】中的【NLP 数据切分】节点，将上传的数据按比例分为训练数据和验证数据。
3. 将两份数据分别连接到 Fasttext 节点的两个输入桩，单击【运行】开始训练。



## TextCNN
#### 算法说明
TextCNN 使用卷积神经网络产生句子的向量表示，再通过全连接层网络对句子进行分类（[参考文档](https://arxiv.org/pdf/1408.5882.pdf)）。

[算法示例](https://tio.cloud.tencent.com/ml/platform.html?projectId=4265&flowId=13363)
#### 输入参数
- 训练数据：每一行为一个句子，词与词之间用空格分隔，句子和标签之间用特定分隔符分隔（分隔符在算法参数中可以设置）。
- 验证数据：格式同训练数据。
- logs 存储路径：存储 events 文件的路径，可以从该路径启动 tensorboard。

#### 算法参数
- 分隔符：用于分隔句子和标签的分隔符。
- 词向量维度：网络中词向量的维度。
- 各层网络卷积核大小：即 kernel_size。
- 各层网络卷积核个数：即通道数
- 批处理大小：即训练的 batch_size。
- 训练 epoch 数：训练数据的训练次数。
- 学习率：即 learning_rate。
- 是否使用预训练好的词向量：如设为 True，可填写词向量文件路径。词向量文件格式与 glove 词向量官方格式相同。**如果使用预训练好的词向量，预训练词向量的维度应等于参数【词向量维度】的值**。

#### 实例生成
1. 使用数据节点上传数据，数据格式请参考【训练数据】部分。
2. （可选）如数据只有一份，可以使用【输入】>【数据转换】中的【NLP 数据切分】节点，将上传的数据按比例分为训练数据和验证数据。
3. 将两份数据分别连接到 TextCNN 节点的两个输入桩，单击【运行】开始训练。



## BiLSTM+CRF
#### 算法说明
BiLSTM+CRF 是一种常用的序列标注模型，能用于分词，词性标注，命名实体识别等序列标注任务。模型使用双向 LSTM 网络产生句子中的各个词语的向量表示，并据此计算词语标签的概率分布，然后使用 CRF 计算总概率最大的标签序列（[参考文档](https://arxiv.org/pdf/1508.01991.pdf)）。
#### 输入参数
- 训练数据：文本文件，其中每一行是一个句子及其各个词语的对应标签，句子和标签之间由特定分隔符分隔（默认为 __label__），句子中的各个词语之间，以及各个标签之间用空格分隔。由于算法对句子中的每个词语进行标注，词语个数必须等于标签个数，如`词语1 词语2 词语3__label__标签1 标签2 标签3`。
- 验证数据：文本文件，格式与训练数据相同。
- 批处理大小：即算法 batch_size。
- 学习率：即算法的 learning_rate。
- 训练次数：即将训练数据训练的 epoch 数。
- 词向量维度：即每个词语向量表示的维度。
- LSTM 维度：即每个词语 LSTM 向量表示的维度。
- 使用预训练好的词向量模型：如设为 True，可填写词向量文件路径。词向量文件格式与 glove 词向量官方格式相同。**如果使用预训练好的词向量，预训练词向量的维度应等于参数【词向量维度】的值**。

#### 输出参数
日志目录：即存放 events 文件目录的路径，可以用此路径启动 tensorboard 查看训练情况。
 
####  实例生成
1. 使用数据节点，上传数据，数据格式请参考【训练数据】部分。
2. （可选）如数据只有一份，可以使用【输入】>【数据转换】中的【NLP 数据切分】节点，将上传的数据按比例分为训练数据和验证数据。
3. 将两份数据分别连接到 BiLSTM-CRF 节点的两个输入桩，单击【运行】开始训练。



## 基于 PMI 和熵的新词发现

#### 算法说明
PMI（点互信息）和左右熵能够刻画一个文本片段的凝固程度和灵活运用程度，因此可以用于发现文本中不存在词库中的新词。

[算法示例](https://tio.cloud.tencent.com/ml/platform.html?projectId=4265&flowId=13351)

#### 输入参数
- 输入数据：未分词的中文纯文本文件。
- 词库：（可选）文本文件，每行为一个词。如发现的新词已经存在这个词库中，则跳过这一新词。

#### 输出参数
输出数据：算法发现的新词，每行为一个词。

#### 算法参数
- 最大词语长度：只考虑长度不超过这一长度的文本片段构成新词的可能性。
- 保留前 n 个新词：只保留可能性最大的前若干个新词。

#### 实例生成
1. 使用数据节点上传数据，数据格式请参考【训练数据】部分
2. 将数据连接到【新词发现】节点的输入桩，设置好输出数据路径，单击【运行】开始发现新词。


## Word2Vec
#### 算法说明
Word2Vec 是一种经典的词向量算法，能够从大量文本中学习出各个词语的向量表示。这一向量表示可以用作其它深度学习模型的初始值（[参考文档](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)）。

[算法示例](https://tio.cloud.tencent.com/ml/platform.html?projectId=4265&flowId=13333 )
#### 输入参数
- 训练数据：文本文件，词语之间使用空格分隔。
- 验证数据：（可选）包含四元组的数据集，如 word2vec 官方 questions-words.txt，用于评价生成的词向量的质量。

#### 输出参数
保存路径：生成的词向量文件的保存路径。生成的文件为文本文件，每行为一个词语及其向量表示，词语和向量之间，向量中的各个数字之间使用空格分隔。

#### 算法参数
- 词向量维度：要训练的词向量维度。
- 窗口大小：skip-gram 算法中的 window_size 参数。
- 最小出现次数：只训练出现次数大于这一次数的词语的词向量。
- 训练 epoch 数：训练数据的训练次数。
- 学习率：即 learning_rate 参数。
- 批处理大小：即训练的 batch_size。
- 负采样个数：skip-gram 算法中的负样本个数。

#### 实例生成
1. 使用数据节点上传数据，数据格式请参见【输入参数】部分。
2. 将数据节点连接到 word2vec 节点，配置好保存路径，单击【运行】开始训练。



## Glove
#### 算法说明
Glove 是一种经典的词向量算法，能够从大量文本中学习出各个词语的向量表示。这一向量表示可以用作其它深度学习模型的初始值（[参考文档](http://www.aclweb.org/anthology/D14-1162)）。
#### 输入参数
- 训练数据：文本文件，词语之间使用空格分隔。
- 验证数据：（可选）包含四元组的数据集，如 word2vec 官方 questions-words.txt，用于评价生成的词向量的质量。

#### 输出参数
保存路径：生成的词向量文件的保存路径。生成的文件为文本文件，每行为一个词语及其向量表示，词语和向量之间，向量中的各个数字之间使用空格分隔。

#### 算法参数
- 词向量维度：要训练的词向量维度。
- 窗口大小：计算共现矩阵时使用的 window_size 参数。
- 最小出现次数：只训练出现次数大于这一次数的词语的词向量。
- 训练 epoch 数：训练数据的训练次数。
- 学习率：即 learning_rate 参数。
- 批处理大小：即训练的 batch_size。
- 最大词汇表大小：如果训练数据中单词总数超过这一大小 n，只训练出现频率最高的 n 个词的词向量。

#### 实例生成
1. 使用数据节点上传数据，数据格式请参考【输入参数】部分。
2. 将数据节点连接到 glove 节点，配置好保存路径，单击【运行】按钮开始训练。



## Sentence2Vec
#### 算法说明
Sentence2Vec 可以将句子转换为向量表示，转换方法有三种：
- 转换为词向量平均值。
- 转换为词向量加权平均值，权重由词频决定，词频越高，权重越低。
- 使用论文 [A simple but tough-to-beat baseline for sentence embeddings](https://openreview.net/pdf?id=SyK00v5xx) 的方法进行转换。


#### 输入参数
输入数据：文本文件，每一行为一个句子，词语之间使用空格分隔。

#### 输出参数
输出数据：文本文件，每一行为输入数据对应行的句子的向量表示，数字之间用空格分隔。

#### 算法参数
- 预训练词向量文本文件，格式请参考 word2vec 和 glove 算法的【输出参数】部分。
- 转换方式：三选一，average 对应词向量平均值；weighted_average 对应词向量加权平均值；svd 对应论文中的方法。

#### 实例生成
1. 使用数据节点上传数据，数据格式请参考【输入数据】部分。
2. 将数据节点连接到 sentence2vec 节点，配置好预训练词向量路径和转换方式，单击【运行】按钮开始训练。

