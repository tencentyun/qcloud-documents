##  HashingTF
 HashingTF 完成文本中词的 Term Frequency 的计算，该方法常见于文本数据的特征抽取。需要统计的词必须满足 minTF 和 minDF 两个条件，其中 minTF 指词w在该篇文档中出现的最小次数，minDF 指所有文档中包含词 w 的最小文档数。


#### 输入
支持本地数据和 COS 两种数据源。
#### 输出
  - 索引对应的tf值
    - 格式：|docId index1:tf1 index2:tf2 ...|
    - 说明：每行表示一篇文档中各个词索引对应的Term Frequency值。表示索引号为index1的词在该篇文档中出现了tf1次，以此类推。docId表示文档Id标识。各索引对之间以**空格**连接
  - 词汇的索引字典
    - 格式：| index | word |
    - 说明：以空格连接各字段
    - index：词汇索引index
    - word：索引对应的词汇

#### 参数
  - 文本所在列：待分析文本所在的列，从0开始计数。HDFS路径无需填写此参数
  - 文档ID所在列：文档ID所在列，从0开始计数。HDFS路径无需填写此参数
  - 并行数：训练数据的分区数、spark的并行数
  - 抽样率：输入数据的采样率
  - 最小词频数：一篇文章中待统计词语出现的最小词频，若为>=1的整数，表示词语出现的次数，若为[0,1)的Double类型值，表示词语出现数目与该篇文章词语总数的比值
  - 最小文本数：待统计词语出现在不同文章中的最小数量，若为>=1的整数，表示词语出现在不同文章中的数量，若为[0,1)的Double类型值，表示词语出现在不同文章中的数量与总文章数的比值


## TF-IDF
Term Frequency-Inverse Document Frequency （TF-IDF）是一个实用的文本特征处理算法，它的计算结果可以表示一篇文档中每个词汇在该文档中的相对重要程度，其中，Term Frequency 是指词 w 在该篇文档中出现的次数，Document Frequency 是指所有文档中包含词 w 的文档数。
  TF-IDF 的思想是词w在该篇文档中的重要性与 Term Frequency 成正比，与 Document Frequency 成反比，本算法的 TF 值采用 Term Frequency，IDF 值采用公式 log（(N+1)/(Document Frequency+1)），此处的 log 以10为底，N 表示文档总数。实践证明，TF-IDF 是一个非常有效的文本特征。

#### 输入
支持本地数据和COS两种数据源（详见3.1数据说明章节）

#### 输出
  - 索引对应的tfidf值
    - 格式：|docId index1:tfidf1 index2:tfidf2 ...|
    - 说明：每行表示一篇文档中各个词索引对应的TF-IDF值。表示索引号为index1的词对应的tfidf值为tfidf1，以此类推。docId表示文档Id标识。各索引对之间以**空格**连接。

  - 词汇的索引字典
    - 格式：| index | word | 
    - 说明：以空格连接各字段    
    - index：词汇索引
    - word：表示index对应的该词语

#### 参数
  - 文本所在列：待分析文本所在的列，从0开始计数。HDFS路径无需填写此参数
  - 文档ID所在列：文档ID所在列，从0开始计数。HDFS路径无需填写此参数
  - 并行数：训练数据的分区数、spark的并行数
  - 抽样率：输入数据的采样率
  - 最小词频数：一篇文章中待统计词语出现的最小词频，若为>=1的整数，表示词语出现的次数，若为[0,1)的Double类型值，表示词语出现数目与该篇文章词语总数的比值
  - 最小文本数：待统计词语出现在不同文章中的最小数量，若为>=1的整数，表示词语出现在不同文章中的数量，若为[0,1)的Double类型值，表示词语出现在不同文章中的数量与总文章数的比值


## Word2Vec_spark
Word2Vec_spark 是一种基于 Spark 实现的算法。Word2Vec 将文本中的词语映射到 K 维向量空间中，同时向量空间上的相似度可以用来表示词语语义上的相似度。首先根据 Word2Vec 算法计算词袋中每个词语对应的向量，然后针对每一篇文档中出现的词语，统计其词向量在 K 维空间上的平均值，以此代表该篇文档对应的向量。


#### 输入
支持本地数据和 COS 两种数据源（详见3.1数据说明章节）

#### 输出
  - word2Vec 转换结果
    - 格式：|Vector|
    - 说明：用每篇文档对应的向量代表该篇文档
      - HDFS 路径：向量中的各列以空格连接
      - TDW 路径：向量维数与表的字段个数对应

  - 词汇对应向量的输出
    - 格式：| word | Vector | 
    - 说明：以空格连接各字段
    - word：每个选定的词汇
    - Vector:每个 word 对应的向量


#### 参数
  - 文本列：待分析文本所在的列，从0开始计数。HDFS 路径无需填写此参数
  - 并行数：训练数据的分区数、Spark 的并行数
  - 抽样率：输入数据的采样率
  - 向量大小：选择的向量大小
  - 学习率：Double 类型
  - 最大迭代次数：默认为1
  - 随机数种子：默认为1
  - 最少统计次数：词袋中词汇被统计的最少出现次数
  - 训练模型的分区数：默认分区数为1，取值越小越准确
  - 上下文的窗口大小：选择上下文的窗口值

## WordSegment
WordSegment 完成对中文文本进行分词的目的，除了可以正常划分常用中文词语外，还可以正确识别数字与人名，同时实现了对用户自定义词典中的词汇的识别。

#### 输入
支持本地数据和COS两种数据源（详见3.1数据说明章节）

#### 输出
  - 格式：|word1 word2 ...|
  - 说明：中文分词结果中的各个词语，以**空格**连接

#### 参数
  - 文本所在列：待分析文本所在的列，从0开始计数。HDFS路径无需填写此参数
  - 并行数：训练数据的分区数、spark的并行数
  - 抽样率：输入数据的采样率
  - 自定义词典路径：用户的自定义词典路径，词典文件必须用UTF-8编码，并且每行一个自定义词
  - 是否标注词性：是否在分词结果中显示词性，默认不显示。词性标注含义可参考 [词性标注规范](https://github.com/NLPchina/ansj_seg/wiki/%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8%E8%A7%84%E8%8C%83)，同时，"udf"表示属于用户自定义词汇。

