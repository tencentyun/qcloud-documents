智能钛机器学习平台支持运行用户自定义的深度学习代码，您可以做如下操作：
 - 使用深度学习框架运行自定义代码
 - 部署使用深度学习框架生成的模型

目前有 PyCaffe，PyTorch，TensorFlow 三种框架可以选择，Horovod 和 TensorFlow PS 为分布式训练的不同方式。下面以 TensorFlow 组件为例，介绍如何在平台上运行自己的 TensorFlow 代码。可点击 [组件 demo](https://tio.cloud.tencent.com/ml/platform.html?projectId=28&flowId=90) 进行查看。如果有生成模型文件（pb 格式），可以通过 COS 或本地上传导入到 TI-ONE 的模型仓库，并部署成在线模型服务。

>!目前平台只支持导入 pb / pmml / angel 格式的模型。

![](https://main.qcloudimg.com/raw/4b326182a9123aad246122ec0f747e57.png)

##  运行自定义代码

本案例代码取自 tensorflow 官方 example，使用公共的 IRIS 数据集，在智能钛上完成下面两项工作：
1. 训练基于神经网络的分类器，评估分类器的效果。
2. 用训练好的分类器对新的数据进行预测代码，并展示如何让代码在智能钛平台上运行。

代码修改自 Tensorflow [官方项目](https://github.com/tensorflow/models/tree/master/samples/core/get_started)。

```python

#  Copyright 2016 The TensorFlow Authors. All Rights Reserved.
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
"""An Example of a DNNClassifier for the Iris dataset."""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse

import tensorflow as tf

import iris_data

parser = argparse.ArgumentParser()

parser.add_argument('--batch_size', default=100, type=int,
                    help='batch size')
parser.add_argument('--train_steps', default=1000, type=int,
                    help='number of training steps')
parser.add_argument('--train_path', type=str,
                    help='path to the train data file.')
parser.add_argument('--test_path', type=str,
                    help='path to the test data file.')
parser.add_argument('--export_dir', type=str,
                    help='path to export the train model (.pb file)')


def main(argv):
    args = parser.parse_args(argv[1:])

    # Fetch the data
    (train_x, train_y), (test_x, test_y) = iris_data.load_data(args.train_path,
                                                               args.test_path)

    # Feature columns describe how to use the input.
    my_feature_columns = []
    for key in train_x.keys():
        my_feature_columns.append(tf.feature_column.numeric_column(key=key))

    # Build 2 hidden layer DNN with 10, 10 units respectively.
    classifier = tf.estimator.DNNClassifier(
        feature_columns=my_feature_columns,
        # Two hidden layers of 10 nodes each.
        hidden_units=[10, 10],
        # The model must choose between 3 classes.
        n_classes=3,
    )

    # Train the Model.
    classifier.train(
        input_fn=lambda: iris_data.train_input_fn(train_x, train_y,
                                                  args.batch_size),
        steps=args.train_steps)

    # Evaluate the model.
    eval_result = classifier.evaluate(
        input_fn=lambda: iris_data.eval_input_fn(test_x, test_y,
                                                 args.batch_size))

    def serving_input_receiver_fn():
        input_placeholder = tf.placeholder(shape=[4], dtype=tf.string)
        sepal_length_placeholder = tf.strings.to_number(input_placeholder[0:1])
        sepal_width_placeholder = tf.strings.to_number(input_placeholder[1:2])
        petal_length_placeholder = tf.strings.to_number(input_placeholder[2:3])
        petal_width_placeholder = tf.strings.to_number(input_placeholder[3:])

        # How the input data is fed into model_fn.
        features = {
            "SepalLength": sepal_length_placeholder,
            "SepalWidth": sepal_width_placeholder,
            "PetalLength": petal_length_placeholder,
            "PetalWidth": petal_width_placeholder
        }

        return tf.estimator.export.ServingInputReceiver(features,
                                                        input_placeholder)

    if not tf.gfile.Exists(args.export_dir):
        tf.gfile.MakeDirs(args.export_dir)

    classifier.export_saved_model(
        export_dir_base=args.export_dir,
        serving_input_receiver_fn=serving_input_receiver_fn)

    print('\nTest set accuracy: {accuracy:0.3f}\n'.format(**eval_result))

    # Generate predictions from the model
    expected = ['Setosa', 'Versicolor', 'Virginica']
    predict_x = {
        'SepalLength': [5.1, 5.9, 6.9],
        'SepalWidth': [3.3, 3.0, 3.1],
        'PetalLength': [1.7, 4.2, 5.4],
        'PetalWidth': [0.5, 1.5, 2.1],
    }

    predictions = classifier.predict(
        input_fn=lambda: iris_data.eval_input_fn(predict_x,
                                                 labels=None,
                                                 batch_size=args.batch_size))

    template = '\nPrediction is "{}" ({:.1f}%), expected "{}"'

    for pred_dict, expec in zip(predictions, expected):
        class_id = pred_dict['class_ids'][0]
        probability = pred_dict['probabilities'][class_id]

        print(template.format(iris_data.SPECIES[class_id],
                              100 * probability, expec))


if __name__ == '__main__':
    tf.logging.set_verbosity(tf.logging.INFO)
    tf.app.run(main)

```

## 参数配置

### 组件参数

将 TensorFlow 组件拖至画布内，点击右侧的【程序脚本】，上传本地编写的脚本，内容为上文代码。

如果入口脚本需要 import 项目中的其它自己编写的模块，需要将其它模块的代码压缩成 zip 包，并上传到【Tensorflow】组件的【依赖包文件】中，该 zip 包会被添加到 Python 的 path 中。在本案例中，我们将 iris_data.py 和 estimator_test.py 两个文件压缩成 iris.zip，上传至【依赖包文件】中，此时，程序脚本中可以使用 import iris_data 来引入这一模块。

![](https://main.qcloudimg.com/raw/a9ea4eb1ded824bd5ae9f5fdc8219c2b.png)

程序参数填入用户自定义参数，自定义参数将会传递给入口 py 文件。
```
--train_path ${ai_dataset_lib}/demo/other/iris_training.csv

--test_path ${ai_dataset_lib}/demo/other/iris_test.csv

--export_dir ${cos}
```

### 资源参数

这里需要给出针对当前的例子，我们推荐的GPU/CPU/内存配置

- GPUs：GPU 的数量，0
- CPUs：CPU 的数量，1
- Memory(m)：2560


### 查看运行状态和结果

保存任务流，点击运行。运行成功后在组件上右击，单击【日志信息】——【Tensorflow 控制台】——【App 详情】中查看 stdout 和 stderr 两个日志。


## 生成并部署 TensorFlow 模型

### 生成模型

智能钛平台支持 TensorFlow 的模型部署，用户在 TensorFlow 代码中生成模型后，可在模型仓库页面进行部署。

### 部署及预测

模型的部署和预测分为两步：
 - 导入模型
 - 模型部署及预测

此处以 tensorflow 框架生成的模型为例进行模型服务部署和在线预测功能展示，详细文档请查看 [模型仓库及模型服务部署](https://cloud.tencent.com/document/product/851/35158)。

#### 导入模型

前往【模型仓库】页面，点击【导入模型】，按照页面弹窗提示输入所需要的信息。

tensorflow 模型部署时，模型文件中需要包含.pb 格式类型的文件，上文代码中.pb 格式类型文件已生成，用户可自行查看。.pb 格式类型的文件将存储在 cos 中，用户可下载模型文件并打包成.zip 格式。

![](https://main.qcloudimg.com/raw/d0cfcbdb23b9ad95722801f5a353bf3d/201910091030.png)
![](https://main.qcloudimg.com/raw/2b12547f789ff8d7e57bbc35cda88f2d/201910091031.png)

>!若部分存量用户所选存储桶不在【重庆地域】，请先至存储桶中将数据下载到本地。

成功导入后，模型将在【模型服务】中展示。


#### 模型部署及预测

进入 【模型仓库】页面，输入模型服务名称及运行环境与资源，模型将成功部署。

![](https://main.qcloudimg.com/raw/6104ac7583d4df933d1cba853a93810c/201909301145.png)


在此模型的【操作】栏中选择【更多】-【测试】-【JSON】，输入符合规范的 json 代码即可在线预测。

例如本案例中输入：

`{"inputs": ["5.1", "3.3", "1.7", "0.5"]}`

![](https://main.qcloudimg.com/raw/1d00b3a635cb0eff6849b71813d01417/201910091026.png)

