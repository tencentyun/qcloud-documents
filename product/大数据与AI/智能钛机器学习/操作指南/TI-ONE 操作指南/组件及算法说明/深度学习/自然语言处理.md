
## BERT 文本分类

BERT文本分类算法首先使用 BERT 网络，产生要分类的句子的向量表示，再通过全连接层网络对句子进行分类。

#### 算法 IO 参数
- 训练数据：每一行为一个句子或两个句子，词与词之间用空格分隔，句子之间以及句子和标签之间用特定分隔符分隔（分隔符在算法参数中可以设置），**句子在分隔符左侧，标签在分隔符右侧**。
- 验证数据：格式同训练数据。
- 模型目录：存储模型文件和 events 文件的路径，可以从该路径启动 tensorboard。
- bert 参数目录：存储预训练的 bert 参数的目录，对于中文和英文，可以分别下载 [压缩包一](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip) 和 [压缩包二](https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip)，并填写解压后的目录

#### 算法参数
- 标签分隔符：用于分隔句子和标签的分隔符。
- 批处理大小：即训练的 batch_size。
- 训练 epoch 数：训练数据的训练次数。
- 学习率：即 learning_rate。
- 句子分隔符：对于输入为两个句子的数据，分隔两个句子的分隔符。
- 最大句子长度：BERT 要求固定的输入长度，因此不足这一长度的句子将被补齐，超过的将被截断。

#### 模型 IO 参数
- 预测数据输入：要预测的数据，格式同训练数据，可以没有标签。
- 输出结果路径：存放预测结果的 csv 文件的路径。csv 文件中每一行是一个句子及其预测结果，每一行中，第一列是要预测的句子，第二列是预测的类别，第三列是真实的类别。
- bert 参数目录：同算法 IO 参数中的 bert 参数目录。

#### 模型参数：
- 标签分隔符：用于分隔句子和标签的分隔符。
- 句子分隔符：对于输入为两个句子的数据，分隔两个句子的分隔符。

#### 实例生成
1. 使用数据节点，上传数据，数据格式见上文【训练数据】部分。
2. （可选）如数据只有一份，可以使用【输入】>【数据转换】中的【文本数据切分】节点，将上传的数据按比例分为训练数据和验证数据。
3. 将两份数据分别连接到 BERT 文本分类节点的两个输入桩。单击【运行】开始训练。


## LSTM 文本分类

LSTM 文本分类算法首先使用双向 LSTM 网络（[参考文档](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320&rep=rep1&type=pdf)）产生要分类的句子的向量表示，再通过全连接层网络对句子进行分类。


#### 算法 IO 参数
- 训练数据：每一行为一个句子，词与词之间用空格分隔，句子和标签之间用特定分隔符分隔（分隔符在算法参数中可以设置），**句子在分隔符左侧，标签在分隔符右侧**。
- 验证数据：格式同训练数据。
- 模型目录：存放模型文件和日志文件的目录。（如果目录中已有模型文件，下次训练时将加载目录中最新的模型文件。因此，如果改变了网络结构或更换了数据，请更换或清空模型目录 。

#### 算法参数
- 分隔符：用于分隔句子和标签的分隔符。
- 词向量维度：网络中词向量的维度。
- LSTM 维度：网络中句子的向量表示的维度。
- 批处理大小：即训练的 batch_size。
- 训练 epoch 数：训练数据的训练次数。
- 学习率：即 learning_rate。
- 是否使用预训练好的词向量：如设为 True，可填写词向量文件路径。词向量文件格式与 glove 词向量官方格式相同。**如果使用预训练好的词向量，预训练词向量的维度应等于参数【词向量维度】的值**。

#### 模型 IO 参数
- 预测数据输入：要预测的数据，格式同训练数据，可以没有标签。
- 输出结果路径：存放预测结果的 csv 文件的路径。csv 文件中每一行是一个句子及其预测结果，每一行中，第一列是要预测的句子，第二列是预测的类别，第三列是真实的类别。

#### 模型参数
分隔符：预测数据中分隔句子和标签的分隔符。

#### 实例生成
1. 使用数据节点上传数据，数据格式请参考【训练数据】部分。
2. （可选）如数据只有一份，可以使用【输入】>【数据转换】中的【文本数据切分】节点，将上传的数据按比例分为训练数据和验证数据。
3. 将两份数据分别连接到 LSTM 句子分类节点的两个输入桩，单击【运行】开始训练。



## Fasttext 文本分类

Fasttext 是一种简单有效的句子分类算法， 通过词向量以及 ngram 向量的平均值计算出句子的向量表示，再通过全连接层网络对句子进行分类（[参考文档](https://arxiv.org/pdf/1607.01759.pdf)）。


#### 算法 IO 参数
- 训练数据：每一行为一个句子，词与词之间用空格分隔，句子和标签之间用特定分隔符分隔（分隔符在算法参数中可以设置）。**句子在分隔符左侧，标签在分隔符右侧**。
- 验证数据：格式同训练数据。
- 模型目录：存放模型文件和日志文件的目录。（如果目录中已有模型文件，下次训练时将加载目录中最新的模型文件。因此，如果改变了网络结构或更换了数据，请更换或清空模型目录。

#### 算法参数
- 分隔符：用于分隔句子和标签的分隔符
- 词向量维度：网络中词向量的维度。
- 批处理大小：即训练的 batch_size。
- 训练 epoch 数：训练数据的训练次数。
- 学习率：即 learning_rate。
- 是否使用预训练好的词向量：如设为 True，可填写词向量文件路径。词向量文件格式与 glove 词向量官方格式相同。**如果使用预训练好的词向量，预训练词向量的维度应等于参数【词向量维度】的值**。

#### 模型 IO 参数
- 预测数据输入：要预测的数据，格式同训练数据，可以没有标签。
- 输出结果路径：存放预测结果的 csv 文件的路径。csv 文件中每一行是一个句子及其预测结果，每一行中，第一列是要预测的句子，第二列是预测的类别，第三列是真实的类别。

#### 模型参数

- 分隔符：预测数据中分隔句子和标签的分隔符。

#### 实例生成
1. 使用数据节点，上传数据，数据格式请参考【训练数据】部分。
2. （可选）如数据只有一份，可以使用【输入】>【数据转换】中的【文本数据切分】节点，将上传的数据按比例分为训练数据和验证数据。
3. 将两份数据分别连接到 Fasttext 节点的两个输入桩，单击【运行】开始训练。


## TextCNN 文本分类

#### 算法说明
TextCNN 使用卷积神经网络产生句子的向量表示，再通过全连接层网络对句子进行分类（[参考文档](https://arxiv.org/pdf/1408.5882.pdf)）。


#### 算法 IO 参数
- 训练数据：每一行为一个句子，词与词之间用空格分隔，句子和标签之间用特定分隔符分隔(分隔符在算法参数中可以设置)。**句子在分隔符左侧，标签在分隔符右侧**。
- 验证数据：格式同训练数据。
- 模型目录：存放模型文件和日志文件的目录。（如果目录中已有模型文件，下次训练时将加载目录中最新的模型文件。因此，如果改变了网络结构或更换了数据，请更换或清空模型目录）。

#### 算法参数
- 分隔符：用于分隔句子和标签的分隔符。
- 词向量维度：网络中词向量的维度。
- 各层网络卷积核大小：即 kernel_size。
- 各层网络卷积核个数：即通道数。
- 批处理大小：即训练的 batch_size。
- 训练 epoch 数：训练数据的训练次数。
- 学习率：即 learning_rate。
- 是否使用预训练好的词向量：如设为 True，可填写词向量文件路径。词向量文件格式与 glove 词向量官方格式相同。**如果使用预训练好的词向量，预训练词向量的维度应等于参数【词向量维度】的值**。

#### 模型 IO 参数
- 预测数据输入：要预测的数据，格式同训练数据，可以没有标签。
- 输出结果路径：存放预测结果的 csv 文件的路径。csv 文件中每一行是一个句子及其预测结果，每一行中，第一列是要预测的句子，第二列是预测的类别，第三列是真实的类别。

#### 模型参数
分隔符：预测数据中分隔句子和标签的分隔符。

#### 实例生成
1. 使用数据节点上传数据，数据格式请参考【训练数据】部分。
2. （可选）如数据只有一份，可以使用【输入】>【数据转换】中的【文本数据切分】节点，将上传的数据按比例分为训练数据和验证数据。
3. 将两份数据分别连接到 TextCNN 节点的两个输入桩，单击【运行】开始训练。

## BiLSTM-CRF 序列标注

BiLSTM-CRF 是一种常用的序列标注模型，能用于分词，词性标注，命名实体识别等序列标注任务。模型使用双向 LSTM 网络产生句子中的各个词语的向量表示，并据此计算词语标签的概率分布，然后使用 CRF 计算总概率最大的标签序列（[参考文档](https://arxiv.org/pdf/1508.01991.pdf)）。

#### 算法 IO 参数
- 训练数据：文本文件，其中每一行是一个句子及其各个词语的对应标签，句子和标签之间由特定分隔符分隔（默认为 \_\_label\_\_），句子中的各个词语之间，以及各个标签之间用空格分隔。由于算法对句子中的每个词语进行标注，词语个数必须等于标签个数。
- 验证数据：文本文件，格式与训练数据相同。
- 模型目录：存放模型文件和日志文件的目录。（如果目录中已有模型文件，下次训练时将加载目录中最新的模型文件。因此，如果改变了网络结构或更换了数据，请更换或清空模型目录）。

#### 算法参数

- 批处理大小：即算法 batch_size。
- 学习率：即算法的 learning_rate。
- 训练次数：即将训练数据训练的 epoch 数。
- 词向量维度：即每个词语向量表示的维度。
- LSTM 维度：即每个词语 LSTM 向量表示的维度。
- 使用预训练好的词向量模型：如设为 True，可填写词向量文件路径。词向量文件格式与 glove 词向量官方格式相同。**如果使用预训练好的词向量，预训练词向量的维度应等于参数【词向量维度】的值**。

#### 模型 IO 参数
- 预测数据输入：要进行预测的数据，格式同训练数据，可以没有标签。
- 预测结果输出路径：存放预测结果的 csv 文件的路径。csv 文件中每行为一个句子及其预测结果，每一行中，第一列是要预测的句子，第二列是预测的标注结果，第三列是真实的标注结果。

#### 模型参数
批处理大小：预测时的 batch_size。

####  实例生成
1. 使用数据节点，上传数据，数据格式请参考【训练数据】部分。
2. （可选）如数据只有一份，可以使用【输入】>【数据转换】中的【文本数据切分】节点，将上传的数据按比例分为训练数据和验证数据。
3. 将两份数据分别连接到 BiLSTM-CRF 节点的两个输入桩，单击【运行】开始训练。

   
## Word2Vec 词向量

Word2Vec 是一种经典的词向量算法，能够从大量文本中学习出各个词语的向量表示。这一向量表示可以用作其它深度学习模型的初始值（[参考文档](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)）。

#### 输入参数
- 训练数据：文本文件，词语之间使用空格分隔。
- 验证数据：（可选）包含四元组的数据集，如 word2vec 官方 questions-words.txt，用于评价生成的词向量的质量。

#### 输出参数
保存路径：生成的词向量文件的保存路径。生成的文件为文本文件，每行为一个词语及其向量表示，词语和向量之间，向量中的各个数字之间使用空格分隔。

#### 算法参数
- 词向量维度：要训练的词向量维度。
- 窗口大小：skip-gram 算法中的 window_size 参数。
- 最小出现次数：只训练出现次数大于这一次数的词语的词向量。
- 训练 epoch 数：训练数据的训练次数。
- 学习率：即 learning_rate 参数。
- 批处理大小：即训练的 batch_size。
- 负采样个数：skip-gram 算法中的负样本个数。

#### 实例生成
1. 使用数据节点上传数据，数据格式请参见【输入参数】部分。
2. 将数据节点连接到 word2vec 节点，配置好保存路径，单击【运行】开始训练。

## Glove 词向量

Glove 是一种经典的词向量算法，能够从大量文本中学习出各个词语的向量表示。这一向量表示可以用作其它深度学习模型的初始值（[参考文档](http://www.aclweb.org/anthology/D14-1162)）。

#### 输入参数
- 训练数据：文本文件，词语之间使用空格分隔。
- 验证数据：（可选）包含四元组的数据集，如 word2vec 官方 questions-words.txt，用于评价生成的词向量的质量。

#### 输出参数
保存路径：生成的词向量文件的保存路径。生成的文件为文本文件，每行为一个词语及其向量表示，词语和向量之间，向量中的各个数字之间使用空格分隔。

#### 算法参数
- 词向量维度：要训练的词向量维度。
- 窗口大小：计算共现矩阵时使用的 window_size 参数。
- 最小出现次数：只训练出现次数大于这一次数的词语的词向量。
- 训练 epoch 数：训练数据的训练次数。
- 学习率：即 learning_rate 参数。
- 批处理大小：即训练的 batch_size。
- 最大词汇表大小：如果训练数据中单词总数超过最大词汇量 n，则只训练出现频率最高的 n 个词的词向量。

#### 实例生成
1. 使用数据节点上传数据，数据格式请参考【输入参数】部分。
2. 将数据节点连接到 glove 节点，配置好保存路径，单击【运行】开始训练。

## 中文文本摘要

中文文本摘要首先使用 BERT 网络，可查看 [论文链接](https://arxiv.org/pdf/1810.04805.pdf)，产生文中句子的向量表示，再通过 TextRank 算法抽取权重较高的句子。

#### 输入参数

- 文本数据输入：要抽取摘要的文本，要求每行为一个句子。

#### 输出参数

- 输出数据：抽取后的摘要。

#### 算法参数
摘要句子数目：从文本中选为摘要的句子数量。

#### 实例生成
1. 使用数据节点，上传数据，数据格式见上文【文本数据输入】部分。
2. 将输入数据连接到【中文文本摘要】节点的输入桩，单击【运行】开始运行。


## 中文关键词抽取

中文关键词抽取算法使用 TF-IDF 抽取输入文本中的关键词。

#### 输入参数

- 文本数据输入：要抽取关键词的文本，无需预先分词。

#### 输出参数

- 输出数据：抽取后的关键词。

#### 算法参数
关键词个数：从文本中抽取的关键词数量。

#### 实例生成
1. 使用数据节点，上传数据，数据格式见上文【文本数据输入】部分。
2. 将输入数据连接到【中文关键词抽取】节点的输入桩，单击【运行】开始运行。


## 中文词频统计

词频统计节点统计文本中词语的出现频率，并从高到低排列。

#### 输入参数

- 输入数据：要统计词频的文本。

#### 输出参数

- 输出文件：词频统计结果。

#### 实例生成
1. 使用数据节点，上传数据，数据格式见上文【文本数据输入】部分。
2. 将输入数据连接到【词频统计】节点的输入桩，单击【运行】开始运行。


## 中文新词发现

PMI（点互信息）和左右熵能够刻画一个文本片段的凝固程度和灵活运用程度，因此可以用于发现文本中不存在词库中的新词。

#### 输入参数
- 输入数据：未分词的中文纯文本文件。
- 词库：（可选）文本文件，每行为一个词。如发现的新词已经存在这个词库中，则跳过这一新词。

#### 输出参数
输出数据：算法发现的新词，每行为一个词。

#### 算法参数
- 最大词语长度：只考虑长度不超过这一长度的文本片段构成新词的可能性。
- 保留前 n 个新词：只保留可能性最大的前若干个新词。

#### 实例生成
1. 使用数据节点上传数据，数据格式请参考【输入数据】部分
2. 将数据连接到【新词发现】节点的输入桩，设置好输出数据路径，单击【运行】开始发现新词。

## BERT-CRF 序列标注

BERT-CRF 是一种表现十分出色的序列标注模型， 能用于分词，词性标注，命名实体识别等序列标注任务。模型使用 BERT 产生句子中各个字的向量表示，并据此计算词语标签的概率分布，然后使用 CRF 计算总概率最大的标签序列。

#### 算法 IO 参数
- 训练数据：文本文件，其中每一行是一个句子及其各个词语的对应标签，句子和标签之间由特定分隔符分隔（默认为 \_\_label\_\_），句子中的各个词语之间，以及各个标签之间用空格分隔。由于算法对句子中的每个词语进行标注，词语个数必须等于标签个数。
- 验证数据：文本文件，格式与训练数据相同，选填。
- 模型目录：存储模型文件和 events 文件的路径，可以从该路径启动
- bert 参数目录：存储预训练的 bert 参数的目录，对于中文和英文，可以分别下载 [压缩包一](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip) 和 [压缩包二](https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip)，并填写解压后的目录。

#### 算法参数
- 最大句子长度：BERT 要求固定的输入长度，因此不足这一长度的句子将被补齐，超过的将被截断。
- 标签分隔符：用于分割句子和标签的分隔符。
- 批处理大小：即训练的 batch_size。
- 训练 epoch 数：训练数据的训练次数。
- 学习率：即 learning_rate。
- 输入转化：带标签的训练数据最小单位是词， 而 BERT 的输入数据最小单位应该为字，需要转换原始数据。
- 存储 checkpoint 步数：即存储 checkpoint 的频率。

#### 模型 IO 参数
- 预测数据：文本数据，数据可包含标签信息以备模型评估使用。格式与训练数据相同。
- bert 参数目录：存储预训练的 bert 参数的目录，对于中文和英文，可以分别下载[压缩包一](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip)和[压缩包二](https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip)，并填写解压后的目录
- 预测结果输出：输出预测结果的 csv 文件的路径。csv 文件中每行为一个句子及其输出结果，每行中第一列为句子中的单词（以空格分隔），第二列为句子中单字的真实标注结果（以空格分隔），第三列为句子中单词的预测标注结果（以空格分隔）。如果output_transformation 参数为 True（待预测文本为中文文本），那么第四列为根据预测标签合并词语后的分词序列，第五列为根据预测标签合并词语后的词性标签序列。 

#### 模型参数
* 预测数据是否标注：如果为 True，模型输出文件中会包含数据原标签，可用于后续评估。
* 输入数据转化：若为 True，则表明输入预测句子未作序列化处理，即由句子转化为一系列单字。
* 输出数据转化：若为 True，则对输出数据做分词处理，即从字级别转换为词级别序列。
* 标签分隔符：用于分割句子和标签的分隔符。
* 最大句子长度：与训练时最大句子长度保持一致。

#### 实例生成
1. 使用数据节点上传数据，数据格式请参考【训练数据】部分。
2. （可选）如数据只有一份，可以使用【输入】> 【数据转换】中的【文本数据切分】节点，将上传的数据按比例分为训练数据和验证数据。
3. 将两份数据分别连接到 BERT-CRF 节点的前两个输入桩，单击【运行】开始训练。

## BERT 中文问答

BERT 可用于机器阅读理解任务，机器阅读理解指问题答案能够在提供文本中找到对应的表示。模型使用 BERT 产生句子中各个字的向量表示，并据此计算出每个字作为答案开头和答案结尾的 logit 值，最后逻辑判断确定文本中答案所在区域。

#### 算法 IO 参数
- 训练数据：json文件，下图表示文件中一个key：value 对。其中 Q_ANN_VAL 表示问题答案对序号、question 表示问题描述、evidences 表示该问题的一至多个证据，其中每一个证据由序号（Q_ANN_VAL)，答案（answer）以及证据（evidence）组成。 
![](https://main.qcloudimg.com/raw/5cde833280f6c86508b2fe7628d49abd.png)
- 模型目录：存储模型文件和 events 文件的路径，可以从该路径启动
- bert 参数目录：存储预训练的 bert 参数的目录，对于中文和英文，对于中文和英文，可以分别下载 [压缩包一](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip) 和 [压缩包二](https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip)，并填写解压后的目录。

#### 算法参数
- 最大句子长度：BERT 要求固定的输入长度，因此不足这一长度的句子将被补齐，超过的将被截断。
- 最大问题长度：BERT 的输入为问题和证据相连后的句子，因此对问题长度做了限制，超过这一长度的问题将被截断。
- 批处理大小：即训练的 batch_size。
- 训练 epoch 数：训练数据的训练次数。
- 学习率：即 learning_rate。
- 存储 checkpoint 步数：即存储 checkpoint 的频率。

#### 模型 IO 参数
- 预测数据：json 文件，格式与训练数据保持一致，文件中可不含 answer 标签。
- bert 参数目录：存储预训练的 bert 参数的目录，对于中文和英文，可以分别下载 [压缩包一](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip) 和 [压缩包二](https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip)，并填写解压后的目录
- 预测结果输出：输出预测结果的 csv 文件的路径。csv 文件中每行为一个问题-证据对及其输出结果，每行中第一列为问题-证据对的编号，第二列为问题，第三题为证据，第四列为真实答案，第五列为预测答案。
#### 模型参数

- 最大预测答案长度：最大的预测答案长度。
- 是否支持 no_answer 结果预测： 若为 True，模型预测时会考虑 no answer 的情况。
- null score 与 best_no_null score 之间阈值：该参数为调整模型预测 no answer 的能力而设定。只有 no answer 的分数与非 no answer 的分数之差大于该阈值，才判定为 no answer， 该阈值可为负数。
- 预测标签是否标注：如果为True，模型输出文件中会包含数据原标签，可用于后续评估。
- 最大句子长度：与算法参数保持一致。
- 最大问题长度：与算法参数保持一致。

#### 实例生成
1. 使用数据节点上传数据，数据格式请参考【训练数据】部分。
2. 将训练数据连接到 BERT 的第一个桩点，单击【运行】开始训练。

## 文本相似度计算

#### 算法说明
文本相似度计算是一个二分类问题， 模型的输入为一对句子，输出为0或1。0代表两个句子不相似，1代表相似。算法采用 Bag of Words 的思想， 通过对词向量的计算获得两个句子的向量表示，计算句子之间的余弦距离，并与通过训练数据获得的最优阈值比较确定标签。其本质为无监督学习模型。

#### 算法 IO 参数
- 训练数据：文本文件，其中每一行是一对句子及其各个词语的对应标签，句子和句子之间有特定的分隔符分隔（默认为\_\_sentence\_\_），句子与标签之间由特定分隔符分隔（默认为 \_\_label\_\_）。句子必须预先分词，各个词语之间用空格分隔。
- 预训练词向量：预训练词向量文件，其中每一行为词及其对应的词向量。

#### 算法参数
- 句子分隔符：用于分割两个句子的分隔符。
- 标签分隔符：用于分割句子和标签的分隔符。
- 词向量维度：确定预训练词向量的维度，与预训练词向量的维度一致。
- 阈值下界：计算最优阈值时所考虑的下界。
- 阈值上界：计算最优阈值时所考虑的上界。
- 阈值更新步长：计算最优阈值时更新步长。

#### 模型 IO 参数
-  预测数据：文本文件，格式与训练数据保持一致，可不包含标签信息。
-  预测结果输出：输出预测结果的 csv 文件的路径。csv 文件中每行为一个句子对及其输出结果，每行中第一列为待判断的句子对。第二列为句子关系的真实标签（相似为1，不相似为0），第三列为句子关系的预测标签。

#### 模型参数

- 预测数据是否标注： 如果为 True，模型输出文件中会包含数据原标签，可用于后续评估。
- 句子分隔符：用于分割句子和标签的分隔符。
- 标签分隔符：用于分割句子和标签的分隔符。如果预测数据无标注信息，用户可忽略此参数。

#### 实例生成
1. 使用数据节点上传数据，数据格式请参考【训练数据】部分。
2. （可选）如数据没有做中文分词及去关键词处理，则需要拉取中文分词及去关键词算子进行数据预处理。
3. 将训练数据连接到算子的输入桩，单击【运行】开始训练。

## BERT 分类模型蒸馏
#### 算法说明
BERT 分类模型准确率较高，但参数量较多，模型较大，推理速度也较低。BERT 分类模型蒸馏算法能够用小型的网络从微调过的 BERT 文本分类模型中学习信息。算法使用双向 LSTM 网络（[论文链接](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320&rep=rep1&type=pdf)）拟合微调过的 BERT 分类模型。

#### 参数设置
#### 算法 IO 参数
- 蒸馏数据：每一行为一个句子，词与词之间用空格分隔，句子和标签之间用特定分隔符分隔（分隔符在算法参数中可以设置）。一般为 BERT 文本分类模型使用的训练集。
- BERT文本分类模型目录：在智能钛平台上训练的 BERT 文本分类模型的模型目录。
- 模型目录：用于保存模型文件和 events 文件的目录。
- 词向量文件路径：用于初始化  LSTM 网络的词向量文件路径，可以为空。
- 验证数据：用于评估蒸馏效果的数据目录，格式与蒸馏数据相同。

#### 算法参数
- 蒸馏训练次数：蒸馏过程遍历蒸馏数据的次数。
- 学习率：即 learning_rate。
- 批处理大小：即训练的 batch_size。
- 标签分隔符：一行中分隔句子和标签的分隔符。如果为\t，请填写 tab。
- 词向量维度：网络中词向量的维度。
- LSTM维度：网络中句子的向量表示的维度。

#### 模型 IO 参数
- 预测数据：要预测的数据路径，如果有标注，格式与蒸馏数据相同。如果无标注，需要一行为一个句子，单词/词语之间用空格分隔。
- 预测结果输出路径：存放预测结果的 csv 文件的输出路径。csv 文件中第一列为句子，第二列为正确标签（如果数据有标注，否则为空），第三列为预测结果。

#### 模型参数
标签分隔符：一行中分隔句子和标签的分隔符。如果为\t，请填写 tab。

#### 实例生成	
1. 参照【BERT 文本分类算法】文档，训练一个BERT 文本分类模型。
2. 将【BERT 文本分类算法】的训练集连接到【BERT 分类模型蒸馏】的【蒸馏数据】输入桩，将前者的【模型目录】连接到后者的【BERT 本分类模型目录】输入桩，将验证数据连接到后者的【验证数据】输入桩。
3. 填写其它参数，配置相应资源，开始蒸馏，蒸馏完成后，可以在日志中查看蒸馏效果。




