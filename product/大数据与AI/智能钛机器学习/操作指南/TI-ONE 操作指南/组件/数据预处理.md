## 概述
数据集通常需要进行预处理后才能进行模型训练，预处理方法主要包括以下处方式：特征提取、特征降维、特征空值处理、特征转换（one-hot）、特征归一化等。
通过 TI-ONE 组件，能够实现常用的数据预处理，用到的组件及其作用如下：

| 组件名称 | 组件功能                          |
| -------- | --------------------------------- |
| Pyspark  | 可对数据进行 SQL 处理，填充缺失值 |
| Dummy    | 可将特征交叉处理                  |
| Scaler   | 可将数据归一化                    |
| Spliter  | 可将数据切分为训练集和验证集      |

## 操作步骤
以特征归一化为例，进行数据预处理步骤的介绍。
1. 在左侧栏【算法】>【机器学习算法】>【特征转换】 导航位置下，选择【Scaler】组件，将其拖拽到画布，再将输入数据连接到该组件。
   ![](https://main.qcloudimg.com/raw/a6fbfb50573a78b82c4d12b2babddf14.png)
2. 单击组件打开右侧配置区域，需配置 Scaler 的算法参数，单击【特征配置文件】右侧的文本框，在展开的资源列表中选择【新建脚本】。
   ![](https://main.qcloudimg.com/raw/11a11b16e72040d8bda0877c35e1c540.png)
   ![](https://main.qcloudimg.com/raw/ff1a449de0bbb2baef23715dcb667c1a.png)
3. 填写脚本名、脚本类型和脚本内容，并保存。
   ![](https://main.qcloudimg.com/raw/535454e6a7c4164900bc357597e7adb4.png)

至此，我们已完成了归一化组件的相关配置。在运行完该节点后，右键单击组件，选择查看输出，可进行归一化数据中间结果的预览。   
![](https://main.qcloudimg.com/raw/3ed412999f811f26f99d02b22f54285e.png)


除了在程序中对数据做预处理，在上游系统对数据作SQL操作，平台的PySpark组件也可以实现简单的SQL处理。


>? Spark 组件是面向使用 Python 的 Spark 用户，用户通过 Python 编写 Spark 应用程序，通过 PySpark 组件完成部署，也支持pyspark的sql功能，本文有部分使用方法介绍（更多用法请参考社区指引：[Spark SQL Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html)）。

和标准的Spark相比，pySpark支持上传Python脚本和实时修改，更加的灵活，而且支持SQL功能，所以我们推荐用来进行数据预处理。

1. 从左侧组件列表中，拖拽出一个 PySpark 节点到画布。
![](https://main.qcloudimg.com/raw/b3347ed7c805332139b03fd5f8edfb90.png)
2. 单击任务节点，会从右侧弹出配置框。
![](https://main.qcloudimg.com/raw/bb1a61a1165ed1edb03015e86eac9161.png)

## 入门示例

首先通过一个简单的例子来看看如何使用python写spark应用程序，如下代码显示通过`range(1, 4)`初始化构造弹性分布式数据集`RDD`，经过transformation方法`map`转换后得到一种新的形式，最后调用action方法`collect`收集结果并打印。

 ![](https://main.qcloudimg.com/raw/3692e0cab38f5b68d462c26f707f2eea.png)

## RDD Style

下面代码显示基于 RDD API，从COS上读取原始数据，经过一系列转换处理后，得到每条记录的前4列，然后通过action方法`take(2)`仅拿出两条记录打印出来看看处理效果，如果处理正确，可以通过`saveAsTextFile`将结果保存到COS结果目录中。

![](https://main.qcloudimg.com/raw/3692e0cab38f5b68d462c26f707f2eea.png)

## DataFrame Style

上述基于 RDD 的代码可能对数据分析人员来说并不是很好懂，用过scikit-learn的同学可能会对dataframe更熟悉，下面代码显示直接通过spark的提供的csv接口加载源数据得到dataframe，并打印schema信息，选出”age”和”marital”两列，显示4条记录。

![](https://main.qcloudimg.com/raw/2bbefdaf984253359880c8b6558921bc.png)

从以上代码可以看到，通过csv直接加载得到的dataframe，”age”列是`string`类型，我们可以使用`cast`表达式将其转换为`int`类型。
![](https://main.qcloudimg.com/raw/4216b3940324163a70c7fe4c3bd73415.png)

基于dataframe，可以使用sql算子灵活方便地处理数据，下面代码显示使用`groupBy`对数据进行分组计算。
![](https://main.qcloudimg.com/raw/c83d93d8ce4c26e2e2092c95c4698eb2.png) 

您还可以参考如下代码，直接写 SQL。 
![](https://main.qcloudimg.com/raw/2e71e389507e5420b4fa2ac3ea120bce.png) 
