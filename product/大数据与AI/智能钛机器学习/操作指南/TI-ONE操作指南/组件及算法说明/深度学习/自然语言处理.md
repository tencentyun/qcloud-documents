## 分词
#### 算法说明
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分词指将中文句子划分成词语，词语之间用空格分隔。

#### 输入参数
输入数据：未分词的中文文本，每行为一个句子。

#### 输出参数
输出数据：分词后的中文文本，每行为一个句子，词与词之间用空格分隔。

#### 算法参数
标签分隔符：（可选）如行中有标签分隔符，则只对标签分隔符左侧的文本进行分词。

####   实例生成
1. 使用数据节点上传数据，数据格式请参考【输入参数】部分。
2. 将数据节点连接到分词节点，配置好输出数据路径和标签分隔符，单击【运行】开始分词。



## 去除停用词

#### 算法说明
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;去除文本中的停用词，文本需要预先分词。

#### 输入参数
- 输入数据：分词后的中文文本，每行为一个句子，词语之间用空格分隔。
- 停用词表：要去除的停用词列表，每行为一个词。

#### 输出参数
输出数据：去除停用词后的中文文本，每行为一个句子。

#### 算法参数
标签分隔符（可选）：如行中有标签分隔符，则只对标签分隔符左侧的文本进行去停用词。

#### 实例生成
1. 使用数据节点，上传输入数据和停用词表数据，数据格式请参考【输入参数】部分。
2. 将数据节点连接到去停用词词节点，配置好输出数据路径和标签分隔符，单击【运行】开始去停用词。



## LSTM 文本分类

#### 算法说明
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LSTM 文本分类算法首先使用双向 LSTM 网络（[参考文档](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320&rep=rep1&type=pdf)）产生要分类的句子的向量表示，再通过全连接层网络对句子进行分类。


#### 输入参数
- 训练数据：每一行为一个句子，词与词之间用空格分隔，句子和标签之间用特定分隔符分隔（分隔符在算法参数中可以设置）。
- 验证数据：格式同训练数据。
- 模型目录：存放模型文件和日志文件的目录。（如果目录中已有模型文件，下次训练时将加载目录中最新的模型文件。因此，如果改变了网络结构或更换了数据，请更换或清空模型目录）  

#### 算法参数
- 分隔符：用于分隔句子和标签的分隔符。
- 词向量维度：网络中词向量的维度。
- LSTM 维度：网络中句子的向量表示的维度。
- 批处理大小：即训练的 batch_size。
- 训练 epoch 数：训练数据的训练次数。
- 学习率：即 learning_rate。
- 是否使用预训练好的词向量：如设为 True，可填写词向量文件路径。词向量文件格式与 glove 词向量官方格式相同。**如果使用预训练好的词向量，预训练词向量的维度应等于参数【词向量维度】的值**。
  
	### 	实例生成
1. 使用数据节点上传数据，数据格式请参考【训练数据】部分。
2. （可选）如数据只有一份，可以使用【输入】>【数据转换】中的【NLP 数据切分】节点，将上传的数据按比例分为训练数据和验证数据。
3. 将两份数据分别连接到 LSTM-classifier 节点的两个输入桩，单击【运行】开始训练。



## Fasttext

#### 算法说明
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Fasttext 是一种简单有效的句子分类算法， 通过词向量以及 ngram 向量的平均值计算出句子的向量表示，再通过全连接层网络对句子进行分类（[参考文档](https://arxiv.org/pdf/1607.01759.pdf)）。


#### 输入参数
- 训练数据：每一行为一个句子，词与词之间用空格分隔，句子和标签之间用特定分隔符分隔（分隔符在算法参数中可以设置）。
- 验证数据：格式同训练数据。
- 模型目录：存放模型文件和日志文件的目录。（如果目录中已有模型文件，下次训练时将加载目录中最新的模型文件。因此，如果改变了网络结构或更换了数据，请更换或清空模型目录）  

#### 算法参数
- 分隔符：用于分隔句子和标签的分隔符
- 词向量维度：网络中词向量的维度。
- 批处理大小：即训练的 batch_size。
- 训练 epoch 数：训练数据的训练次数。
- 学习率：即 learning_rate。
- 是否使用预训练好的词向量：如设为 True，可填写词向量文件路径。词向量文件格式与 glove 词向量官方格式相同。**如果使用预训练好的词向量，预训练词向量的维度应等于参数【词向量维度】的值**。
- 是否使用 ngrams：如设为 True，在计算句子向量表示时将引入 ngrams 以建模词序信息。需要配置使用的 n 值，ngrams 进行 hash 后的 bucket 数和 ngram 向量表示的维度。

#### 实例生成
1. 使用数据节点，上传数据，数据格式请参考【训练数据】部分。
2. （可选）如数据只有一份，可以使用【输入】>【数据转换】中的【NLP 数据切分】节点，将上传的数据按比例分为训练数据和验证数据。
3. 将两份数据分别连接到 Fasttext 节点的两个输入桩，单击【运行】开始训练。



## TextCNN
#### 算法说明
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TextCNN 使用卷积神经网络产生句子的向量表示，再通过全连接层网络对句子进行分类（[参考文档](https://arxiv.org/pdf/1408.5882.pdf)）。


#### 输入参数
- 训练数据：每一行为一个句子，词与词之间用空格分隔，句子和标签之间用特定分隔符分隔（分隔符在算法参数中可以设置）。
- 验证数据：格式同训练数据。
- 模型目录：存放模型文件和日志文件的目录。（如果目录中已有模型文件，下次训练时将加载目录中最新的模型文件。因此，如果改变了网络结构或更换了数据，请更换或清空模型目录）  

#### 算法参数
- 分隔符：用于分隔句子和标签的分隔符。
- 词向量维度：网络中词向量的维度。
- 各层网络卷积核大小：即 kernel_size。
- 各层网络卷积核个数：即通道数
- 批处理大小：即训练的 batch_size。
- 训练 epoch 数：训练数据的训练次数。
- 学习率：即 learning_rate。
- 是否使用预训练好的词向量：如设为 True，可填写词向量文件路径。词向量文件格式与 glove 词向量官方格式相同。**如果使用预训练好的词向量，预训练词向量的维度应等于参数【词向量维度】的值**。

#### 实例生成
1. 使用数据节点上传数据，数据格式请参考【训练数据】部分。
2. （可选）如数据只有一份，可以使用【输入】>【数据转换】中的【NLP 数据切分】节点，将上传的数据按比例分为训练数据和验证数据。
3. 将两份数据分别连接到 TextCNN 节点的两个输入桩，单击【运行】开始训练。



## BiLSTM+CRF
#### 算法说明
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;BiLSTM+CRF 是一种常用的序列标注模型，能用于分词，词性标注，命名实体识别等序列标注任务。模型使用双向 LSTM 网络产生句子中的各个词语的向量表示，并据此计算词语标签的概率分布，然后使用 CRF 计算总概率最大的标签序列（[参考文档](https://arxiv.org/pdf/1508.01991.pdf)）。
#### 输入参数
- 训练数据：文本文件，其中每一行是一个句子及其各个词语的对应标签，句子和标签之间由特定分隔符分隔（默认为 __label__），句子中的各个词语之间，以及各个标签之间用空格分隔。由于算法对句子中的每个词语进行标注，词语个数必须等于标签个数，如`词语1 词语2 词语3__label__标签1 标签2 标签3`。
- 验证数据：文本文件，格式与训练数据相同。
- 批处理大小：即算法 batch_size。
- 学习率：即算法的 learning_rate。
- 训练次数：即将训练数据训练的 epoch 数。
- 词向量维度：即每个词语向量表示的维度。
- LSTM 维度：即每个词语 LSTM 向量表示的维度。
- 使用预训练好的词向量模型：如设为 True，可填写词向量文件路径。词向量文件格式与 glove 词向量官方格式相同。**如果使用预训练好的词向量，预训练词向量的维度应等于参数【词向量维度】的值**。

#### 输出参数
日志目录：即存放 events 文件目录的路径，可以用此路径启动 tensorboard 查看训练情况。

####  实例生成
1. 使用数据节点，上传数据，数据格式请参考【训练数据】部分。
2. （可选）如数据只有一份，可以使用【输入】>【数据转换】中的【NLP 数据切分】节点，将上传的数据按比例分为训练数据和验证数据。
3. 将两份数据分别连接到 BiLSTM-CRF 节点的两个输入桩，单击【运行】开始训练。



## 基于 PMI 和熵的新词发现

#### 算法说明
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PMI（点互信息）和左右熵能够刻画一个文本片段的凝固程度和灵活运用程度，因此可以用于发现文本中不存在词库中的新词。



#### 输入参数
- 输入数据：未分词的中文纯文本文件。
- 词库：（可选）文本文件，每行为一个词。如发现的新词已经存在这个词库中，则跳过这一新词。

#### 输出参数
输出数据：算法发现的新词，每行为一个词。

#### 算法参数
- 最大词语长度：只考虑长度不超过这一长度的文本片段构成新词的可能性。
- 保留前 n 个新词：只保留可能性最大的前若干个新词。

#### 实例生成
1. 使用数据节点上传数据，数据格式请参考【训练数据】部分
2. 将数据连接到【新词发现】节点的输入桩，设置好输出数据路径，单击【运行】开始发现新词。

## Glove
#### 算法说明
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Glove 是一种经典的词向量算法，能够从大量文本中学习出各个词语的向量表示。这一向量表示可以用作其它深度学习模型的初始值（[参考文档](http://www.aclweb.org/anthology/D14-1162)）。
#### 输入参数
- 训练数据：文本文件，词语之间使用空格分隔。
- 验证数据：（可选）包含四元组的数据集，如 word2vec 官方 questions-words.txt，用于评价生成的词向量的质量。

#### 输出参数
保存路径：生成的词向量文件的保存路径。生成的文件为文本文件，每行为一个词语及其向量表示，词语和向量之间，向量中的各个数字之间使用空格分隔。

#### 算法参数
- 词向量维度：要训练的词向量维度。
- 窗口大小：计算共现矩阵时使用的 window_size 参数。
- 最小出现次数：只训练出现次数大于这一次数的词语的词向量。
- 训练 epoch 数：训练数据的训练次数。
- 学习率：即 learning_rate 参数。
- 批处理大小：即训练的 batch_size。
- 最大词汇表大小：如果训练数据中单词总数超过这一大小 n，只训练出现频率最高的 n 个词的词向量。

#### 实例生成
1. 使用数据节点上传数据，数据格式请参考【输入参数】部分。
2. 将数据节点连接到 glove 节点，配置好保存路径，单击【运行】按钮开始训练。



## Sentence2Vec
#### 算法说明
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sentence2Vec 可以将句子转换为向量表示，转换方法有三种：
- 转换为词向量平均值。
- 转换为词向量加权平均值，权重由词频决定，词频越高，权重越低。
- 使用论文 [A simple but tough-to-beat baseline for sentence embeddings](https://openreview.net/pdf?id=SyK00v5xx) 的方法进行转换。


#### 输入参数
输入数据：文本文件，每一行为一个句子，词语之间使用空格分隔。

#### 输出参数
输出数据：文本文件，每一行为输入数据对应行的句子的向量表示，数字之间用空格分隔。

#### 算法参数
- 预训练词向量文本文件，格式请参考 word2vec 和 glove 算法的【输出参数】部分。
- 转换方式：三选一，average 对应词向量平均值；weighted_average 对应词向量加权平均值；svd 对应论文中的方法。

#### 实例生成
1. 使用数据节点上传数据，数据格式请参考【输入数据】部分。
2. 将数据节点连接到 sentence2vec 节点，配置好预训练词向量路径和转换方式，单击【运行】按钮开始训练。

## BERT文本分类

#### 算法说明

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;BERT文本分类算法首先使用BERT网络（论文链接：https://arxiv.org/pdf/1810.04805.pdf）产生要分类的句子的向量表示，再通过全连接层网络对句子进行分类。

#### 参数设置

输入参数

- 训练数据：每一行为一个句子或两个句子，词与词之间用空格分隔，句子之间以及句子和标签之间用特定分隔符分隔（分隔符在算法参数中可以设置）

- 验证数据：格式同训练数据。

- 模型目录：存储模型文件和events文件的路径，可以从该路径启动tensorboard。

- bert参数目录：存储预训练的bert参数的目录，对于中文和英文，可以分别填写${ai_dataset_lib}/BERT/chinese_L-12_H-768_A-12和${ai_dataset_lib}/BERT/uncased_L-12_H-768_A-12，或使用自行上传的地址。

  

#### 算法参数

- 标签分隔符：用于分隔句子和标签的分隔符

- 批处理大小：即训练的batch_size。

- 训练epoch数：训练数据的训练次数。

- 学习率：即learning_rate。

- 句子分隔符：对于输入为两个句子的数据，分隔两个句子的分隔符

- 最大句子长度：BERT要求固定的输入长度，因此不足这一长度的句子将被补齐，超过的将被截断。

  

  ### 实例生成

1. 使用数据节点，上传数据，数据格式见上文【训练数据】部分。
2. （可选）如数据只有一份，可以使用【输入】-【数据转换】中的【NLP数据切分】节点，将上传的数据按比例分为训练数据和验证数据。
3. 将两份数据分别连接到BERT文本分类节点的两个输入桩。点击【运行】开始训练。



## 中文文本摘要

#### 算法说明

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;中文文本摘要首先使用BERT网络（论文链接：https://arxiv.org/pdf/1810.04805.pdf）产生文中句子的向量表示，再通过TextRank算法抽取权重较高的句子。

#### 参数设置

输入参数

- 文本数据输入：要抽取摘要的文本，要求每行为一个句子。

输出参数

- 输出数据：抽取后的摘要。



#### 算法参数

- 摘要句子数目：从文本中选为摘要的句子数量。

  

  ### 实例生成

1. 使用数据节点，上传数据，数据格式见上文【文本数据输入】部分。
2. 将输入数据连接到【中文文本摘要】节点的输入桩，点击【运行】按钮开始运行。



## 中文关键词抽取

#### 算法说明

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;中文关键词抽取算法使用TF-IDF抽取输入文本中的关键词。



#### 参数设置

输入参数

- 文本数据输入：要抽取关键词的文本，无需预先分词。

输出参数

- 输出数据：抽取后的关键词。



#### 算法参数

- 关键词个数：从文本中抽取的关键词数量。

  

  ### 实例生成

1. 使用数据节点，上传数据，数据格式见上文【文本数据输入】部分。
2. 将输入数据连接到【中文关键词抽取】节点的输入桩，点击【运行】按钮开始运行。



## 词频统计

#### 算法说明

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;词频统计节点统计文本中词语的出现频率，并从高到低排列。



#### 参数设置

输入参数

- 输入数据：要统计词频的文本。

输出参数

- 输出文件：词频统计结果。



#### 算法参数

​	无



#### 实例生成

1. 使用数据节点，上传数据，数据格式见上文【文本数据输入】部分。
2. 将输入数据连接到【词频统计】节点的输入桩，点击【运行】按钮开始运行。