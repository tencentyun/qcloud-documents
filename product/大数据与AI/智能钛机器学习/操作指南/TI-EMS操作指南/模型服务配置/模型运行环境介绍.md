# TI-EMS支持的模型运行环境

目前，TI-EMS提供五种模型运行环境：

- TF-Serving： 业界流行的 TensorFlow Serving 模型服务，支持SavedModel模型。
- PMML：通用标准的 PMML 模型服务，支持标准PMML模型。
- TensorRT: 支持模型自动优化的Tensorflow模型服务镜像，支持SavedModel模型。
- OpenVINO：专门针对CPU优化的模型服务镜像，支持Intermediate Representation(IR)模型。
- Angel：[腾讯自研的 Angel 框架](https://github.com/Angel-ML/angel)。

## 1. TF-Serving
TI-EMS 提供的Tfserving推理运行环境可以加载Tensorflow SavedModel模型，
## 2. PMML 
[PMML](https://www.ibm.com/developerworks/cn/opensource/ind-PMML1/)（Predictive Model Markup Language），一种预测模型标记语言,TI-EMS可以加载标准的PMML模型。
## 3.TensorRT
TensorRT镜像是支持模型自动优化的Tensorflow模型服务镜像,支持用户使用gRPC和HTTP访问模型服务，访问方式兼容TF-serving镜像。TensorRT支持用户上传Tensorflow saved_model模型，系统自动对模型进行优化。对8种主流分类模型测试，TensorRT镜像推理性能相对TF-serving镜像平均提升60%以上。对5种主流目标识别模型测试，平均性能提升40%以上。

使用TensorRT镜像可能遇到问题说明：

- 1.模型启动时间较长：因为需要做模型优化，模型启动时间可能会比较长。对于某些大模型，比如NASNET，模型启动时间可能需要3到5分钟。
- 2.模型首次推理时间较长：系统会根据用户上传数据大小生成最优代码，模型首次推理时间（仅限首次推理时间）可能需要0.5-5分钟。
- 3.模型加载失败：因为Tensorflow对读取protobuf文件大小限制，单个protobuf文件大于1GB会读取失败。即使上传模型小于1GB，模型优化过程也可能会生成大与1GB模型文件而导致加载失败。
- 4.模型服务失败：Tensorflow自动模型优化功能在快速迭代中，某些优化后的saved_model可能会服务失败，请通过[智能钛AI开发者社区](https://cloud.tencent.com/developer/timl/ask)讨论相关问题，腾讯云AI专家会及时帮助解答。 
- 5.TensorRT镜像相对TF-serving镜像推理性能提升不明显。请联系腾讯云客服，针对大客户，腾讯云AI专家可以针对具体模型深度优化。
## 4. OpenVINO
OpenVINO镜像是专门针对CPU优化的模型服务镜像，支持用户上传经过OpenVINO模型优化器优化的 Intermediate Representation (IR)模型，支持HTTP和gRPC访问模型服务，模型访问方式兼容TF-serving镜像。 对5种主流分类模型测试，OpenVINO镜像推理性能相对TF-serving镜像平均提升2.4倍以上。对4种主流目标识别模型测试，OpenVINO镜像推理性能相对TF-serving镜像平均性能提升30%以上。

使用OpenVINO镜像可能遇到问题说明：

- 1.模型推理结果有误：因为OpenVINO模型转换包括数据预处理，请检查模型转换过程参数设置，本地验证IR推理结果正确后再长传模型。
- 2.OpenVINO镜像推理性能问题。请联系腾讯云客服，针对大客户，腾讯云AI专家可以针对具体模型深度优化。
- 3.如何将Tensorflow/Caffe/MXNET/ONNX等模型转换为OpenVINO IR模型，请参考[模型格式转换](https://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html)。
- 4.HTTP接口调用仅支持数值数据类型，详情请参考：[OpenVINO接口调用说明](https://github.com/IntelAI/OpenVINO-model-server)
## 5. Angel
腾讯自研深度学习框架，Angel相关内容请访问https://github.com/Angel-ML/angel。


