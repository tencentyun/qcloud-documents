离线工作流即对实时性要求不高，可接受 T+1 的数据计算结果的任务类型，下面以一个实际的离线工作流为例子进行介绍。
## 1、示例任务介绍 ##
首先按照 **快速入门>创建任务** 方式创建一个 **FTP导入HDFS** 任务。
![创建工作流](https://i.imgur.com/dVUJmYQ.png)
- 离线工作流以某业务场景下的按天处理测速日志为例
- 数据流向为测速 **agent—>FTP—>HDFS—>HIVESQL—>MySQL**
- 原始日志已采集并按天存入到 **FTP**（集群内 FTP，也可以外部数据源，设置方法参见“服务器配置”）
- **HIVESQL** 做运营商维度的 PV 统计，并最终导出到 **MySQL** 结果数据库
- **FTP** 存储位置：**/FTP/origin_stat/in/${YYYYMMDD}**（注:这里变量表示按天分目录）

## 2、数据接入 ##
 **2.1 数据导入示例说明**
右击新建的离线接入任务模块，然后选择【编辑】，根据详细说明设置离线接入的相关内容。
![编辑离线任务](https://i.imgur.com/xHAX6tK.png)
![编辑离线任务2](https://i.imgur.com/vUNJq2p.png)
![编辑离线任务3](https://i.imgur.com/2nqfwlb.png)
![编辑离线任务4](https://i.imgur.com/FJHGINd.png)
详细说明：
（1）任务名称：任务标识名称
（2）任务类型：数据导入集群的方式
（3）告警：即依据任务状态设置告警，目前支持任务失败或执行超时告警
（4）周期类型：数据导入任务的执行周期。可每月，每周，每天，每小时，每分钟或一次性非周期和持续性非周期方式
（5）起始数据时间：以数据的更新时间作为任务执行的起始时间。例如现在是8月21日，但任务需要从昨天的（8月20日产生的数据）数据开始计算，那么这里选择起始数据时间为8月20日
（6）自身依赖：同一个任务不同周期产生的调度任务之间是否有依赖关系。例如离线导入任务按天执行，那每天都会生成一个数据导入任务，如果这里选择“是”，那只有前一天的导入任务成功完成，第二天的才会顺序执行。
（7）调度时间：与任务周期相关的参数，即任务下发的相对时间，例如这里选择0点0分，任务周期选为“天”时，则每天的0点0分下发任务执行，以此类推
（8）责任人：工作流负责人，可修改删除工作流
（9）任务说明：备注信息
（10）基础设置-源服务器：【服务器设置】里增加的服务器配置，这里直接选择即可。介绍示例为 FTP 导入到 HDFS，故这里选择事先加入的FTP服务器即可
（11）基础配置-目标服务器：【服务器配置】里增加的服务器配置，这里直接选择即可。介绍示例为 FTP 导入到 HDFS，故这里选择事先加入的 HDFS  服务器即可
（12）源目录：由于这里的选择是 FTP 导入到 HDFS，故选择的是FTP的目录。这里支持时间内置变量即 ${YYYYMMDDhhmm}，即年，月，日，小时，分钟
（13）目标目录：由于这里选择的是 FTP 导入到 HDFS，故填的是HDFS的主目录。这里同样支持内置时间变量 ${YYYYMMDDhhmm} 
（14）其他配置：其他配置里选项默认即可

**2.2 其他离线数据导入方式介绍**
多种导入方式的基础配置基本类似，任务周期选择均相同。差异点即取源数据的方式不同，根据需要选择不同方式即可。
目前离线数据导入的方式支持如下图所示。
![其他离线数据导入](https://i.imgur.com/NuZDMfw.png)

## 3、数据计算 ##
**3.1 数据计算示例说明**
本示例以 HIVESQL 进行数据计算，首先新建一个 **HIVE SQL脚本** 任务。
![SQL脚本](https://i.imgur.com/SPRXyMW.png)
右键单击【编辑】之后，根据详细说明填写以下设置表。
![设置表1](https://i.imgur.com/deIPhRg.png)
![设置表2](https://i.imgur.com/BWsQfjk.png)
![设置表3](https://i.imgur.com/EuhC48X.png)
详细说明：
（1）任务名称、类型、周期类型、起始数据时间等和数据导入类似，不再赘述。
（2）源服务器：即在【服务器设置】里增加的服务器配置，这里为 **HIVESQL** 计算，故选择之前配置的 **HIVEserver** 即可。
（3）SQL脚本：即该任务执行的 **SQL** **脚本**，点击【上传脚本】，选择本任务执行的脚本即可，如下图:
![上传脚本](https://i.imgur.com/MFuWgSs.png)
示例脚本如下：           
```
use demo_stat_db;
ALERT TABLE demo_stat_tb_day ADD PARTITION(date_id = '${YYYYMMDD}')
LOCATION 'hdfs://hdfsCluster/project/demo/in/${YYYYMMDD}/';

insert into table demo_stat_db.isp_conn_total_times
SELECT date_id,isp,sum(conn_succ)
	FROM demo_stat_tb_day
	WHERE 1.date_id = '${YYYYMMDD}'
GROUP BY isp,1.date_id;
```
简单解释：第一步使用外部表的方式使HDFS与HIVE建立关联；第二步即常用的SQL语法（详细使用可通过搜索引擎学习），进行简单的运算并存入到HIVE结果表。

**3.2 其他数据计算方法说明**
目前支持的计算方法包括 **HIVESQL，Map-Reduce，Shell 脚本，Spark 和 JStorm**，每种计算方法的基础配置如周期性等均完全一样。
![其他计算方法](https://i.imgur.com/64yULMl.png)   
差异在于 **Spark 和 JStorm** 上传脚本时，可以自定义参数，同时 **JStorm** 需要指定 **Topology** 类和名称，如下图所示：
![自定义参数](https://i.imgur.com/IaDdl1D.png)

## 4、数据导出 ##
**4.1 数据导出示例说明**
本示例中，到目前为止，数据经过计算已经将结果存入到HIVE表，即HDFS的文件中，导出即选择从HDFS导出到MySQL永久存储。
首先新建一个 **HDFS导出MYSQL** 任务。
![导出数据](https://i.imgur.com/sn6k1cQ.png)    
右键单击【编辑】，根据详细说明进行相关设置。
![导出数据1](https://i.imgur.com/wD4truV.png)
![导出数据2](https://i.imgur.com/V1dN12D.png)
![导出数据3](https://i.imgur.com/Tapkk1o.png)
详细说明：
（1）任务名称，周期类型，起始数据时间，自身依赖，调度时间，责任人，任务说明均与数据导入，数据计算的含义一致
（2）任务类型：本例使用HDFS导出到MySQL，目前可使用的导出方式后面介绍
（3）源服务器：即【3.2服务器配置】里增加的服务器，这里示例中选择配置好的HDFS服务器即可
（4）目标服务器：即【3.2服务器配置】里增加的服务器，这里示例中选择配置好的MySQL服务器即可
（5）源目录：即源服务器中HIVESQL表在HDFS上目录位置
（6）目标表：即目标MySQL服务器的数据库中对应table。
（7）目标表列名：目标MySQL服务数据库中table的表名称列表以,分隔，注意需要和HIVESQL表的列数量一一对应
（8）分隔符：HIVESQL表的字段之间的分隔符，这里要注意的是HIVESQL表在创建时指定好这里的分隔符
（9）是否分区：即导出到MySQL表时，MySQL表是否需要分区，目前支持按照时间分区，分区的格式支持P_${YYYYMM}，P_${YYYYMMDD}和P_${YYYYMMDDHH}
（10）数据库入库模式：目前支持两种入库模式：TRUNCATE和APPEND，其中 TRUNCATE 每次都会清理掉目标MySQL数据表全表，将HIVESQL全表重新导入；APPEND方式会在MySQL表中增加一个etl_timestamp字段，填入数据导入时间戳，当导出任务重跑时会根据这个时间戳来进行覆盖

**4.2 其他数据导出方法说明**
目前支持的数据导出方法如下图所示：
![数据导出类型](https://i.imgur.com/o7XQGrP.png)   
任务基本信息的设置均一致，差异如下：
数据导出到 **HBase**，列的对应信息填写准确即可。（其余均为导出到类 SQL 数据库）
![HBase配置](https://i.imgur.com/2nwNBrT.png)
