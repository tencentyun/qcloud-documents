TI-EMS 可将模型服务配置启动为在线推理或批处理作业，下面以启动和调用 Tf-Serving 在线推理服务为例说明使用方法：

### 内容准备
**经典深度学习 inception 模型：** [inception_model.zip](http://ti-ems-1255502019.cosbj.myqcloud.com/tfserving/inception/inception_model.zip)。
**测试图片：**
 [flower.jpg](http://ti-ems-1255502019.cosbj.myqcloud.com/test-data/tfserving_data/flower.jpg)
![](https://main.qcloudimg.com/raw/9e74659a24ec62bc47b0f14160afee98.png)
**base64 编码**
将上图测试花朵图片按照 inception 模型定义的 JSON 数据格式`{"instances":[{"b64": "图片 base64 编码"}]}`进行编码，将 jpg 转换成 base64。`flower.json`为经过编码的测试图片数据，或者您可以直接下载已经编码完成的 json 文件 [flower.json](http://ti-ems-1255502019.cosbj.myqcloud.com/test-data/tfserving_data/flower.json)，跳过该图片编码步骤，直接进行下一步。
```shell
echo "{\"instances\":[{\"b64\": \"$(base64 -i flower.jpg)\"}]}" | tee flower.json
```
### 创建模型服务配置
在模型服务配置页面单击【新建】，进入模型服务配置新建页面，输入配置名称：demo_tfserving，单击【运行环境】，在弹出页面的【公共运行环境】栏选择 tfserving。

单击【对象存储 cos 文件】，弹出 COS 文件选择页面，选择 inception 模型文件夹所在的路径，单击【确定】。

模型服务配置创建详细操作指引请见【快速入门】>[【创建模型服务配置】](https://cloud.tencent.com/document/product/1120/36585)。

### 启动服务
在模型服务配置页面找到 demo_tfserving 配置，单击配置卡片的【在线推理】，进入启动服务页面。
![](https://main.qcloudimg.com/raw/d8002f843b83d60cd13784a23b54a8c4.png)
**输入服务名称**：输入启动的服务名称。
**选择资源组**：选择将要启动的资源组，这里选择已购买的专用资源组。资源组详细介绍请看 [资源组管理](https://cloud.tencent.com/document/product/1120/38968)。
**选择实例配置**：选择【CPU 配置】，实例配置填写为1核2G。

TI-EMS 针对不同的模型服务运行资源需求为您提供了【CPU 配置】和【GPU 配置】，若您选择在公共资源组启动服务，则需要选择 CPU quota 或 GPU quota 并指定对应 quota 的数量，公共资源组的服务计费方式按照 quota 单价 * 数量 * 服务运行时长，若您选择在专用资源组启动服务，则需自定义 CPU 配置和 GPU 配置，用户在预购的专用资源组启动的服务不再另外收费。

**实例调节策略**：选择【手动调节】，实例数量设置为1。
TI-EMS 支持用户手动设定实例数量或设置实例自动调节策略。选择手动调节实例，可以直接设定启动服务时的实例数量，实例数量最小设置为1。选择自动调节实例，可以设置多种触发策略，当满足任一触发策略时，按照指定的实例伸缩范围自动调节。TI-EMS 目前支持的触发指标如下：
- CPU 利用率
- 内存利用率
- GPU 利用率

当系统实际指标小于触发策略指标设定值时，执行自动缩容，大于设定值时，执行自动扩容。
**选择是否生成鉴权**：勾选生成鉴权，生成服务访问地址的鉴权密钥。

全部设置完成后，单击【启动服务】，进行在线服务列表页面。
![](https://main.qcloudimg.com/raw/9bafe1dcca5c041f90469a404640d518.png)

### 调用测试
单击【模型服务】>【在线推理】页面选择 tfserving 模型服务，在对应的【操作】列单击【更多】>【调用】，选择公网地址访问，获得模型服务的公网访问地址和密钥 TOKEN。

使用 curl 工具为例，展示如何调用服务接口：
```shell
curl -H "Content-Type: application/json" \
-H "x-Auth-Token: TOKEN" \
-X POST IP/v1/models/m:predict -d @flower.json
```

**调用参数说明**
- TOKEN：通过单击模型服务页面的【调用】获取的密钥地址 token。
- IP：通过单击模型服务页面的【调用】获取的服务访问地址。

**返回结果：**
```shell
{
 "predictions": [{
  "class_idx": 2,
  "probabilities": [0.00685295, 0.00298388, 0.965622, 0.00721011, 0.0173306],
  "classes": "roses"
 }]
}
```

可以看出模型服务运行正常，并且正确的对发送的花朵进行了分类，具体详情可参考 [TensorFlow 模型服务文档](https://www.tensorflow.org/tfx/serving/api_rest)。
