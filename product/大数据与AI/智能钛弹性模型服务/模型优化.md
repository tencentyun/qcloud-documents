TI-EMS 提供了模型量化功能，针对 Tensorflow SavedModel 格式的模型文件，希望在英伟达 GPU 上获得更高的推理性能。可以使用模型优化操作，对您的模型进行优化。优化后的模型可以部署在带有 Nvidia P4或T4卡的机器上。
目前模型优化使用基于 TensorRT 的模型转换。包括FP32模型到FP32模型转换，FP32模型到FP16模型的转换和FP32模型到INT8模型的转换。

## 创建模型量化任务
1. 登录 [TI-EMS 控制台](https://console.cloud.tencent.com/tiems)，在左侧导航栏中选择“【模型仓库】 > 【模型优化】”，进入模型优化页面。
2. 单击左上角“创建量化任务”，进入任务创建页面。
![](https://main.qcloudimg.com/raw/672cedfb817d32287a69535b458d9b27.png)
3. 在创建任务页面，输入以下信息：
 - 任务名称：1-20字符。
 - 任务描述：输入您对该次量化任务的备忘信息。
 - 选择资源组：目前模型量化任务仅支持部署在专用资源组，下拉选择合适的专用资源组。
 - 实例配置：TensorRT 量化任务一般需要 GPU 资源，请选择【GPU 配置】，并选择内存大于8G的资源。
 - 转换模版：TI-EMS 目前仅支持将 Tensorflow SavedModel 转换成基于 TensorRT 的模型, 使其可在 GPU 上进行推理。
 - 转换输入目录：请查看【模型输入格式说明】。
 - 转换输出目录：请选择量化后模型需要输出的目录。
 - 量化精度：选择量化精度。
 - 量化批大小：选择量化批大小。
4. 任务创建完成后，单击【启动任务】，完成量化任务的创建。单击量化任务的名称，可进入任务详情页，切换至【日志】选项卡，可查看任务运行过程中的日志。量化任务可以进行删除。

## 模型输入格式说明
```
|
|---model                    模型存放目录，必须以model命名。

    |----saved_model.pb    模型文件。
    |----variables          变量存储文件夹。

|---data         转INT8模型所需校准数据集存放目录，必须以 data 命名。FP16和FP32转换不需要。输入目录下有且只能有一个。

    |---data.npy             校准数据集为npy格式文件，文件名为data.npy. npy是在预处理后直接输入模型的数据。假设模型输入图片大小为{1, 3, 299, 299},数据集大小为100张图片。data.npy的格式为{100, 3, 299, 299}。
```
很少情况下 Tensorflow SavedModel 可能包含多个模型，模型优化会默认优化标记为“serve”的模型。目前模型优化仅支持单输入模型。

如果模型优化操作成功结束，模型优化后输出为单个 pb 文件。优化后的模型可以和未优化模型一样使用 Tfserving 环境部署。
鉴于目前 TensorRT 支持的模型有限，如果您的模型不能成功使用模型优化功能，并且希望提升模型推理性能，您可 [提交工单](https://console.cloud.tencent.com/workorder/category?level1_id=517&level2_id=727&source=0&data_title=%E5%85%B6%E4%BB%96%E8%85%BE%E8%AE%AF%E4%BA%91%E4%BA%A7%E5%93%81&step=1) 寻求帮助。
