模型部署和服务，目的是将模型从离线服务轻松变成在线服务，对闭环模型非常重要，尤其在深度学习中更是如此。TI-ONE上就可以把模型通过在线部署方式，让您的模型可以直接对用户提供服务
### 1. 模型部署

在模型训练节点（小尾巴），左侧工具栏中的个人模型/共享模型右键菜单里都有模型部署的入口。 如下

小尾巴

![](https://main.qcloudimg.com/raw/966e4a2c407508f2cfb922ac9e5f146a.png) 



模型组件

![](https://main.qcloudimg.com/raw/22369830cd3eb8e3459378ee45906697.jpg)


进入部署参数配置页面，配置好以后就可以部署了

![](https://main.qcloudimg.com/raw/efdb572631116aaf9ea7897bb0380022.png)


字段说明：

模型组：个人模型服务的组别，和工程类似，一个用户可以有多个模型服务组。系统给每个用户一个默认的模型服务组；
实例类型：模型服务的资源配置（CPU，GPU，内存），平台初始化定制好多种类型。
实例数：上述的资源配置的套数。
服务分类：分为2类TFServing和inference模式。机器学习(如Spark，xgBoost)模型部署为inference模式；深度学习则选择TFServing。
部署版本：分为新增部署或替换部署。 新增指在原有的服务基础上，用当前的模型新建一个版本的服务，替换则是把原有的版本替换成当前的。
进入部署参数配置页面，配置好以后就可以部署了


### 2. 模型服务
#### 新增模型服务组
系统为每个用户自动新增了一个“用户登录名的模型服务组”，用户也可以新建自己的模型服务组，如下


![](https://main.qcloudimg.com/raw/1c93924dcd577ef9d84b5356245e226a.png)

模型部署后，可以在“模型服务”TAB进行管理，如下

![](https://main.qcloudimg.com/raw/cc331f5deb1861a7654ec9e2b11a8f48.png)


模型服务状态有3种：

**部署中**：模型服务正在部署过程中

**服务中**：部署已经成功，可提供服务

**部署失败**: 失败后可查询部署的日志（卡片右上角）定位，如果申请的资源超过上限，也会部署失败

可以最多支持6个版本的模型服务，即可支持1个模型6个版本同时在线服务。



#### 模型服务操作


1. 重新部署：部署后可能由于底层系统的app没启动起来，或者app执行异常退出部署失败，可进行重新部署。
2. 删除：点击卡片上的“删除”按钮是删除这个版本的模型服务，点击模型服务右上角的删除按钮则是一键删除模型的所有版本的在线实例。

#### 模型服务日志查看：
提供2种日志查看：


![](https://main.qcloudimg.com/raw/49137125048d010ff14465bbd0c8c6a3.png)   


上图中，右上角的日志按钮点击进去后，将列出该模型部署日志，下面的在版本卡片上的“日志”链接点击进去，可以看到这个版本的模型的服务日志，即APP日志。当然，如果部署失败，服务没有启动起来，那么服务日志是看不到内容的。


### 3.模型服务指标
 点击模型服务的卡片，就可以进入到模型服务指标和可视化性能页面。
模型的指标都可以在这里看到，您可以在此查阅到模型自身信息以及服务的性能。

![](https://main.qcloudimg.com/raw/5ce5ca39721b3f9c6e4bae10b5b38564.png)
![](https://main.qcloudimg.com/raw/d6b4f54d2cfaf02c81ccafdb9b6fdeab.png)

这个页面中会展示如下信息

1.	模型指标：模型基本信息，和训练时模型观察台看到的是一致的。
	.	服务地址：本版本提供服务的URL
	.	服务性能：该版本模型服务的访问量曲线
	.	可视化性能：模型自身的服务性能。如上图中例子就是模型训练过程损失函数曲线


### 4.模型服务的使用
模型服务支持两种协议类型，HTTP和GRPC模型。其中HTTP模块目前支持xgboost，spark以及tf。GRPC是社区版本tensorflow serving的内部服务实现，相关的接口参考serving的相关配置。

HTTP 接口说明：在使用前需要准备 Inference 在线模型加载和处理类，该类中必须包括下面的内容。

//固定名称，实现的接口类，必须这样的名称，请不要修改。
class InferenceService: 

    //类初始化函数，该函数包含了变量model_path，服务框架在加载类时会把模型所在目录或是模型文件传入进来，根据这个路径用户可以加载自己的模型到内存中以提供服务
    def __init__(self, model_path):

//类的请求处理函数，传入的是http请求的json内容 例如请求的http请求为：
    //curl -H "Content-Type: application/json" -X POST --data '{"type":"libsvm","feature_num":"524","data":{"insts":"0 1:0 2:1.0 3:565 4:9 5:6 6:47200.0 7:15 8:0.48 9:1959200.0 10:-65535.0 11:0.0 12:0.6447333333333334 524:-65535.0"}}' http://119.29.47.157:9991/gpu_cluster/tione-33333333/inference-1525426013831-85-model-service-2009-0//predict/m47/1 
//其中 request为 -Data指定的内容，这里用户可以自己定位传输数据的接口，我们这里建议接口为type：libsvm和dense feature_num：feasture的number数据，data为数据内容，可以支持多条数据，建议批量请求
 def process_request(self, request):

#### 完整的参考示例：

用户的请求测试命令为：
curl -H "Content-Type: application/json" -X POST --data '{"type":"libsvm","feature_num":"524","data":{"insts":"0 1:0 2:1.0 3:565 4:9 5:6 6:47200.0 7:15 8:0.48 9:1959200.0 10:-65535.0 11:0.0 12:0.6447333333333334 524:-65535.0"}}' http://119.29.47.157:9991/gpu_cluster/tione-33333333/inference-1525426013831-85-model-service-2009-0//predict/m47/1 

返回json结果：
{"message": "SUCCESS", "code": 0, "data": {"insts": [0.6868814826011658]}}










