### 1. Information Based

- **算法说明**

  基于信息的特征选择，该模块共包括4种算法：信息增益（Information Gain）、基尼系数（gini）、信息增益率（Information Gain Ratio）以及对称不确定性(Symmetry Uncertainly)

  - 信息增益公式：

  ![](https://main.qcloudimg.com/raw/89f0438d45be5b88f76dbc3e9c16381e.png)

  - 基尼系数公式：

  ![](https://main.qcloudimg.com/raw/09df70d23473921cc22303031aba7bef.png)

  - 信息增益率公式：

  ![](https://main.qcloudimg.com/raw/a4eb4a4abca82f00f3131e7cd6b03e37.png)

  - 对称不确定性公式：

  ![](https://main.qcloudimg.com/raw/43c21c037ae821017f5bfef193fd678b.png)，其中H(X)与H(Y)为信息熵, IG(X/Y)为信息增益

- [**样例**](https://tio.cloud.tencent.com/ml/platform.html?projectId=33&flowId=136)

- **输入**
  - 数据形式：Dense
  - 格式：| label | 参与计算的features | 不参与计算的features | 
  - label：仅存在0或1。通过**算法参数**中的**目标标签所在列**指定。
  - 参与计算的features：可通过**算法参数**的**特征所在列**指定
  - 不参与计算的features：可包括不参与计算的特征

- **输出**：
  - 格式：
    | X | IGR | GI | MI | SU |
    | Id | IGRImp | GIImp | MIImp | SUImp |
    - X：无实际意义，用来形成有效的矩阵形式
    - IGR：标题，表示信息增益率
    - GI：标题，表示基尼系数
    - MI：标题，表示信息增益
    - SU：标题，表示对称不确定性
    - Id：被选择的特征Id
    - IGRImp：特征的信息增益率
    - GIImp：特征的基尼系数
    - MIImp：特征的信息增益
    - SUImp：特征的对称不确定性 
    - 举例：

  	```
  	# 特征重要度矩阵
		  X IGR GI MI SU
	  1 0.03 0.04 0.2 0.07 
	  2 0.15 0.018 0.38 0.009 
	  3 0.25 0.33 0.025 0.17   
		```

- **参数**：
  - 特征所在列：表示需要计算的特征所在列，例如“1-12,15”，其说明取特征在表中的1到12列，15列，从0开始计数
  - 目标标签所在列：根据目标标签在表中的位置，从0开始计数
  - 并行数：训练数据的分区数、spark的并行数
  - 抽样率：输入数据的采样率


### 2.  ChiSqSelector

- **算法说明**

  该模块基于卡方独立性检验进行特征选择。特征选择过程将根据卡方独立性检验结果，将每个特征对应的卡方统计量按照从大到小的顺序进行排序，根据这一排序用户可指定选择的特征个数，否则系统将根据默认值提取前几个特征。**需要注意的是**，该模块对连续型数据也采用离散数据的方式进行统计，并且要求目标变量和特征的数值种类个数不能超过10000。因此，对于连续型数据最好先通过离散化方式进行处理，再进行特征选择。

- [**样例**](https://tio.cloud.tencent.com/ml/platform.html?projectId=33&flowId=136)

- **输入**
  - 数据形式：Dense
  - 格式：| label | 参与计算的features | 不参与计算的features |
  - label：通过**算法参数**中的**目标标签所在列**指定。
  - 参与计算的features：可通过**算法参数**的**特征所在列**指定
  - 不参与计算的features：可包括不参与计算的特征，如果存在则保留在输出中

- **输出**
  - 格式：|不参与计算的features | 被选择的特征 |

- **参数**：
  - 特征所在列：表示需要计算的特征所在列，例如“1-12,15”，其说明取特征在表中的1到12列，15列，从0开始计数
  - 目标标签列：目标标签所在列，从0开始计数
  - 选择的特征个数：选择的top特征个数
  - 并行数：训练数据的分区数、spark的并行数
  - 抽样率：输入数据的采样率 

### 3.  FeatureImpByGBDT

- **算法说明**

  FeatureImpByGBDT模块是一个基于XGBoost组件计算特征重要度的模块。通过用特征数据训练出集成模型从而判断特征的重要度。该模块提供了三种衡量特征重要度的指标：weight、gain以及cover。

  - weight: 特征在所有树中出现的总次数
  - gain: 特征在所有树中出现对于模型的平均提升（Gini不纯度增益）
  - cover: 特征在所有树中平均覆盖的样本数

**注意**：spark集群必须选择：深汕Spark集群-若兰

- [**样例**](https://tio.cloud.tencent.com/ml/platform.html?projectId=29&flowId=91)

- **输入**

  - 数据形式：Dense 或 Libsvm
  - 格式：| label | 参与计算的features | 不参与计算的features |
  - label：通过**算法参数**中的**目标标签所在列**指定。
  - 参与计算的features：可通过**算法参数**的**特征所在列**指定
  - 不参与计算的features：可包括不参与计算的特征

- **输出**
  - 特征重要度输出： 
  - 格式：|重要度类型|Id|重要度|
  - 重要度类型：weight、gain还是cover
  - Id:参与建树的特征Id
  - 重要度：重要度数值
  - modelOut：新训练的XGBoost模型
- **参数**
  - 目标标签所在列：从0开始计数
  - 特征所在列：例如“1-10,12,15”，其说明取特征在表中的第1到第10列，第12列以及第15列，从0开始计数
  - 并行数：训练数据的分区数、spark的并行数
  - 抽样率：输入数据的采样率
  - 迭代轮数：numRound
  - 收缩步长：eta
  - 最大深度：max_depth
  - 目标函数：同XGBoost的目标函数，如reg:linear” 表示线性回归；“reg:logistic” 表示逻辑回归
  - worker数：表示xgboost的并发度，worker数量应<=【spark的executor个数】*【每个executor上的core数】
  - XGBoost配置文件：XGBoost其他参数的配置，在tesla页面，脚本类型选择"纯文本"，以下是配置文件格式：
    
     ```
     subsample=0.6
     colsample_bytree=0.9
     ```
特征选择（新）

1、卡方特征选择

- 算法说明
  卡方检验是一种常用的特征选择方法。卡方用来描述两个事件的独立性或者描述实际观察值与期望值的偏离程度。卡方值越大，则表明实际观察值与期望值偏离越大，也说明两个事件的相互独立性越弱。
- 样例
- 输入
  - 输入数据路径：输入文件所在路径
  - 输入数据格式：每行样本的各列以空格连接，如10.2 12.8 3.67 …。
- 输出
  - 输出数据路径：输出文件所在路径
  - 输出数据格式：
    每行样本的各列以空格连接，如10.2 12.8 3.67 …。最后的结果中，选中的原始特征列会被删除，经过卡方选择的特征会append到数据的最后几列。
- 参数
  - 标签列：标签列所在的列号，从0开始计数，如填写0，表示第一列是标签。
  - 特征列：表示需要计算的特征所在列，例如“1-12,15”，表示取特征在表中的1到12列，15列，从0开始计数。
  - 选择的特征个数：根据卡方值选择的特征个数。

2、基于方差的特征选择

- 算法说明
  如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。所以通过方差对低方差的特征进行过滤，是特征选择常用的方法。
- 样例
- 输入
  - 输入数据路径：输入文件所在路径
  - 输入数据格式：每行样本的各列以空格连接，如10.2 12.8 3.67 …。
- 输出
  - 输出数据路径：输出文件所在路径
  - 输出数据格式：每行样本的各列以空格连接，如10.2 12.8 3.67 …。最后的结果中，选中的原始特征列会被删除，经过方差选择的特征会append到数据的最后几列。
- 参数
  - 标签列：标签列所在的列号，从0开始计数，如填写0，表示第一列是标签。
  - 特征列：表示需要计算的特征所在列，例如“1-12,15”，表示取特征在表中的1到12列，15列，从0开始计数。
  - 方差阈值：根据方差阈值选择特征。

3、基于树的特征选择

- 算法说明
  基于树的集成算法有一个很好的特性，就是模型训练结束后可以输出模型所使用的特征的相对重要度，便于我们选择特征，理解哪些因素是对预测有关键影响。
- 样例
- 输入
  - 输入数据路径：输入文件所在路径
  - 输入数据格式：每行样本的各列以空格连接，如10.2 12.8 3.67 …。
- 输出
  - 输出数据路径：输出文件所在路径
  - 输出数据格式：每行样本的各列以空格连接，如10.2 12.8 3.67 …。最后的结果中，选中的原始特征列会被删除，经过树选择的特征会append到数据的最后几列。
- 参数
  - 标签列：标签列所在的列号，从0开始计数，如填写0，表示第一列是标签。
  - 特征列：表示需要计算的特征所在列，例如“1-12,15”，表示取特征在表中的1到12列，15列，从0开始计数。
  - 选择的特征个数：根据树模型选择的特征个数。

4、基于信息的特征选择

- 算法说明
  基于信息的特征选择是常用的特征选择方法。总共有四种信息值用于特征选择：信息增益率（Information Gain Ratio）、基尼系数（Gini）、互信息（Mutual Information）以及对称不确定性(Symmetry Uncertainly)。
- 样例
- 输入
  - 输入数据路径：输入文件所在路径
  - 输入数据格式：每行样本的各列以空格连接，如10.2 12.8 3.67 …。
- 输出
  - 输出数据路径：输出文件所在路径
  - 输出数据格式：每行样本的各列以空格连接，如10.2 12.8 3.67 …。最后的结果中，选中的原始特征列会被删除，经过树选择的特征会append到数据的最后几列。
- 参数
  - 标签列：标签列所在的列号，从0开始计数，如填写0，表示第一列是标签。
  - 特征列：表示需要计算的特征所在列，例如“1-12,15”，表示取特征在表中的1到12列，15列，从0开始计数。
  - 选择的特征个数：根据树模型选择的特征个数。
  - 特征选择方法：信息增益、基尼系数、互信息、对称不确定性。



