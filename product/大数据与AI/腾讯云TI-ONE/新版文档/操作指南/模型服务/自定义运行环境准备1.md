
## 前言
本文档将向您介绍 TI 自定义镜像需要遵循的一些容器规范约束，再通过几个典型案例向您演示如何制作镜像。

## 平台基础镜像说明
平台为CPU、GPU机器分别提供了基础推理镜像。

|环境	|镜像地址|
|--|--|
|CPU	|`ccr.ccs.tencentyun.com/tione-public-images/ti-infer-cpu-base:1.0.0` |
|GPU	|`ccr.ccs.tencentyun.com/tione-public-images/ti-infer-gpu-base:1.0.0` |

基础镜像基于 centos 制作，其中包含的软件有：

<table>
<tbody><tr>
<td>CPU、GPU 均包含    python-3.8.13</td>
<td>xgboost<br>sklearn<br>pandas<br>gcc-7.3.0<br>cmake-3.18.5<br>tiinfer</td>
</tr>
<tr>
<td>仅 GPU 包含</td>
<td>cuda-11.1.1</td>
</tr>
</tbody></table>

- 基础镜像的启动命令 /usr/local/service/ti-cloud-infer/entrypoint.sh
- entrypoint.sh 中的内容为：
```
#!/bin/bash
source /etc/profile
source /root/.bashrc
export LD_LIBRARY_PATH=/usr/local/python3/lib/python3.8/site-packages/torch/lib:/usr/local/openmpi/lib:/usr/local/nccl/lib:/usr/local/cuda/lib64:/usr/local/python3/lib:/usr/local/python3/lib64:/usr/local/openmpi/lib:/usr/local/gcc/lib:/usr/local/gcc/lib64

MODEL_DIR=/data/model
ALGORITHM_NAME=m
REST_PORT=8501

echo "================== code path ${MODEL_DIR}=========="
cd ${MODEL_DIR}

if [ -f "requirements.txt" ]; then
  echo "============== install python requirements   ===================="
  echo "python3 -m pip install -r requirements.txt"
  python3 -m pip install -r requirements.txt
  echo "============== install python requirements done ================="
fi

echo "====================== start serving ============================"
echo "python3 -m tiinfer --http_port ${REST_PORT} --algorithm_name ${ALGORITHM_NAME} --model_dir ${MODEL_DIR} "
python3 -m tiinfer --http_port ${REST_PORT} --algorithm_name "${ALGORITHM_NAME}" --model_dir ${MODEL_DIR}
```
-	启动逻辑为:
1)	读取环境变量 ${MODEL_DIR} 目录下的 requirements.txt 文件，使用 pip 安装其中制定的依赖 python 包。
2)	tiinfer 框架会读取 ${MODEL_DIR} 下的文件，加载模型后，启动一个 HTTP 服务并监听在 ${REST_PORT} 定义的端口。
3)	tiinfer 框架启动时，会从 model_service.py 文件中加载模型。

## 自定义镜像规范

### 基于平台基础镜像制作自定义镜像
1.	dockerfile 中添加对基础镜像的引用，例如：
```
FROM ccr.ccs.tencentyun.com/tione-public-images/ti-infer-cpu-base:1.0.0
```
2.	自定义部分集中在 model_service.py 文件及 entrypoint.sh 文件的修改注意 model_service.py 以及 entrypoint.sh 不能置于 data/model 目录，否则会被平台覆盖。详细可以参考典型案例。

### 基于其他镜像制作自定义镜像
1.	在线推理服务端口限制为8501。
2.	服务必须以 HTTP 协议接受请求，并且只支持 POST 方法。
3.	自定义的代码及数据不能置于 /data/model 目录，否则会被平台覆盖。
4.	dockerfile 文件中必须包含启动命令。

## 典型案例
本案例介绍了基于 CPU 版本的平台基础镜像，通过修改 model_service.py 及 entrypoint.sh 文件，实现一个简单的加法器。

### 制作镜像

#### 编写代码
一共包含三个文件：

|文件|	作用|
|--|--|
|model_service.py	|按照 tiinfer 的要求，编写加法器模型。|
|entrypoint.sh	|启动脚本，可在此自行安装更多的依赖包。|
|Dockerfile|	负责将前两个文件拷贝到镜像中。|

1. model_service.py 的内容：
```
from typing import Dict
import tiinfer

class AdderModel(tiinfer.Model):
    def __init__(self, model_dir: str):
        super().__init__(model_dir)

    def load(self) -> bool:
        self.ready = True
        return self.ready

    def preprocess(self, request: Dict) -> Dict:
        return request

    def predict(self, request: Dict) -> Dict:
        return {'result': request['a'] + request['b']}

    def postprocess(self, result: Dict) -> Dict:
        return result
```

2. entrypoint.sh的内容：
```
#!/bin/bash
source /etc/profile
source /root/.bashrc
export LD_LIBRARY_PATH=/usr/local/python3/lib/python3.8/site-packages/torch/lib:/usr/local/openmpi/lib:/usr/local/nccl/lib:/usr/local/cuda/lib64:/usr/local/python3/lib:/usr/local/python3/lib64:/usr/local/openmpi/lib:/usr/local/gcc/lib:/usr/local/gcc/lib64


MODEL_DIR=/usr/local/service/adder
ALGORITHM_NAME=m
REST_PORT=8501


echo "====================== start serving ============================"
echo "python3 -m tiinfer --http_port ${REST_PORT} --algorithm_name ${ALGORITHM_NAME} --model_dir ${MODEL_DIR} "
cd ${MODEL_DIR}
python3 -m tiinfer --http_port ${REST_PORT} --algorithm_name "${ALGORITHM_NAME}" --model_dir ${MODEL_DIR}

3. Dockerfile 的内容:
FROM ccr.ccs.tencentyun.com/tione-public-images/ti-infer-cpu-base:1.0.0 

COPY model_service.py /usr/local/service/adder/model_service.py
COPY entrypoint.sh ./entrypoint.sh
RUN chmod +x ./entrypoint.sh
```

#### 打包镜像
1. 整体步骤：
	-	本地配置 docker 环境，并开通 腾讯云容器镜像服务；
	-	创建命名空间及新建个人镜像仓库；
	-	编译自定义推理镜像，推送到个人镜像仓库；
	-	在启动模型服务时，实例容器栏选择不使用模型文件，选择运行环境进入个人镜像仓库列表，选择上一步推送的自定义镜像环境；
	-	配置好参数，启动服务。
2. 详细说明：
执行如下命令来打包：
```
docker build . --tag ccr.ccs.tencentyun.com/YOUR_NAMESPACE/YOUR_IMAGENAME
```
打包完成后，可以通过如下方式在本地检查服务运行是否正常：
	-	执行 `docker run -d --name myinfer ccr.ccs.tencentyun.com/YOUR_NAMESPACE/YOUT_IMAGENAME` 将服务运行起来；
	-	执行 `docker exec -it ccr.ccs.tencentyun.com/YOUR_NAMESPACE/YOUT_IMAGENAME bash` 进入容器中；
	-	在容器中执行 `curl http://127.0.0.1:8501/v1/models/m:predict -d '{"a": 1, "b": 2}'` 得到正确返回: {"result": 3}；
确认服务正常运行后，执行如下命令上传镜像：`docker push ccr.ccs.tencentyun.com/YOUR_NAMESPACE/YOUR_IMAGENAME`。

### 运行自定义镜像
创建服务的时候，选择不使用模型。
 ![](https://qcloudimg.tencent-cloud.cn/raw/edfd6c45c14d7f96618bb1976c7cedf8.png)
运行环境选择刚刚上传的镜像即可。
 ![](https://qcloudimg.tencent-cloud.cn/raw/fc43f8f1ab4e93dd0170b27fb99d7210.png)

