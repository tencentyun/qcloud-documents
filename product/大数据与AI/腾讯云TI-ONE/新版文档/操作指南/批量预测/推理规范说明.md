

批量预测模块支持用户使用在 TI-ONE 模型仓库管理的模型创建预测任务，或者使用自定义镜像创建预测任务。

## 模型包目录结构
```
cos://bucket/train_models/mv-2302330437837096
├── config.json
├── model
│   ├── model.pth
├── model_service.py
└── requirements.txt
```
- config.json：模型包中 API 接口的配置文件，需参照下方的推理配置文件规范进行整理；
- model：存放模型的文件夹目录，该目录可以客户自己指定，但需要确保 model_service.py 可以正确加载；
- model.pth：原始模型文件，名称可以客户自己指定，但需要确保 model_service.py 可以正确加载；
- model_service.py：模型推理脚本，用户可复用在线服务的model_service.py进行改造，支持批量预测；
- requirements.txt：第三方依赖包，如推理脚本有第三方依赖可以放在 requirements.txt 文件内，如没有可直接忽略，容器服务启动前，会利用 pip 安装 requirements.txt 里面的第三方依赖；
- 其他：其他相关依赖文件，如 model_service.py 依赖的 .py 文件，可以自由放置并打包上传。

## 推理配置文件规范
config.json 配置脚本，和推理脚本 model_service.py 是配套的，平台批量预测脚本会解析此文件，获取 request_example 的 key 作为批量输入 key，因此请保持 key 与推理脚本中一致，获取 batch.size 作为批量输入数量。
配置文件示例：
```
{
    "apis": [
        {
            "request_example": {
                "images": [
                    "<base64>",
                    "<base64>"
                ]
            }
        }
    ],
    "batch": {
        "size": 8
    }
}
```

## 平台路径规范
1. /opt/ml/input/data：数据输入路径，平台默认会将控制台选择的“输入数据”挂载到此路径下，并递归读取所有子目录下的文件进行批量输入，输入值为文件路径。
2. /opt/ml/model：模型包路径，平台默认会将模型包下载到此路径，批量预测脚本会根据此路径加载推理脚本model_service.py。
3. /opt/ml/output：数据输出路径，平台批量预测脚本会将推理结果写入/opt/ml/output/result.txt文件，任务完成后，平台会将result.txt上传到控制台选择的“输出路径”下。
4. /opt/ml/code：使用代码包的情况下，平台会将用户的代码包下载到此路径。

## 无代码包场景
不使用代码包场景下，用户只需在模型仓库对应文件的模型包中上传推理相关文件（参考上方模型包目录结构相关说明），平台内置批量预测脚本会自动调用推理脚本进行批量预测。

## 有代码包场景
用户可自定义开发批量预测的推理脚本，并上传到 COS，并在创建预测任务时进行选择；平台会将代码包下载到 /opt/ml/code 目录，且默认启动命令为  sh /opt/ml/code/start.sh；用户也可自定义填写启动命令，填写后平台会使用自定义启动命令启动。

## 自定义镜像规范
1. 自定义镜像限制及建议
	1. 代码包放置路径建议为 /opt/ml/code，平台启动命令默认为 sh /opt/ml/code/start.sh；如果代码包为其他路径，用户可自定义启动命令；
	2. 数据挂载目录：/opt/ml/input/data；
	3. 结果输出目录：/opt/ml/output。
2. 自定义运行环境使用步骤
	1. 本地配置 docker 环境，并开通 腾讯云容器镜像服务（Tencent Container Registry，TCR）;
	2. 编译自定义推理镜像，推送到TCR的镜像仓库；
	3. 在创建批量预测任务时，实例容器选择不使用模型文件，选择运行环境进入镜像仓库列表，选择上一步推送的自定义镜像环境；
	4. 配置参数，启动批量预测任务。



