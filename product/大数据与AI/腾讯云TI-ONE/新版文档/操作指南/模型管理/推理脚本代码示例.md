# 推理脚本规范


## 推理脚本规范
- 推理脚本需要实现 tiinfer.Model 类对应的函数，包含 __init__ 初始化、load 模型加载、preprocess 数据预处理、predict 模型推理、postprocess 后处理函数。
- 其中初始化函数中，根据 modir_dir 参数可以获取代码运行的绝对路径目录，如模型加载需要绝对路径，需要根据 modir_dir 参数和目录内模型的相对路径拼接获取绝对路径，其他配置文件的绝对路径类似。
- 服务启动默认是单进程的，如果想要多进程，可以通过在 __init__ 函数中设置 self.workers 参数，如 self.workers=2 会自动启动两个进程的推理服务；

## mnist 手写数字识别对应的模型推理脚本 Pytorch 实现示例:

```
# model_service.py 示例
import os
from typing import Dict
import logging
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms
from net import Net

import numpy as np
import cv2
import tiinfer
import base64
import urllib
import time



PYTORCH_FILE = "model/mnist_cnn.pt"

class MnistClassifyModel(tiinfer.Model):
    def __init__(self, model_dir: str):
        super().__init__(model_dir)
        self.model_dir = model_dir
        self.model_name = "mnist-classify"
        self.result_type = "classification"
        self.ready = False
        self.model = None
        self.imgsz=28
        self.mean = 0.1307
        self.std = 0.3081
        self.use_gpu = torch.cuda.is_available()
        self.device = torch.device('cuda:0' if self.use_gpu else 'cpu')
        self.resize = transforms.Resize((28, 28))
        self.gray_trans = transforms.Grayscale()
        self.normalize = transforms.Normalize(mean=0.1307,std=0.3081)


    def load(self) -> bool:
        try:
            if self.use_gpu:
                import torchnvjpeg
                self.decoder = torchnvjpeg.Decoder()

            # Load  model file
            model = Net().to(self.device)
            model_file = os.path.join(self.model_dir,  PYTORCH_FILE)
            checkpoint = torch.load(model_file, map_location=self.device)
            model.load_state_dict(checkpoint)
            model.eval()
            self.model = model
            logging.info("load model to device %s" % (self.device) )
        except Exception as e:  # pylint: disable=broad-except
            logging.error("load model failed: %s", e)
            self.ready = False
            return self.ready
        self.ready = True
        return self.ready

    def preprocess(self, request: Dict) -> Dict:
        try:
            if "images" not in request:
                return {"error": "invalid param"}
            start_time = time.time()
            images = request["images"]
            inputs = []
            resize_time = 0.0
            decode_time = 0.0
            copy_time = 0.0
            b64_time = 0.0
            for ni, img_data in enumerate(images):
                t1 = time.time()
                if img_data.startswith("http"):
                    url_response = urllib.request.urlopen(img_data)
                    image_data = url_response.read()
                    if not self.use_gpu:
                        image_nparr = np.array(bytearray(image_data), dtype=np.uint8)
                else:
                    image_data = base64.b64decode(img_data)
                    if not self.use_gpu:
                        image_nparr = np.frombuffer(image_data, np.uint8)
                b64_time += (time.time()-t1)
                t1 = time.time()
                if  self.use_gpu:
                    img_tensor = self.decoder.decode(image_data)
                    decode_time += (time.time()-t1)
                    t1 = time.time()
                    img_tensor = self.gray_trans(self.resize(img_tensor.permute((2,0,1)))).type(torch.float32)/255
                    img_tensor = self.normalize(img_tensor)
                    inputs.append(img_tensor.unsqueeze(0))
                    copy_time +=  (time.time()-t1)
                else:
                    img_np = cv2.imdecode(image_nparr, cv2.IMREAD_COLOR)
                    decode_time += (time.time()-t1)
                    t1 = time.time()
                    img_tensor = self.gray_trans(self.resize(torch.Tensor(img_np).permute((2,0,1)))).type(torch.float32)/255
                    img_tensor = self.normalize(img_tensor)
                    resize_time += (time.time()-t1)
                    t1 = time.time()
                    inputs.append(img_tensor.unsqueeze(0).to(self.device))
                    copy_time +=  (time.time()-t1)

            logging.info("copy cpu image to gpu cost: %.2f ms", (1000*copy_time))
            logging.info("b64    cost: %.2f ms", (1000*b64_time))
            logging.info("decode cost: %.2f ms", (1000*decode_time))
            logging.info("resize cost: %.2f ms", (1000*resize_time))
            logging.info("preprocess cost: %.2f ms", 1000*(time.time() - start_time))

            return {"instances": inputs}
        except Exception as e:
            logging.error("Preprocess failed: %s" % (e))
            return {"error": "preprocess failed"}

    def predict(self, request: Dict) -> Dict:
        if "instances" not in request :
            return request
        with torch.no_grad():
            try:
                inputs = torch.cat(request["instances"], dim=0)
                logging.info("inputs shape %s", inputs.shape)
                out = self.model(inputs)
                pred = out.argmax(dim=1, keepdim=False)
                return {
                    "pred_out":  pred.to('cpu').tolist()
                }
            except Exception as e:  # pylint: disable=broad-except
                logging.error("Failed to predict" % (e))
                request.pop("image", '')
                request.pop("instances", '')
                request["predict_err"] = str(e)
                return request

    def postprocess(self, request: Dict) -> Dict:
        if "pred_out" not in request:
            return request
        try:
            pred_out = request["pred_out"]

            return {"result": {
                "type": self.result_type,
                "model_name": self.model_name,
                "pred": pred_out[0]
            }}
        except Exception as e:  # pylint: disable=broad-except
            logging.error("Postprocess failed: %s" % (e))
            request.pop("image", '')
            request.pop("instances", '')
            request.pop("predictions", '')
            request["post_process_err"] = str(e)
            return request


```
```
#### 依赖的net.py 文件

import torch
import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.dropout1 = nn.Dropout(0.25)
        self.dropout2 = nn.Dropout(0.5)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        output = F.log_softmax(x, dim=1)
        return output

```


##  mnist 手写数字识别对应的模型推理脚本 TensorFlow 实现示例:
```
# model_service.py 示例

import os
from typing import Dict
import logging
import numpy as np
import tensorflow as tf
import cv2
import tiinfer
import base64
import urllib
import time

MODEL_PATH = "model"


class MnistModel(tiinfer.Model):
    def __init__(self, model_dir: str):
        super().__init__( model_dir)
        self.model_dir = model_dir
        self.model_name = "sequential"
        self.result_type = "mnist"
        self.ready = False
        self.model = None
        self.device_gpu = False
        self.top_predit = 1
        self.img_size = 28

    def load(self) -> bool:
        try:
            tf.config.set_soft_device_placement(True)
            # tf.debugging.set_log_device_placement(True)
            gpus = tf.config.experimental.list_physical_devices('GPU')
            logging.info("gpus %s" % gpus)
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
                tf.config.experimental.set_visible_devices(gpu, 'GPU')
                logging.info("config gpu done.......")

            self.model = tf.keras.models.load_model(os.path.join(self.model_dir, MODEL_PATH))
            self.model.summary()
            self.device_gpu = tf.test.is_gpu_available()
            logging.info("load model to device_gpu %s" % (self.device_gpu) )
        except Exception as e:  # pylint: disable=broad-except
            logging.error("load model failed: %s", e)
            self.ready = False
            return self.ready
        self.ready = True
        return self.ready

    def preprocess(self, request: Dict) -> Dict:
        try:
            if "images" not in request:
                return {"error": "invalid param"}
            t1 = time.time()
            image_array = []
            images = request["images"]
            for _, img_data in enumerate(images):
                if img_data.startswith("http"):
                    url_response = urllib.request.urlopen(img_data)
                    image_data = url_response.read()
                else:
                    image_data = base64.b64decode(img_data)
                img_tensor = tf.io.decode_jpeg(image_data, channels=3)
                img_tensor = tf.image.resize(img_tensor, [self.img_size, self.img_size])
                img_tensor = tf.image.rgb_to_grayscale(img_tensor)
                img_tensor = tf.expand_dims(img_tensor, 0)
                image_array.append(img_tensor)


            return {"instances": image_array}
        except Exception as e:  # pylint: disable=broad-except
            logging.error("Preprocess failed: %s" % (e))
            return {"error": "preprocess failed", "message": str(e)}

    def predict(self, request: Dict) -> Dict:
        if "instances" not in request:
            return request
        try:
            inputs = tf.concat(request["instances"], 0)
            logging.info("image shape: %s", inputs.shape)
            output = self.model.predict(inputs)
            logging.info("model predicted %s" % output)
            pred = np.argmax(output,1)
            return {"result": {
                "type": self.result_type,
                "model_name": self.model_name,
                "pred": int(pred[0])
            }}
        except Exception as e:
            logging.error("Failed to predict %s" % (e))
            request.pop("image", '')
            request.pop("instances", '')
            request["predict_err"] = str(e)
            return request


```
