
随着客户业务的不断发展，ES 集群的数据规模和访问量也越来越大，当集群规模和配置逐渐与业务实际需求不匹配时，客户可选择及时的扩容 ES 集群以满足业务的需要。

下表中是腾讯云 ES 集群扩容的总体概览。客户可根据下表快速选择一种适合当前集群的扩容方式。

| 扩容变配类型 | 原理 | 特点 |  适用场景
|---------|---------|---------|-----|
| 集群扩容磁盘 | 逐个对集群中节点上挂载的磁盘进行纵向扩容磁盘容量 | 平滑扩容不停服，流程时间短，每个节点预计30秒 |  计算资源充足，磁盘存储空间不足
| 集群添加节点  |  向集群中加入更多同等规格的节点   |  平滑扩容不停服，流程时间短，不受节点数量影响，预计5 - 10分钟完成   |  节点规格较高，但集群整体计算资源不足
|  节点提升规格  |  向集群中加入数量相等的更高机型配置的节点，将老节点中的数据搬迁到新节点后，剔除老节点  |  平滑扩容不停服，扩容过程中需要进行数据搬迁，数据量越大，扩容时间越长   |  节点规格较低，且集群整体计算资源不足


下面分别介绍腾讯云 ES 目前支持的几种扩容场景。

## 扩容磁盘
**扩容磁盘：**是指在不改变节点计算资源配置的情况下，提升集群整体的存储容量。
**扩容原理：**磁盘扩容的流程是对集群中每个数据节点上挂载的磁盘依次进行纵向扩容操作，以达到提升整个集群磁盘存储容量的目的。例如，磁盘原有容量为1000GB，将该磁盘容量提升至5000GB。此种扩容方式无需重启节点和集群，对业务使用集群不造成任何影响。由于集群磁盘容量的扩容是采用数据节点滚动扩容的方式，因此扩容时间与集群节点数量呈正相关。预计一个节点扩容需要30秒，以5个数据节点的集群为例，预计扩容磁盘的时间在2分钟左右。流程原理示意图如下图所示：
 ![](https://main.qcloudimg.com/raw/1bde355bce7b7b68ec920c50bc97d7e1.png)
**适用场景：**适用于计算资源充足，但是磁盘容量不足的情况。例如，客户集群 CPU 负载和内存使用率都较低，但是磁盘平均利用率较高，且数据都比较重要，不能通过删除数据的方式来释放磁盘空间。此时，客户可选择只扩容磁盘容量的方式来扩容集群。

## 扩容节点
扩容节点适用于集群的计算资源遇到瓶颈，如 CPU 负载持续较高、内存使用率居高不下、多次触发内存熔断，甚至内存出现 Out Of Memory 现象。这时可通过对集群节点进行扩容，以提升集群整体性能。

### 集群添加节点
**集群添加节点：**是指在不改变集群节点机型配置的情况下，通过向集群中加入更多节点来完成集群的扩容操作，其流程原理示意图如下所示：
![](https://main.qcloudimg.com/raw/e955275f114fac35a73e5b97efa56fbc.png)
这种扩容流程的**优势**主要是能够做到**平滑扩容**，扩容过程中不影响业务使用。由于在扩容流程中不涉及新老数据节点间的数据搬迁，因此扩容时间不受节点数量影响，通常在5-10分钟内完成。

**适用场景：**节点的配置已经很高，且期望进一步提升集群整体的读写性能，同时对扩容期间集群的稳定性要求较高，这种情况下可选择向集群中添加节点方式进行扩容。

### 节点提升规格
**节点提升规格：**是指在不改变集群节点个数的情况下，整体提升集群中数据节点或专用主节点计算资源的配置。例如，将数据节点的配置从4核16G提升到8核16G。

- 采用数据搬迁的方式
数据搬迁的方式，其主要原理是先将数量大小相等且更高规格的节点加入集群，然后将老节点上的数据全部搬迁到新节点上后，再将老节点剔除集群，以完成集群的扩容操作。其流程原理示意图如下所示：
 ![](https://main.qcloudimg.com/raw/d8056af7acef6c499456a5bae0ee149f.png)
这种方式的**优势**主要是能够做到**平滑扩容**，扩容过程不影响集群的使用及可用性。不足之处在于这种扩容流程需要将老节点中的数据迁移到新节点上，迁移时间受数据量的影响较大。因此如果集群的数据量在 TB 以上，且希望能够尽快完成扩容流程，可选择节点滚动重启的扩容方式。或者通过在 kibana 中设置如下属性，提升数据搬迁的速度：
```
PUT _cluster/settings
{
		"persistent": {
			"cluster.routing.allocation.node_concurrent_recoveries": "8",
			"indices.recovery.max_bytes_per_sec": "80mb"
		}
}
```
- `cluster.routing.allocation.node_concurrent_recoveries`属性表示集群中每个节点上分片并发恢复的个数，默认为2。可根据老数据节点的 CPU 核数 * 4来确定具体的值，但不要超过50。例如，老数据节点为4核16G，则建议该值设置为16；老数据节点为16核64G，则建议该值设置为50。如果发现调大了该值后集群的稳定性受到影响，可适当减小该值。
- `indices.recovery.max_bytes_per_sec`属性表示节点之间数据传输的最大带宽限制，默认为40mb。该值不宜设置的过高，否则会破坏集群的稳定性。客户可以5mb为步长，逐步调整该限制值，并持续观察集群的稳定性，最终选择一个相对平衡的值。

**适用场景：**节点的配置较低，期望提升集群整体的读写性能，但是对集群在扩容期间的稳定性要求较高，且对扩容时间不是很紧急，这种情况下可选择数据搬迁的方式进行扩容。

